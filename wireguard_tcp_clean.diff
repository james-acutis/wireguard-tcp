diff --git a/wireguard-linux/drivers/net/wireguard/allowedips.c b/wireguard-linux/drivers/net/wireguard/allowedips.c
index 0ba714ca5185..41ff2ae636eb 100644
--- a/wireguard-linux/drivers/net/wireguard/allowedips.c
+++ b/wireguard-linux/drivers/net/wireguard/allowedips.c
@@ -35,7 +35,8 @@ static void copy_and_assign_cidr(struct allowedips_node *node, const u8 *src,
 
 static inline u8 choose(struct allowedips_node *node, const u8 *key)
 {
-	return (key[node->bit_at_a] >> node->bit_at_b) & 1;
+	u8 result = (key[node->bit_at_a] >> node->bit_at_b) & 1;
+	return result;
 }
 
 static void push_rcu(struct allowedips_node **stack,
@@ -81,19 +82,23 @@ static void root_remove_peer_lists(struct allowedips_node *root)
 
 static unsigned int fls128(u64 a, u64 b)
 {
-	return a ? fls64(a) + 64U : fls64(b);
+	unsigned int result = a ? fls64(a) + 64U : fls64(b);
+	return result;
 }
 
 static u8 common_bits(const struct allowedips_node *node, const u8 *key,
 		      u8 bits)
 {
+	u8 result;
 	if (bits == 32)
-		return 32U - fls(*(const u32 *)node->bits ^ *(const u32 *)key);
+		result = 32U - fls(*(const u32 *)node->bits ^ *(const u32 *)key);
 	else if (bits == 128)
-		return 128U - fls128(
+		result = 128U - fls128(
 			*(const u64 *)&node->bits[0] ^ *(const u64 *)&key[0],
 			*(const u64 *)&node->bits[8] ^ *(const u64 *)&key[8]);
-	return 0;
+	else
+		result = 0;
+	return result;
 }
 
 static bool prefix_matches(const struct allowedips_node *node, const u8 *key,
@@ -105,7 +110,8 @@ static bool prefix_matches(const struct allowedips_node *node, const u8 *key,
 	 * modern processors, even taking into account the unfortunate bswap.
 	 * So, we just inline it like this instead.
 	 */
-	return common_bits(node, key, bits) >= node->cidr;
+	bool result = common_bits(node, key, bits) >= node->cidr;
+	return result;
 }
 
 static struct allowedips_node *find_node(struct allowedips_node *trie, u8 bits,
@@ -123,6 +129,8 @@ static struct allowedips_node *find_node(struct allowedips_node *trie, u8 bits,
 	return found;
 }
 
+
+#ifdef ORIGINAL
 /* Returns a strong reference to a peer */
 static struct wg_peer *lookup(struct allowedips_node __rcu *root, u8 bits,
 			      const void *be_ip)
@@ -145,6 +153,47 @@ static struct wg_peer *lookup(struct allowedips_node __rcu *root, u8 bits,
 	rcu_read_unlock_bh();
 	return peer;
 }
+#endif // ORIGINAL
+
+/* Returns a strong reference to a peer */
+static struct wg_peer *lookup(struct allowedips_node __rcu *root, u8 bits,
+                              const void *be_ip)
+{
+    u8 ip[16] __aligned(__alignof(u64));
+    struct allowedips_node *node;
+    struct wg_peer *peer = NULL;
+
+    /* Defensive check for null pointers */
+    if (!root || !be_ip) {
+        return NULL;
+    }
+
+    swap_endian(ip, be_ip, bits);
+
+    rcu_read_lock_bh();
+retry:
+    node = rcu_dereference_bh(root);
+    if (!node) {
+        rcu_read_unlock_bh();
+        return NULL;
+    }
+
+    node = find_node(node, bits, ip);
+    if (node) {
+        peer = rcu_dereference_bh(node->peer);
+        if (!peer) {
+            goto retry;
+        }
+        peer = wg_peer_get_maybe_zero(peer);
+        if (!peer) {
+            goto retry;
+        }
+    }
+
+    rcu_read_unlock_bh();
+    return peer;
+}
+
 
 static bool node_placement(struct allowedips_node __rcu *trie, const u8 *key,
 			   u8 cidr, u8 bits, struct allowedips_node **rnode,
@@ -194,11 +243,13 @@ static int add(struct allowedips_node __rcu **trie, u8 bits, const u8 *key,
 		list_add_tail(&node->peer_list, &peer->allowedips_list);
 		copy_and_assign_cidr(node, key, cidr, bits);
 		connect_node(trie, 2, node);
+		printk(KERN_INFO "Exiting add with return 0\n");
 		return 0;
 	}
 	if (node_placement(*trie, key, cidr, bits, &node, lock)) {
 		rcu_assign_pointer(node->peer, peer);
 		list_move_tail(&node->peer_list, &peer->allowedips_list);
+		printk(KERN_INFO "Exiting add with return 0\n");
 		return 0;
 	}
 
@@ -216,6 +267,7 @@ static int add(struct allowedips_node __rcu **trie, u8 bits, const u8 *key,
 		down = rcu_dereference_protected(node->bit[bit], lockdep_is_held(lock));
 		if (!down) {
 			connect_node(&node->bit[bit], bit, newnode);
+			printk(KERN_INFO "Exiting add with return 0\n");
 			return 0;
 		}
 	}
@@ -286,7 +338,8 @@ int wg_allowedips_insert_v4(struct allowedips *table, const struct in_addr *ip,
 
 	++table->seq;
 	swap_endian(key, (const u8 *)ip, 32);
-	return add(&table->root4, 32, key, cidr, peer, lock);
+	int result = add(&table->root4, 32, key, cidr, peer, lock);
+	return result;
 }
 
 int wg_allowedips_insert_v6(struct allowedips *table, const struct in6_addr *ip,
@@ -297,12 +350,14 @@ int wg_allowedips_insert_v6(struct allowedips *table, const struct in6_addr *ip,
 
 	++table->seq;
 	swap_endian(key, (const u8 *)ip, 128);
-	return add(&table->root6, 128, key, cidr, peer, lock);
+	int result = add(&table->root6, 128, key, cidr, peer, lock);
+	return result;
 }
 
 void wg_allowedips_remove_by_peer(struct allowedips *table,
 				  struct wg_peer *peer, struct mutex *lock)
 {
+	printk(KERN_INFO "Entering wg_allowedips_remove_by_peer(table=%p, peer=%p, lock=%p)\n", table, peer, lock);
 	struct allowedips_node *node, *child, **parent_bit, *parent, *tmp;
 	bool free_parent;
 
@@ -349,35 +404,43 @@ int wg_allowedips_read_node(struct allowedips_node *node, u8 ip[16], u8 *cidr)
 		ip[cidr_bytes - 1U] &= ~0U << (-node->cidr % 8U);
 
 	*cidr = node->cidr;
-	return node->bitlen == 32 ? AF_INET : AF_INET6;
+	int result = node->bitlen == 32 ? AF_INET : AF_INET6;
+	return result;
 }
 
 /* Returns a strong reference to a peer */
 struct wg_peer *wg_allowedips_lookup_dst(struct allowedips *table,
 					 struct sk_buff *skb)
 {
+	struct wg_peer *result;
 	if (skb->protocol == htons(ETH_P_IP))
-		return lookup(table->root4, 32, &ip_hdr(skb)->daddr);
+		result = lookup(table->root4, 32, &ip_hdr(skb)->daddr);
 	else if (skb->protocol == htons(ETH_P_IPV6))
-		return lookup(table->root6, 128, &ipv6_hdr(skb)->daddr);
-	return NULL;
+		result = lookup(table->root6, 128, &ipv6_hdr(skb)->daddr);
+	else
+		result = NULL;
+	return result;
 }
 
 /* Returns a strong reference to a peer */
 struct wg_peer *wg_allowedips_lookup_src(struct allowedips *table,
 					 struct sk_buff *skb)
 {
+	struct wg_peer *result;
 	if (skb->protocol == htons(ETH_P_IP))
-		return lookup(table->root4, 32, &ip_hdr(skb)->saddr);
+		result = lookup(table->root4, 32, &ip_hdr(skb)->saddr);
 	else if (skb->protocol == htons(ETH_P_IPV6))
-		return lookup(table->root6, 128, &ipv6_hdr(skb)->saddr);
-	return NULL;
+		result = lookup(table->root6, 128, &ipv6_hdr(skb)->saddr);
+	else
+		result = NULL;
+	return result;
 }
 
 int __init wg_allowedips_slab_init(void)
 {
 	node_cache = KMEM_CACHE(allowedips_node, 0);
-	return node_cache ? 0 : -ENOMEM;
+	int result = node_cache ? 0 : -ENOMEM;
+	return result;
 }
 
 void wg_allowedips_slab_uninit(void)
diff --git a/wireguard-linux/drivers/net/wireguard/cookie.c b/wireguard-linux/drivers/net/wireguard/cookie.c
index f89581b5e8cb..ebfd2a518ba3 100644
--- a/wireguard-linux/drivers/net/wireguard/cookie.c
+++ b/wireguard-linux/drivers/net/wireguard/cookie.c
@@ -75,9 +75,11 @@ void wg_cookie_init(struct cookie *cookie)
 static void compute_mac1(u8 mac1[COOKIE_LEN], const void *message, size_t len,
 			 const u8 key[NOISE_SYMMETRIC_KEY_LEN])
 {
+	printk(KERN_INFO "Entering: compute_mac1 with mac1=%p, message=%p, len=%zu, key=%p\n", mac1, message, len, key);
 	len = len - sizeof(struct message_macs) +
 	      offsetof(struct message_macs, mac1);
 	blake2s(mac1, message, key, COOKIE_LEN, len, NOISE_SYMMETRIC_KEY_LEN);
+	printk(KERN_INFO "Exiting: compute_mac1\n");
 }
 
 static void compute_mac2(u8 mac2[COOKIE_LEN], const void *message, size_t len,
@@ -120,32 +122,44 @@ enum cookie_mac_state wg_cookie_validate_packet(struct cookie_checker *checker,
 						struct sk_buff *skb,
 						bool check_cookie)
 {
-	struct message_macs *macs = (struct message_macs *)
-		(skb->data + skb->len - sizeof(*macs));
+
+	struct message_macs *macs = (struct message_macs *)(skb->data + skb->len - sizeof(*macs));
 	enum cookie_mac_state ret;
 	u8 computed_mac[COOKIE_LEN];
 	u8 cookie[COOKIE_LEN];
 
 	ret = INVALID_MAC;
-	compute_mac1(computed_mac, skb->data, skb->len,
-		     checker->message_mac1_key);
-	if (crypto_memneq(computed_mac, macs->mac1, COOKIE_LEN))
+
+	// Compute MAC1 and compare
+	compute_mac1(computed_mac, skb->data, skb->len, checker->message_mac1_key);
+
+	if (crypto_memneq(computed_mac, macs->mac1, COOKIE_LEN)) {
+		printk(KERN_ERR "MAC1 validation failed.\n");
 		goto out;
+	}
 
 	ret = VALID_MAC_BUT_NO_COOKIE;
 
 	if (!check_cookie)
 		goto out;
 
+	// Generate cookie and compute MAC2
 	make_cookie(cookie, skb, checker);
 
 	compute_mac2(computed_mac, skb->data, skb->len, cookie);
-	if (crypto_memneq(computed_mac, macs->mac2, COOKIE_LEN))
+
+	if (crypto_memneq(computed_mac, macs->mac2, COOKIE_LEN)) {
+		printk(KERN_ERR "MAC2 validation failed.\n");
 		goto out;
+	}
 
 	ret = VALID_MAC_WITH_COOKIE_BUT_RATELIMITED;
-	if (!wg_ratelimiter_allow(skb, dev_net(checker->device->dev)))
+
+	// Rate limiting check
+	if (!wg_ratelimiter_allow(skb, dev_net(checker->device->dev))) {
+		printk(KERN_INFO "Packet rate-limited.\n");
 		goto out;
+	}
 
 	ret = VALID_MAC_WITH_COOKIE;
 
@@ -198,6 +212,7 @@ void wg_cookie_message_create(struct message_handshake_cookie *dst,
 void wg_cookie_message_consume(struct message_handshake_cookie *src,
 			       struct wg_device *wg)
 {
+
 	struct wg_peer *peer = NULL;
 	u8 cookie[COOKIE_LEN];
 	bool ret;
diff --git a/wireguard-linux/drivers/net/wireguard/device.c b/wireguard-linux/drivers/net/wireguard/device.c
index deb9636b0ecf..8ad413c91168 100644
--- a/wireguard-linux/drivers/net/wireguard/device.c
+++ b/wireguard-linux/drivers/net/wireguard/device.c
@@ -19,6 +19,8 @@
 #include <linux/if_arp.h>
 #include <linux/icmp.h>
 #include <linux/suspend.h>
+#include <linux/spinlock.h>
+#include <linux/wireguard.h>
 #include <net/dst_metadata.h>
 #include <net/gso.h>
 #include <net/icmp.h>
@@ -26,6 +28,11 @@
 #include <net/ip_tunnels.h>
 #include <net/addrconf.h>
 
+#define WG_TRANSPORT_UDP	0
+#define WG_TRANSPORT_TCP	1
+
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
 static LIST_HEAD(device_list);
 
 static int wg_open(struct net_device *dev)
@@ -34,7 +41,7 @@ static int wg_open(struct net_device *dev)
 	struct inet6_dev *dev_v6 = __in6_dev_get(dev);
 	struct wg_device *wg = netdev_priv(dev);
 	struct wg_peer *peer;
-	int ret;
+	int ret = 0;
 
 	if (dev_v4) {
 		/* At some point we might put this check near the ip_rt_send_
@@ -47,10 +54,17 @@ static int wg_open(struct net_device *dev)
 	if (dev_v6)
 		dev_v6->cnf.addr_gen_mode = IN6_ADDR_GEN_MODE_NONE;
 
-	mutex_lock(&wg->device_update_lock);
+	
+	wg->listener_active = false;
+	if (wg->transport == WG_TRANSPORT_TCP) {
+		ret = wg_tcp_listener_socket_init(wg, wg->incoming_port);
+		if (ret < 0)
+			goto out;
+	}
 	ret = wg_socket_init(wg, wg->incoming_port);
 	if (ret < 0)
-		goto out;
+		 goto out;
+	mutex_lock(&wg->device_update_lock);
 	list_for_each_entry(peer, &wg->peer_list, peer_list) {
 		wg_packet_send_staged_packets(peer);
 		if (peer->persistent_keepalive_interval)
@@ -71,11 +85,15 @@ static int wg_pm_notification(struct notifier_block *nb, unsigned long action, v
 	 * don't actually want to clear keys.
 	 */
 	if (IS_ENABLED(CONFIG_PM_AUTOSLEEP) ||
-	    IS_ENABLED(CONFIG_PM_USERSPACE_AUTOSLEEP))
+	    IS_ENABLED(CONFIG_PM_USERSPACE_AUTOSLEEP)) {
+		printk(KERN_INFO "Exiting wg_pm_notification (no action): nb=%p, action=%lu, data=%p\n", nb, action, data);
 		return 0;
+	}
 
-	if (action != PM_HIBERNATION_PREPARE && action != PM_SUSPEND_PREPARE)
+	if (action != PM_HIBERNATION_PREPARE && action != PM_SUSPEND_PREPARE) {
+		printk(KERN_INFO "Exiting wg_pm_notification (no action): nb=%p, action=%lu, data=%p\n", nb, action, data);
 		return 0;
+	}
 
 	rtnl_lock();
 	list_for_each_entry(wg, &device_list, device_list) {
@@ -129,7 +147,20 @@ static int wg_stop(struct net_device *dev)
 	mutex_unlock(&wg->device_update_lock);
 	while ((skb = ptr_ring_consume(&wg->handshake_queue.ring)) != NULL)
 		kfree_skb(skb);
+
 	atomic_set(&wg->handshake_queue_len, 0);
+
+	// Cancel the delayed work and wait for it to finish
+	cancel_delayed_work_sync(&wg->tcp_cleanup_work);
+
+	// Clear the tcp_cleanup_scheduled flag
+	spin_lock_bh(&wg->tcp_cleanup_lock);
+	wg->tcp_cleanup_scheduled = false;
+	spin_unlock_bh(&wg->tcp_cleanup_lock);
+
+	if (wg->transport == WG_TRANSPORT_TCP)
+		wg_tcp_listener_socket_release(wg);
+	
 	wg_socket_reinit(wg, NULL, NULL);
 	return 0;
 }
@@ -265,9 +296,8 @@ static void wg_destruct(struct net_device *dev)
 	free_percpu(dev->tstats);
 	kvfree(wg->index_hashtable);
 	kvfree(wg->peer_hashtable);
+	wg_destruct_tcp_connection_list(wg);
 	mutex_unlock(&wg->device_update_lock);
-
-	pr_debug("%s: Interface destroyed\n", dev->name);
 	free_netdev(dev);
 }
 
@@ -286,7 +316,7 @@ static void wg_setup(struct net_device *dev)
 	dev->header_ops = &ip_tunnel_header_ops;
 	dev->hard_header_len = 0;
 	dev->addr_len = 0;
-	dev->needed_headroom = DATA_PACKET_HEAD_ROOM;
+	dev->needed_headroom = DATA_PACKET_HEAD_ROOM + (wg->transport ? WG_TCP_ENCAP_HDR_LEN : 0);
 	dev->needed_tailroom = noise_encrypted_len(MESSAGE_PADDING_MULTIPLE);
 	dev->type = ARPHRD_NONE;
 	dev->flags = IFF_POINTOPOINT | IFF_NOARP;
@@ -296,7 +326,8 @@ static void wg_setup(struct net_device *dev)
 	dev->hw_features |= WG_NETDEV_FEATURES;
 	dev->hw_enc_features |= WG_NETDEV_FEATURES;
 	dev->mtu = ETH_DATA_LEN - overhead;
-	dev->max_mtu = round_down(INT_MAX, MESSAGE_PADDING_MULTIPLE) - overhead;
+	dev->max_mtu = round_down(INT_MAX, MESSAGE_PADDING_MULTIPLE) - overhead -
+	               (wg->transport == WG_TRANSPORT_TCP ? WG_TCP_ENCAP_HDR_LEN : 0);
 
 	SET_NETDEV_DEVTYPE(dev, &device_type);
 
@@ -322,6 +353,12 @@ static int wg_newlink(struct net *src_net, struct net_device *dev,
 	wg_cookie_checker_init(&wg->cookie_checker, wg);
 	INIT_LIST_HEAD(&wg->peer_list);
 	wg->device_update_gen = 1;
+	// Initialize the tcp_cleanup_scheduled flag and spinlock
+	wg->tcp_cleanup_scheduled = false;
+	spin_lock_init(&wg->tcp_cleanup_lock);
+
+	// Initialize the work for tcp_cleanup_worker
+	INIT_DELAYED_WORK(&wg->tcp_cleanup_work, wg_tcp_cleanup_worker);
 
 	wg->peer_hashtable = wg_pubkey_hashtable_alloc();
 	if (!wg->peer_hashtable)
@@ -375,12 +412,17 @@ static int wg_newlink(struct net *src_net, struct net_device *dev,
 
 	list_add(&wg->device_list, &device_list);
 
+	wg->incoming_port = WG_INCOMING_PORT;
+
+	INIT_LIST_HEAD(&wg->tcp_connection_list);
+	spin_lock_init(&wg->tcp_connection_list_lock);
+	wg->tcp_socket4_ready = false;
+	wg->tcp_socket6_ready = false;
+
 	/* We wait until the end to assign priv_destructor, so that
 	 * register_netdevice doesn't call it for us if it fails.
 	 */
 	dev->priv_destructor = wg_destruct;
-
-	pr_debug("%s: Interface created\n", dev->name);
 	return ret;
 
 err_uninit_ratelimiter:
@@ -444,7 +486,7 @@ int __init wg_device_init(void)
 
 	ret = register_pm_notifier(&pm_notifier);
 	if (ret)
-		return ret;
+		goto error;
 
 	ret = register_random_vmfork_notifier(&vm_notifier);
 	if (ret)
@@ -457,7 +499,6 @@ int __init wg_device_init(void)
 	ret = rtnl_link_register(&link_ops);
 	if (ret)
 		goto error_pernet;
-
 	return 0;
 
 error_pernet:
@@ -466,6 +507,7 @@ int __init wg_device_init(void)
 	unregister_random_vmfork_notifier(&vm_notifier);
 error_pm:
 	unregister_pm_notifier(&pm_notifier);
+error:
 	return ret;
 }
 
diff --git a/wireguard-linux/drivers/net/wireguard/device.h b/wireguard-linux/drivers/net/wireguard/device.h
index 43c7cebbf50b..3640df915fee 100644
--- a/wireguard-linux/drivers/net/wireguard/device.h
+++ b/wireguard-linux/drivers/net/wireguard/device.h
@@ -11,12 +11,17 @@
 #include "peerlookup.h"
 #include "cookie.h"
 
+
 #include <linux/types.h>
 #include <linux/netdevice.h>
 #include <linux/workqueue.h>
+#include <linux/ktime.h>
 #include <linux/mutex.h>
 #include <linux/net.h>
 #include <linux/ptr_ring.h>
+#include <linux/spinlock.h>
+
+#define WG_INCOMING_PORT 51820
 
 struct wg_device;
 
@@ -37,10 +42,27 @@ struct prev_queue {
 	atomic_t count;
 };
 
+struct endpoint {
+	union {
+		struct sockaddr addr;
+		struct sockaddr_in addr4;
+		struct sockaddr_in6 addr6;
+	};
+	union {
+		struct {
+			struct in_addr src4;
+			/* Essentially the same as addr6->scope_id */
+			int src_if4;
+		};
+		struct in6_addr src6;
+	};
+};
+
 struct wg_device {
 	struct net_device *dev;
 	struct crypt_queue encrypt_queue, decrypt_queue, handshake_queue;
-	struct sock __rcu *sock4, *sock6;
+	struct sock __rcu *sock4, *sock6; // UDP listening sockets
+	struct socket __rcu *tcp_listen_socket4, *tcp_listen_socket6; // TCP listening sockets
 	struct net __rcu *creating_net;
 	struct noise_static_identity static_identity;
 	struct workqueue_struct *packet_crypt_wq,*handshake_receive_wq, *handshake_send_wq;
@@ -49,11 +71,21 @@ struct wg_device {
 	struct index_hashtable *index_hashtable;
 	struct allowedips peer_allowedips;
 	struct mutex device_update_lock, socket_update_lock;
-	struct list_head device_list, peer_list;
+	struct endpoint device_endpoint;
+	struct list_head device_list, peer_list, tcp_connection_list;
+	struct task_struct *tcp_listener4_thread, *tcp_listener6_thread;
+	struct delayed_work tcp_cleanup_work;
+        spinlock_t tcp_cleanup_lock; // Add a spinlock to protect the flag
+	bool tcp_cleanup_scheduled;
+	bool tcp_socket4_ready;
+	bool tcp_socket6_ready;
+	bool listener_active;
+	spinlock_t tcp_connection_list_lock;
 	atomic_t handshake_queue_len;
 	unsigned int num_peers, device_update_gen;
 	u32 fwmark;
 	u16 incoming_port;
+	u8 transport;
 };
 
 int wg_device_init(void);
diff --git a/wireguard-linux/drivers/net/wireguard/main.c b/wireguard-linux/drivers/net/wireguard/main.c
index ee4da9ab8013..2fc401c803eb 100644
--- a/wireguard-linux/drivers/net/wireguard/main.c
+++ b/wireguard-linux/drivers/net/wireguard/main.c
@@ -9,6 +9,7 @@
 #include "queueing.h"
 #include "ratelimiter.h"
 #include "netlink.h"
+#include "socket.h"
 
 #include <uapi/linux/wireguard.h>
 
@@ -17,22 +18,93 @@
 #include <linux/genetlink.h>
 #include <net/rtnetlink.h>
 
+#include <linux/netdevice.h>  // Required for net_device
+#include <linux/inetdevice.h> // Required for inetdev processing
+
+// Global structure to hold default network interface information
+struct default_interface_info {
+	struct net_device *dev;   	// Default network interface
+	__be32 ipv4_address;      	// IPv4 address of the default interface
+	struct in6_addr ipv6_address;	// IPv6 address of the default interface
+	bool ipv4_available;		// Flag indicating if IPv4 is available
+	bool ipv6_available;		// Flag indicating if IPv6 is available
+};
+
+struct default_interface_info default_iface_info;
+
+#include <net/route.h>			// Required for routing table access
+#include <net/ip_fib.h>			// Required for FIB (Forwarding Information Base) access
+
+void lookup_default_interface(void)
+{
+	// Use the initial network namespace
+	struct net *net = &init_net;
+	struct flowi4 fl4 = {
+		// Using 8.8.8.8 as a dummy external destination
+		.daddr = htonl(0x08080808),
+	};
+	struct rtable *rt = ip_route_output_key(net, &fl4);
+
+	if (!rt) {
+		return;
+	}
+
+	// Get the main interface used for routing
+	struct net_device *main_dev = rt->dst.dev;
+
+	if (!main_dev) {
+		ip_rt_put(rt);
+		return;
+	}
+
+	default_iface_info.dev = main_dev;
+
+	// Retrieve the IPv4 address
+	struct in_device *in_dev = __in_dev_get_rtnl(main_dev);
+	struct in_ifaddr *ifa = NULL;
+
+	if (in_dev) {
+		for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+			if (ifa->ifa_scope == RT_SCOPE_UNIVERSE) {
+				default_iface_info.ipv4_address = ifa->ifa_address;
+				default_iface_info.ipv4_available = true;
+				break;
+			}
+		}
+	}
+
+	// Retrieve the IPv6 address
+	struct inet6_dev *in6_dev = __in6_dev_get(main_dev);
+	struct inet6_ifaddr *ifa6 = NULL;
+
+	if (in6_dev) {
+		list_for_each_entry(ifa6, &in6_dev->addr_list, if_list) {
+			if (ifa6->scope == RT_SCOPE_UNIVERSE) {
+				default_iface_info.ipv6_address = ifa6->addr;
+				default_iface_info.ipv6_available = true;
+				break;
+			}
+		}
+	}
+
+	// Clean up routing table reference
+	ip_rt_put(rt);
+}
+
 static int __init wg_mod_init(void)
 {
 	int ret;
-
 	ret = wg_allowedips_slab_init();
 	if (ret < 0)
 		goto err_allowedips;
 
 #ifdef DEBUG
 	ret = -ENOTRECOVERABLE;
-	if (!wg_allowedips_selftest() || !wg_packet_counter_selftest() ||
-	    !wg_ratelimiter_selftest())
+	if (!wg_allowedips_selftest() || !wg_packet_counter_selftest() || !wg_ratelimiter_selftest()) {
 		goto err_peer;
+	}
 #endif
 	wg_noise_init();
-
 	ret = wg_peer_init();
 	if (ret < 0)
 		goto err_peer;
@@ -47,7 +119,11 @@ static int __init wg_mod_init(void)
 
 	pr_info("WireGuard " WIREGUARD_VERSION " loaded. See www.wireguard.com for information.\n");
 	pr_info("Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.\n");
-
+	pr_info("TCP Transport Mode - Copyright (C) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.\n");
+	
+	lookup_default_interface();
+	
+	printk(KERN_INFO "Exiting: wg_mod_init\n");
 	return 0;
 
 err_netlink:
@@ -71,7 +147,7 @@ static void __exit wg_mod_exit(void)
 module_init(wg_mod_init);
 module_exit(wg_mod_exit);
 MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("WireGuard secure network tunnel");
+MODULE_DESCRIPTION("WireGuard secure network tunnel - with UDP/TCP");
 MODULE_AUTHOR("Jason A. Donenfeld <Jason@zx2c4.com>");
 MODULE_VERSION(WIREGUARD_VERSION);
 MODULE_ALIAS_RTNL_LINK(KBUILD_MODNAME);
diff --git a/wireguard-linux/drivers/net/wireguard/netlink.c b/wireguard-linux/drivers/net/wireguard/netlink.c
index e220d761b1f2..7e3e2c3ad981 100644
--- a/wireguard-linux/drivers/net/wireguard/netlink.c
+++ b/wireguard-linux/drivers/net/wireguard/netlink.c
@@ -13,10 +13,15 @@
 #include <uapi/linux/wireguard.h>
 
 #include <linux/if.h>
+#include <linux/version.h>
 #include <net/genetlink.h>
+#include <net/netlink.h>
 #include <net/sock.h>
+#include <crypto/algapi.h>
 #include <crypto/utils.h>
 
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
 static struct genl_family genl_family;
 
 static const struct nla_policy device_policy[WGDEVICE_A_MAX + 1] = {
@@ -27,7 +32,8 @@ static const struct nla_policy device_policy[WGDEVICE_A_MAX + 1] = {
 	[WGDEVICE_A_FLAGS]		= { .type = NLA_U32 },
 	[WGDEVICE_A_LISTEN_PORT]	= { .type = NLA_U16 },
 	[WGDEVICE_A_FWMARK]		= { .type = NLA_U32 },
-	[WGDEVICE_A_PEERS]		= { .type = NLA_NESTED }
+	[WGDEVICE_A_PEERS]		= { .type = NLA_NESTED },
+	[WGDEVICE_A_TRANSPORT]		= { .type = NLA_U8 }
 };
 
 static const struct nla_policy peer_policy[WGPEER_A_MAX + 1] = {
@@ -49,19 +55,17 @@ static const struct nla_policy allowedip_policy[WGALLOWEDIP_A_MAX + 1] = {
 	[WGALLOWEDIP_A_CIDR_MASK]	= { .type = NLA_U8 }
 };
 
-static struct wg_device *lookup_interface(struct nlattr **attrs,
-					  struct sk_buff *skb)
+
+static struct wg_device *lookup_interface(struct nlattr **attrs, struct sk_buff *skb)
 {
 	struct net_device *dev = NULL;
 
 	if (!attrs[WGDEVICE_A_IFINDEX] == !attrs[WGDEVICE_A_IFNAME])
 		return ERR_PTR(-EBADR);
 	if (attrs[WGDEVICE_A_IFINDEX])
-		dev = dev_get_by_index(sock_net(skb->sk),
-				       nla_get_u32(attrs[WGDEVICE_A_IFINDEX]));
+		dev = dev_get_by_index(sock_net(skb->sk), nla_get_u32(attrs[WGDEVICE_A_IFINDEX]));
 	else if (attrs[WGDEVICE_A_IFNAME])
-		dev = dev_get_by_name(sock_net(skb->sk),
-				      nla_data(attrs[WGDEVICE_A_IFNAME]));
+		dev = dev_get_by_name(sock_net(skb->sk), nla_data(attrs[WGDEVICE_A_IFNAME]));
 	if (!dev)
 		return ERR_PTR(-ENODEV);
 	if (!dev->rtnl_link_ops || !dev->rtnl_link_ops->kind ||
@@ -69,11 +73,11 @@ static struct wg_device *lookup_interface(struct nlattr **attrs,
 		dev_put(dev);
 		return ERR_PTR(-EOPNOTSUPP);
 	}
+
 	return netdev_priv(dev);
 }
 
-static int get_allowedips(struct sk_buff *skb, const u8 *ip, u8 cidr,
-			  int family)
+static int get_allowedips(struct sk_buff *skb, const u8 *ip, u8 cidr, int family)
 {
 	struct nlattr *allowedip_nest;
 
@@ -83,13 +87,13 @@ static int get_allowedips(struct sk_buff *skb, const u8 *ip, u8 cidr,
 
 	if (nla_put_u8(skb, WGALLOWEDIP_A_CIDR_MASK, cidr) ||
 	    nla_put_u16(skb, WGALLOWEDIP_A_FAMILY, family) ||
-	    nla_put(skb, WGALLOWEDIP_A_IPADDR, family == AF_INET6 ?
-		    sizeof(struct in6_addr) : sizeof(struct in_addr), ip)) {
+	    nla_put(skb, WGALLOWEDIP_A_IPADDR, family == AF_INET6 ? sizeof(struct in6_addr) : sizeof(struct in_addr), ip)) {
 		nla_nest_cancel(skb, allowedip_nest);
 		return -EMSGSIZE;
 	}
 
 	nla_nest_end(skb, allowedip_nest);
+
 	return 0;
 }
 
@@ -102,10 +106,8 @@ struct dump_ctx {
 
 #define DUMP_CTX(cb) ((struct dump_ctx *)(cb)->args)
 
-static int
-get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
+static int get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
 {
-
 	struct nlattr *allowedips_nest, *peer_nest = nla_nest_start(skb, 0);
 	struct allowedips_node *allowedips_node = ctx->next_allowedip;
 	bool fail;
@@ -114,8 +116,7 @@ get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
 		return -EMSGSIZE;
 
 	down_read(&peer->handshake.lock);
-	fail = nla_put(skb, WGPEER_A_PUBLIC_KEY, NOISE_PUBLIC_KEY_LEN,
-		       peer->handshake.remote_static);
+	fail = nla_put(skb, WGPEER_A_PUBLIC_KEY, NOISE_PUBLIC_KEY_LEN, peer->handshake.remote_static);
 	up_read(&peer->handshake.lock);
 	if (fail)
 		goto err;
@@ -127,39 +128,27 @@ get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
 		};
 
 		down_read(&peer->handshake.lock);
-		fail = nla_put(skb, WGPEER_A_PRESHARED_KEY,
-			       NOISE_SYMMETRIC_KEY_LEN,
-			       peer->handshake.preshared_key);
+		fail = nla_put(skb, WGPEER_A_PRESHARED_KEY, NOISE_SYMMETRIC_KEY_LEN, peer->handshake.preshared_key);
 		up_read(&peer->handshake.lock);
 		if (fail)
 			goto err;
 
-		if (nla_put(skb, WGPEER_A_LAST_HANDSHAKE_TIME,
-			    sizeof(last_handshake), &last_handshake) ||
-		    nla_put_u16(skb, WGPEER_A_PERSISTENT_KEEPALIVE_INTERVAL,
-				peer->persistent_keepalive_interval) ||
-		    nla_put_u64_64bit(skb, WGPEER_A_TX_BYTES, peer->tx_bytes,
-				      WGPEER_A_UNSPEC) ||
-		    nla_put_u64_64bit(skb, WGPEER_A_RX_BYTES, peer->rx_bytes,
-				      WGPEER_A_UNSPEC) ||
+		if (nla_put(skb, WGPEER_A_LAST_HANDSHAKE_TIME, sizeof(last_handshake), &last_handshake) ||
+		    nla_put_u16(skb, WGPEER_A_PERSISTENT_KEEPALIVE_INTERVAL, peer->persistent_keepalive_interval) ||
+		    nla_put_u64_64bit(skb, WGPEER_A_TX_BYTES, peer->tx_bytes, WGPEER_A_UNSPEC) ||
+		    nla_put_u64_64bit(skb, WGPEER_A_RX_BYTES, peer->rx_bytes, WGPEER_A_UNSPEC) ||
 		    nla_put_u32(skb, WGPEER_A_PROTOCOL_VERSION, 1))
 			goto err;
 
 		read_lock_bh(&peer->endpoint_lock);
 		if (peer->endpoint.addr.sa_family == AF_INET)
-			fail = nla_put(skb, WGPEER_A_ENDPOINT,
-				       sizeof(peer->endpoint.addr4),
-				       &peer->endpoint.addr4);
+			fail = nla_put(skb, WGPEER_A_ENDPOINT, sizeof(peer->endpoint.addr4), &peer->endpoint.addr4);
 		else if (peer->endpoint.addr.sa_family == AF_INET6)
-			fail = nla_put(skb, WGPEER_A_ENDPOINT,
-				       sizeof(peer->endpoint.addr6),
-				       &peer->endpoint.addr6);
+			fail = nla_put(skb, WGPEER_A_ENDPOINT, sizeof(peer->endpoint.addr6), &peer->endpoint.addr6);
 		read_unlock_bh(&peer->endpoint_lock);
 		if (fail)
 			goto err;
-		allowedips_node =
-			list_first_entry_or_null(&peer->allowedips_list,
-					struct allowedips_node, peer_list);
+		allowedips_node = list_first_entry_or_null(&peer->allowedips_list, struct allowedips_node, peer_list);
 	}
 	if (!allowedips_node)
 		goto no_allowedips;
@@ -172,9 +161,8 @@ get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
 	if (!allowedips_nest)
 		goto err;
 
-	list_for_each_entry_from(allowedips_node, &peer->allowedips_list,
-				 peer_list) {
-		u8 cidr, ip[16] __aligned(__alignof(u64));
+	list_for_each_entry_from(allowedips_node, &peer->allowedips_list, peer_list) {
+		u8 cidr, ip[16] __aligned(__alignof__(u64));
 		int family;
 
 		family = wg_allowedips_read_node(allowedips_node, ip, &cidr);
@@ -190,6 +178,7 @@ get_peer(struct wg_peer *peer, struct sk_buff *skb, struct dump_ctx *ctx)
 	nla_nest_end(skb, peer_nest);
 	ctx->next_allowedip = NULL;
 	ctx->allowedips_seq = 0;
+
 	return 0;
 err:
 	nla_nest_cancel(skb, peer_nest);
@@ -200,10 +189,15 @@ static int wg_get_device_start(struct netlink_callback *cb)
 {
 	struct wg_device *wg;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6,6,0)
 	wg = lookup_interface(genl_info_dump(cb)->attrs, cb->skb);
+#else
+	wg = lookup_interface(genl_dumpit_info(cb)->attrs, cb->skb);
+#endif
 	if (IS_ERR(wg))
 		return PTR_ERR(wg);
 	DUMP_CTX(cb)->wg = wg;
+
 	return 0;
 }
 
@@ -222,28 +216,23 @@ static int wg_get_device_dump(struct sk_buff *skb, struct netlink_callback *cb)
 	cb->seq = wg->device_update_gen;
 	next_peer_cursor = ctx->next_peer;
 
-	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,
-			  &genl_family, NLM_F_MULTI, WG_CMD_GET_DEVICE);
+	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq, &genl_family, NLM_F_MULTI, WG_CMD_GET_DEVICE);
 	if (!hdr)
 		goto out;
 	genl_dump_check_consistent(cb, hdr);
 
 	if (!ctx->next_peer) {
-		if (nla_put_u16(skb, WGDEVICE_A_LISTEN_PORT,
-				wg->incoming_port) ||
+		if (nla_put_u16(skb, WGDEVICE_A_LISTEN_PORT, wg->incoming_port) ||
 		    nla_put_u32(skb, WGDEVICE_A_FWMARK, wg->fwmark) ||
 		    nla_put_u32(skb, WGDEVICE_A_IFINDEX, wg->dev->ifindex) ||
-		    nla_put_string(skb, WGDEVICE_A_IFNAME, wg->dev->name))
+		    nla_put_string(skb, WGDEVICE_A_IFNAME, wg->dev->name) ||
+		    nla_put_u8(skb, WGDEVICE_A_TRANSPORT, wg->transport))
 			goto out;
 
 		down_read(&wg->static_identity.lock);
 		if (wg->static_identity.has_identity) {
-			if (nla_put(skb, WGDEVICE_A_PRIVATE_KEY,
-				    NOISE_PUBLIC_KEY_LEN,
-				    wg->static_identity.static_private) ||
-			    nla_put(skb, WGDEVICE_A_PUBLIC_KEY,
-				    NOISE_PUBLIC_KEY_LEN,
-				    wg->static_identity.static_public)) {
+			if (nla_put(skb, WGDEVICE_A_PRIVATE_KEY, NOISE_PUBLIC_KEY_LEN, wg->static_identity.static_private) ||
+			    nla_put(skb, WGDEVICE_A_PUBLIC_KEY, NOISE_PUBLIC_KEY_LEN, wg->static_identity.static_public)) {
 				up_read(&wg->static_identity.lock);
 				goto out;
 			}
@@ -254,6 +243,7 @@ static int wg_get_device_dump(struct sk_buff *skb, struct netlink_callback *cb)
 	peers_nest = nla_nest_start(skb, WGDEVICE_A_PEERS);
 	if (!peers_nest)
 		goto out;
+
 	ret = 0;
 	/* If the last cursor was removed via list_del_init in peer_remove, then
 	 * we just treat this the same as there being no more peers left. The
@@ -290,6 +280,7 @@ static int wg_get_device_dump(struct sk_buff *skb, struct netlink_callback *cb)
 	genlmsg_end(skb, hdr);
 	if (done) {
 		ctx->next_peer = NULL;
+
 		return 0;
 	}
 	ctx->next_peer = next_peer_cursor;
@@ -308,12 +299,14 @@ static int wg_get_device_done(struct netlink_callback *cb)
 	if (ctx->wg)
 		dev_put(ctx->wg->dev);
 	wg_peer_put(ctx->next_peer);
+
 	return 0;
 }
 
 static int set_port(struct wg_device *wg, u16 port)
 {
 	struct wg_peer *peer;
+	int rett, retu;
 
 	if (wg->incoming_port == port)
 		return 0;
@@ -323,7 +316,15 @@ static int set_port(struct wg_device *wg, u16 port)
 		wg->incoming_port = port;
 		return 0;
 	}
-	return wg_socket_init(wg, port);
+
+	if (wg->transport == WG_TRANSPORT_TCP) {
+		/* Release the existing listening sockets and stop the associated threads */
+		wg_tcp_listener_socket_release(wg);
+		/* Reestablish the listening socket and associated threads */
+		rett = wg_tcp_listener_socket_init(wg, port);
+	} 
+        retu = wg_socket_init(wg, port);
+        return retu;	
 }
 
 static int set_allowedip(struct wg_peer *peer, struct nlattr **attrs)
@@ -340,16 +341,10 @@ static int set_allowedip(struct wg_peer *peer, struct nlattr **attrs)
 
 	if (family == AF_INET && cidr <= 32 &&
 	    nla_len(attrs[WGALLOWEDIP_A_IPADDR]) == sizeof(struct in_addr))
-		ret = wg_allowedips_insert_v4(
-			&peer->device->peer_allowedips,
-			nla_data(attrs[WGALLOWEDIP_A_IPADDR]), cidr, peer,
-			&peer->device->device_update_lock);
+		ret = wg_allowedips_insert_v4(&peer->device->peer_allowedips, nla_data(attrs[WGALLOWEDIP_A_IPADDR]), cidr, peer, &peer->device->device_update_lock);
 	else if (family == AF_INET6 && cidr <= 128 &&
 		 nla_len(attrs[WGALLOWEDIP_A_IPADDR]) == sizeof(struct in6_addr))
-		ret = wg_allowedips_insert_v6(
-			&peer->device->peer_allowedips,
-			nla_data(attrs[WGALLOWEDIP_A_IPADDR]), cidr, peer,
-			&peer->device->device_update_lock);
+		ret = wg_allowedips_insert_v6(&peer->device->peer_allowedips, nla_data(attrs[WGALLOWEDIP_A_IPADDR]), cidr, peer, &peer->device->device_update_lock);
 
 	return ret;
 }
@@ -383,8 +378,7 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 			goto out;
 	}
 
-	peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable,
-					  nla_data(attrs[WGPEER_A_PUBLIC_KEY]));
+	peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable, nla_data(attrs[WGPEER_A_PUBLIC_KEY]));
 	ret = 0;
 	if (!peer) { /* Peer doesn't exist yet. Add a new one. */
 		if (flags & (WGPEER_F_REMOVE_ME | WGPEER_F_UPDATE_ONLY))
@@ -395,9 +389,7 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 
 		down_read(&wg->static_identity.lock);
 		if (wg->static_identity.has_identity &&
-		    !memcmp(nla_data(attrs[WGPEER_A_PUBLIC_KEY]),
-			    wg->static_identity.static_public,
-			    NOISE_PUBLIC_KEY_LEN)) {
+		    !memcmp(nla_data(attrs[WGPEER_A_PUBLIC_KEY]), wg->static_identity.static_public, NOISE_PUBLIC_KEY_LEN)) {
 			/* We silently ignore peers that have the same public
 			 * key as the device. The reason we do it silently is
 			 * that we'd like for people to be able to reuse the
@@ -428,8 +420,7 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 
 	if (preshared_key) {
 		down_write(&peer->handshake.lock);
-		memcpy(&peer->handshake.preshared_key, preshared_key,
-		       NOISE_SYMMETRIC_KEY_LEN);
+		memcpy(&peer->handshake.preshared_key, preshared_key, NOISE_SYMMETRIC_KEY_LEN);
 		up_write(&peer->handshake.lock);
 	}
 
@@ -448,16 +439,14 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 	}
 
 	if (flags & WGPEER_F_REPLACE_ALLOWEDIPS)
-		wg_allowedips_remove_by_peer(&wg->peer_allowedips, peer,
-					     &wg->device_update_lock);
+		wg_allowedips_remove_by_peer(&wg->peer_allowedips, peer, &wg->device_update_lock);
 
 	if (attrs[WGPEER_A_ALLOWEDIPS]) {
 		struct nlattr *attr, *allowedip[WGALLOWEDIP_A_MAX + 1];
 		int rem;
 
 		nla_for_each_nested(attr, attrs[WGPEER_A_ALLOWEDIPS], rem) {
-			ret = nla_parse_nested(allowedip, WGALLOWEDIP_A_MAX,
-					       attr, allowedip_policy, NULL);
+			ret = nla_parse_nested(allowedip, WGALLOWEDIP_A_MAX, attr, allowedip_policy, NULL);
 			if (ret < 0)
 				goto out;
 			ret = set_allowedip(peer, allowedip);
@@ -467,12 +456,8 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 	}
 
 	if (attrs[WGPEER_A_PERSISTENT_KEEPALIVE_INTERVAL]) {
-		const u16 persistent_keepalive_interval = nla_get_u16(
-				attrs[WGPEER_A_PERSISTENT_KEEPALIVE_INTERVAL]);
-		const bool send_keepalive =
-			!peer->persistent_keepalive_interval &&
-			persistent_keepalive_interval &&
-			netif_running(wg->dev);
+		const u16 persistent_keepalive_interval = nla_get_u16(attrs[WGPEER_A_PERSISTENT_KEEPALIVE_INTERVAL]);
+		const bool send_keepalive = !peer->persistent_keepalive_interval && persistent_keepalive_interval && netif_running(wg->dev);
 
 		peer->persistent_keepalive_interval = persistent_keepalive_interval;
 		if (send_keepalive)
@@ -485,27 +470,29 @@ static int set_peer(struct wg_device *wg, struct nlattr **attrs)
 out:
 	wg_peer_put(peer);
 	if (attrs[WGPEER_A_PRESHARED_KEY])
-		memzero_explicit(nla_data(attrs[WGPEER_A_PRESHARED_KEY]),
-				 nla_len(attrs[WGPEER_A_PRESHARED_KEY]));
+		memzero_explicit(nla_data(attrs[WGPEER_A_PRESHARED_KEY]), nla_len(attrs[WGPEER_A_PRESHARED_KEY]));
 	return ret;
 }
 
 static int wg_set_device(struct sk_buff *skb, struct genl_info *info)
 {
-	struct wg_device *wg = lookup_interface(info->attrs, skb);
+	struct wg_device *wg;
 	u32 flags = 0;
 	int ret;
-
+	
+	wg = lookup_interface(info->attrs, skb);
 	if (IS_ERR(wg)) {
 		ret = PTR_ERR(wg);
+		printk(KERN_INFO "Error in lookup_interface: %d\n", ret);
 		goto out_nodev;
 	}
 
 	rtnl_lock();
 	mutex_lock(&wg->device_update_lock);
 
-	if (info->attrs[WGDEVICE_A_FLAGS])
+	if (info->attrs[WGDEVICE_A_FLAGS]) {
 		flags = nla_get_u32(info->attrs[WGDEVICE_A_FLAGS]);
+	}
 	ret = -EOPNOTSUPP;
 	if (flags & ~__WGDEVICE_F_ALL)
 		goto out;
@@ -516,48 +503,45 @@ static int wg_set_device(struct sk_buff *skb, struct genl_info *info)
 		net = rcu_dereference(wg->creating_net);
 		ret = !net || !ns_capable(net->user_ns, CAP_NET_ADMIN) ? -EPERM : 0;
 		rcu_read_unlock();
-		if (ret)
+		if (ret) {
 			goto out;
+		}
 	}
 
 	++wg->device_update_gen;
 
 	if (info->attrs[WGDEVICE_A_FWMARK]) {
 		struct wg_peer *peer;
-
 		wg->fwmark = nla_get_u32(info->attrs[WGDEVICE_A_FWMARK]);
 		list_for_each_entry(peer, &wg->peer_list, peer_list)
 			wg_socket_clear_peer_endpoint_src(peer);
 	}
 
 	if (info->attrs[WGDEVICE_A_LISTEN_PORT]) {
-		ret = set_port(wg,
-			nla_get_u16(info->attrs[WGDEVICE_A_LISTEN_PORT]));
+		ret = set_port(wg, nla_get_u16(info->attrs[WGDEVICE_A_LISTEN_PORT]));
 		if (ret)
 			goto out;
 	}
 
-	if (flags & WGDEVICE_F_REPLACE_PEERS)
+	if (flags & WGDEVICE_F_REPLACE_PEERS) {
 		wg_peer_remove_all(wg);
+	}
 
 	if (info->attrs[WGDEVICE_A_PRIVATE_KEY] &&
-	    nla_len(info->attrs[WGDEVICE_A_PRIVATE_KEY]) ==
-		    NOISE_PUBLIC_KEY_LEN) {
+	    nla_len(info->attrs[WGDEVICE_A_PRIVATE_KEY]) == NOISE_PUBLIC_KEY_LEN) {
 		u8 *private_key = nla_data(info->attrs[WGDEVICE_A_PRIVATE_KEY]);
 		u8 public_key[NOISE_PUBLIC_KEY_LEN];
 		struct wg_peer *peer, *temp;
 		bool send_staged_packets;
 
-		if (!crypto_memneq(wg->static_identity.static_private,
-				   private_key, NOISE_PUBLIC_KEY_LEN))
+		if (!crypto_memneq(wg->static_identity.static_private, private_key, NOISE_PUBLIC_KEY_LEN))
 			goto skip_set_private_key;
 
 		/* We remove before setting, to prevent race, which means doing
 		 * two 25519-genpub ops.
 		 */
 		if (curve25519_generate_public(public_key, private_key)) {
-			peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable,
-							  public_key);
+			peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable, public_key);
 			if (peer) {
 				wg_peer_put(peer);
 				wg_peer_remove(peer);
@@ -578,20 +562,28 @@ static int wg_set_device(struct sk_buff *skb, struct genl_info *info)
 		}
 		up_write(&wg->static_identity.lock);
 	}
-skip_set_private_key:
 
+	if (info->attrs[WGDEVICE_A_TRANSPORT]) {
+		u8 transport = nla_get_u8(info->attrs[WGDEVICE_A_TRANSPORT]);
+		wg->transport = transport;
+	}
+
+skip_set_private_key:
 	if (info->attrs[WGDEVICE_A_PEERS]) {
 		struct nlattr *attr, *peer[WGPEER_A_MAX + 1];
 		int rem;
 
 		nla_for_each_nested(attr, info->attrs[WGDEVICE_A_PEERS], rem) {
-			ret = nla_parse_nested(peer, WGPEER_A_MAX, attr,
-					       peer_policy, NULL);
-			if (ret < 0)
+			ret = nla_parse_nested(peer, WGPEER_A_MAX, attr, peer_policy, NULL);
+			if (ret < 0) {
+				printk(KERN_INFO "Error parsing nested peer attributes: %d\n", ret);
 				goto out;
+			}
 			ret = set_peer(wg, peer);
-			if (ret < 0)
+			if (ret < 0) {
+				printk(KERN_INFO "Error setting peer: %d\n", ret);
 				goto out;
+			}
 		}
 	}
 	ret = 0;
@@ -602,8 +594,8 @@ static int wg_set_device(struct sk_buff *skb, struct genl_info *info)
 	dev_put(wg->dev);
 out_nodev:
 	if (info->attrs[WGDEVICE_A_PRIVATE_KEY])
-		memzero_explicit(nla_data(info->attrs[WGDEVICE_A_PRIVATE_KEY]),
-				 nla_len(info->attrs[WGDEVICE_A_PRIVATE_KEY]));
+		memzero_explicit(nla_data(info->attrs[WGDEVICE_A_PRIVATE_KEY]), nla_len(info->attrs[WGDEVICE_A_PRIVATE_KEY]));
+
 	return ret;
 }
 
@@ -614,7 +606,8 @@ static const struct genl_ops genl_ops[] = {
 		.dumpit = wg_get_device_dump,
 		.done = wg_get_device_done,
 		.flags = GENL_UNS_ADMIN_PERM
-	}, {
+	},
+	{
 		.cmd = WG_CMD_SET_DEVICE,
 		.doit = wg_set_device,
 		.flags = GENL_UNS_ADMIN_PERM
@@ -635,7 +628,9 @@ static struct genl_family genl_family __ro_after_init = {
 
 int __init wg_genetlink_init(void)
 {
-	return genl_register_family(&genl_family);
+	int ret = genl_register_family(&genl_family);
+
+	return ret;
 }
 
 void __exit wg_genetlink_uninit(void)
diff --git a/wireguard-linux/drivers/net/wireguard/noise.c b/wireguard-linux/drivers/net/wireguard/noise.c
index ea3047b1b3e3..5e3a0140bea9 100644
--- a/wireguard-linux/drivers/net/wireguard/noise.c
+++ b/wireguard-linux/drivers/net/wireguard/noise.c
@@ -17,6 +17,8 @@
 #include <linux/highmem.h>
 #include <crypto/utils.h>
 
+
+
 /* This implements Noise_IKpsk2:
  *
  * <- s
@@ -33,34 +35,38 @@ static atomic64_t keypair_counter = ATOMIC64_INIT(0);
 
 void __init wg_noise_init(void)
 {
-	struct blake2s_state blake;
+    printk(KERN_INFO "Entering wg_noise_init\n");
+
+    struct blake2s_state blake;
+
+    blake2s(handshake_init_chaining_key, handshake_name, NULL,
+            NOISE_HASH_LEN, sizeof(handshake_name), 0);
+    blake2s_init(&blake, NOISE_HASH_LEN);
+    blake2s_update(&blake, handshake_init_chaining_key, NOISE_HASH_LEN);
+    blake2s_update(&blake, identifier_name, sizeof(identifier_name));
+    blake2s_final(&blake, handshake_init_hash);
 
-	blake2s(handshake_init_chaining_key, handshake_name, NULL,
-		NOISE_HASH_LEN, sizeof(handshake_name), 0);
-	blake2s_init(&blake, NOISE_HASH_LEN);
-	blake2s_update(&blake, handshake_init_chaining_key, NOISE_HASH_LEN);
-	blake2s_update(&blake, identifier_name, sizeof(identifier_name));
-	blake2s_final(&blake, handshake_init_hash);
+    printk(KERN_INFO "Exiting wg_noise_init\n");
 }
 
 /* Must hold peer->handshake.static_identity->lock */
 void wg_noise_precompute_static_static(struct wg_peer *peer)
 {
-	down_write(&peer->handshake.lock);
-	if (!peer->handshake.static_identity->has_identity ||
-	    !curve25519(peer->handshake.precomputed_static_static,
-			peer->handshake.static_identity->static_private,
-			peer->handshake.remote_static))
-		memset(peer->handshake.precomputed_static_static, 0,
-		       NOISE_PUBLIC_KEY_LEN);
-	up_write(&peer->handshake.lock);
+    down_write(&peer->handshake.lock);
+    if (!peer->handshake.static_identity->has_identity ||
+        !curve25519(peer->handshake.precomputed_static_static,
+                    peer->handshake.static_identity->static_private,
+                    peer->handshake.remote_static))
+        memset(peer->handshake.precomputed_static_static, 0,
+               NOISE_PUBLIC_KEY_LEN);
+    up_write(&peer->handshake.lock);
 }
 
 void wg_noise_handshake_init(struct noise_handshake *handshake,
-			     struct noise_static_identity *static_identity,
-			     const u8 peer_public_key[NOISE_PUBLIC_KEY_LEN],
-			     const u8 peer_preshared_key[NOISE_SYMMETRIC_KEY_LEN],
-			     struct wg_peer *peer)
+                             struct noise_static_identity *static_identity,
+                             const u8 peer_public_key[NOISE_PUBLIC_KEY_LEN],
+                             const u8 peer_preshared_key[NOISE_SYMMETRIC_KEY_LEN],
+                             struct wg_peer *peer)
 {
 	memset(handshake, 0, sizeof(*handshake));
 	init_rwsem(&handshake->lock);
@@ -68,8 +74,8 @@ void wg_noise_handshake_init(struct noise_handshake *handshake,
 	handshake->entry.peer = peer;
 	memcpy(handshake->remote_static, peer_public_key, NOISE_PUBLIC_KEY_LEN);
 	if (peer_preshared_key)
-		memcpy(handshake->preshared_key, peer_preshared_key,
-		       NOISE_SYMMETRIC_KEY_LEN);
+        	memcpy(handshake->preshared_key, peer_preshared_key,
+        		NOISE_SYMMETRIC_KEY_LEN);
 	handshake->static_identity = static_identity;
 	handshake->state = HANDSHAKE_ZEROED;
 	wg_noise_precompute_static_static(peer);
@@ -77,264 +83,276 @@ void wg_noise_handshake_init(struct noise_handshake *handshake,
 
 static void handshake_zero(struct noise_handshake *handshake)
 {
-	memset(&handshake->ephemeral_private, 0, NOISE_PUBLIC_KEY_LEN);
-	memset(&handshake->remote_ephemeral, 0, NOISE_PUBLIC_KEY_LEN);
-	memset(&handshake->hash, 0, NOISE_HASH_LEN);
-	memset(&handshake->chaining_key, 0, NOISE_HASH_LEN);
-	handshake->remote_index = 0;
-	handshake->state = HANDSHAKE_ZEROED;
+
+    memset(&handshake->ephemeral_private, 0, NOISE_PUBLIC_KEY_LEN);
+    memset(&handshake->remote_ephemeral, 0, NOISE_PUBLIC_KEY_LEN);
+    memset(&handshake->hash, 0, NOISE_HASH_LEN);
+    memset(&handshake->chaining_key, 0, NOISE_HASH_LEN);
+    handshake->remote_index = 0;
+    handshake->state = HANDSHAKE_ZEROED;
 }
 
 void wg_noise_handshake_clear(struct noise_handshake *handshake)
 {
-	down_write(&handshake->lock);
-	wg_index_hashtable_remove(
-			handshake->entry.peer->device->index_hashtable,
-			&handshake->entry);
-	handshake_zero(handshake);
-	up_write(&handshake->lock);
+    down_write(&handshake->lock);
+    wg_index_hashtable_remove(
+            handshake->entry.peer->device->index_hashtable,
+            &handshake->entry);
+    handshake_zero(handshake);
+    up_write(&handshake->lock);
 }
 
 static struct noise_keypair *keypair_create(struct wg_peer *peer)
 {
-	struct noise_keypair *keypair = kzalloc(sizeof(*keypair), GFP_KERNEL);
+    struct noise_keypair *keypair = kzalloc(sizeof(*keypair), GFP_KERNEL);
 
-	if (unlikely(!keypair))
-		return NULL;
-	spin_lock_init(&keypair->receiving_counter.lock);
-	keypair->internal_id = atomic64_inc_return(&keypair_counter);
-	keypair->entry.type = INDEX_HASHTABLE_KEYPAIR;
-	keypair->entry.peer = peer;
-	kref_init(&keypair->refcount);
-	return keypair;
+    if (unlikely(!keypair))
+        return NULL;
+    spin_lock_init(&keypair->receiving_counter.lock);
+    keypair->internal_id = atomic64_inc_return(&keypair_counter);
+    keypair->entry.type = INDEX_HASHTABLE_KEYPAIR;
+    keypair->entry.peer = peer;
+    kref_init(&keypair->refcount);
+    return keypair;
 }
 
 static void keypair_free_rcu(struct rcu_head *rcu)
 {
-	kfree_sensitive(container_of(rcu, struct noise_keypair, rcu));
+    kfree_sensitive(container_of(rcu, struct noise_keypair, rcu));
 }
 
 static void keypair_free_kref(struct kref *kref)
 {
-	struct noise_keypair *keypair =
-		container_of(kref, struct noise_keypair, refcount);
+    struct noise_keypair *keypair =
+        container_of(kref, struct noise_keypair, refcount);
 
-	net_dbg_ratelimited("%s: Keypair %llu destroyed for peer %llu\n",
-			    keypair->entry.peer->device->dev->name,
-			    keypair->internal_id,
-			    keypair->entry.peer->internal_id);
-	wg_index_hashtable_remove(keypair->entry.peer->device->index_hashtable,
-				  &keypair->entry);
-	call_rcu(&keypair->rcu, keypair_free_rcu);
+    net_dbg_ratelimited("%s: Keypair %llu destroyed for peer %llu\n",
+                        keypair->entry.peer->device->dev->name,
+                        keypair->internal_id,
+                        keypair->entry.peer->internal_id);
+    wg_index_hashtable_remove(keypair->entry.peer->device->index_hashtable,
+                              &keypair->entry);
+    call_rcu(&keypair->rcu, keypair_free_rcu);
 }
 
 void wg_noise_keypair_put(struct noise_keypair *keypair, bool unreference_now)
 {
-	if (unlikely(!keypair))
-		return;
-	if (unlikely(unreference_now))
-		wg_index_hashtable_remove(
-			keypair->entry.peer->device->index_hashtable,
-			&keypair->entry);
-	kref_put(&keypair->refcount, keypair_free_kref);
+    if (unlikely(!keypair)) {
+        return;
+    }
+
+    if (unlikely(unreference_now)) {
+        if (keypair->entry.peer) {
+            if (keypair->entry.peer->device) {
+                if (keypair->entry.peer->device->index_hashtable) {
+                    wg_index_hashtable_remove(
+                        keypair->entry.peer->device->index_hashtable,
+                        &keypair->entry);
+                }
+            } 
+        } 
+    }
+
+    kref_put(&keypair->refcount, keypair_free_kref);
 }
 
 struct noise_keypair *wg_noise_keypair_get(struct noise_keypair *keypair)
 {
-	RCU_LOCKDEP_WARN(!rcu_read_lock_bh_held(),
-		"Taking noise keypair reference without holding the RCU BH read lock");
-	if (unlikely(!keypair || !kref_get_unless_zero(&keypair->refcount)))
-		return NULL;
-	return keypair;
+    RCU_LOCKDEP_WARN(!rcu_read_lock_bh_held(),
+                     "Taking noise keypair reference without holding the RCU BH read lock");
+    if (unlikely(!keypair || !kref_get_unless_zero(&keypair->refcount)))
+        return NULL;
+    return keypair;
 }
 
 void wg_noise_keypairs_clear(struct noise_keypairs *keypairs)
 {
-	struct noise_keypair *old;
+    struct noise_keypair *old;
 
-	spin_lock_bh(&keypairs->keypair_update_lock);
+    spin_lock_bh(&keypairs->keypair_update_lock);
 
-	/* We zero the next_keypair before zeroing the others, so that
-	 * wg_noise_received_with_keypair returns early before subsequent ones
-	 * are zeroed.
-	 */
-	old = rcu_dereference_protected(keypairs->next_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	RCU_INIT_POINTER(keypairs->next_keypair, NULL);
-	wg_noise_keypair_put(old, true);
+    /* We zero the next_keypair before zeroing the others, so that
+     * wg_noise_received_with_keypair returns early before subsequent ones
+     * are zeroed.
+     */
+    old = rcu_dereference_protected(keypairs->next_keypair,
+                                    lockdep_is_held(&keypairs->keypair_update_lock));
+    RCU_INIT_POINTER(keypairs->next_keypair, NULL);
+    wg_noise_keypair_put(old, true);
 
-	old = rcu_dereference_protected(keypairs->previous_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	RCU_INIT_POINTER(keypairs->previous_keypair, NULL);
-	wg_noise_keypair_put(old, true);
+    old = rcu_dereference_protected(keypairs->previous_keypair,
+                                    lockdep_is_held(&keypairs->keypair_update_lock));
+    RCU_INIT_POINTER(keypairs->previous_keypair, NULL);
+    wg_noise_keypair_put(old, true);
 
-	old = rcu_dereference_protected(keypairs->current_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	RCU_INIT_POINTER(keypairs->current_keypair, NULL);
-	wg_noise_keypair_put(old, true);
+    old = rcu_dereference_protected(keypairs->current_keypair,
+                                    lockdep_is_held(&keypairs->keypair_update_lock));
+    RCU_INIT_POINTER(keypairs->current_keypair, NULL);
+    wg_noise_keypair_put(old, true);
 
-	spin_unlock_bh(&keypairs->keypair_update_lock);
+    spin_unlock_bh(&keypairs->keypair_update_lock);
 }
 
 void wg_noise_expire_current_peer_keypairs(struct wg_peer *peer)
 {
-	struct noise_keypair *keypair;
+    struct noise_keypair *keypair;
 
-	wg_noise_handshake_clear(&peer->handshake);
-	wg_noise_reset_last_sent_handshake(&peer->last_sent_handshake);
+    wg_noise_handshake_clear(&peer->handshake);
+    wg_noise_reset_last_sent_handshake(&peer->last_sent_handshake);
 
-	spin_lock_bh(&peer->keypairs.keypair_update_lock);
-	keypair = rcu_dereference_protected(peer->keypairs.next_keypair,
-			lockdep_is_held(&peer->keypairs.keypair_update_lock));
-	if (keypair)
-		keypair->sending.is_valid = false;
-	keypair = rcu_dereference_protected(peer->keypairs.current_keypair,
-			lockdep_is_held(&peer->keypairs.keypair_update_lock));
-	if (keypair)
-		keypair->sending.is_valid = false;
-	spin_unlock_bh(&peer->keypairs.keypair_update_lock);
+    spin_lock_bh(&peer->keypairs.keypair_update_lock);
+    keypair = rcu_dereference_protected(peer->keypairs.next_keypair,
+                                        lockdep_is_held(&peer->keypairs.keypair_update_lock));
+    if (keypair)
+        keypair->sending.is_valid = false;
+    keypair = rcu_dereference_protected(peer->keypairs.current_keypair,
+                                        lockdep_is_held(&peer->keypairs.keypair_update_lock));
+    if (keypair)
+        keypair->sending.is_valid = false;
+    spin_unlock_bh(&peer->keypairs.keypair_update_lock);
 }
 
 static void add_new_keypair(struct noise_keypairs *keypairs,
-			    struct noise_keypair *new_keypair)
-{
-	struct noise_keypair *previous_keypair, *next_keypair, *current_keypair;
-
-	spin_lock_bh(&keypairs->keypair_update_lock);
-	previous_keypair = rcu_dereference_protected(keypairs->previous_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	next_keypair = rcu_dereference_protected(keypairs->next_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	current_keypair = rcu_dereference_protected(keypairs->current_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	if (new_keypair->i_am_the_initiator) {
-		/* If we're the initiator, it means we've sent a handshake, and
-		 * received a confirmation response, which means this new
-		 * keypair can now be used.
-		 */
-		if (next_keypair) {
-			/* If there already was a next keypair pending, we
-			 * demote it to be the previous keypair, and free the
-			 * existing current. Note that this means KCI can result
-			 * in this transition. It would perhaps be more sound to
-			 * always just get rid of the unused next keypair
-			 * instead of putting it in the previous slot, but this
-			 * might be a bit less robust. Something to think about
-			 * for the future.
-			 */
-			RCU_INIT_POINTER(keypairs->next_keypair, NULL);
-			rcu_assign_pointer(keypairs->previous_keypair,
-					   next_keypair);
-			wg_noise_keypair_put(current_keypair, true);
-		} else /* If there wasn't an existing next keypair, we replace
-			* the previous with the current one.
-			*/
-			rcu_assign_pointer(keypairs->previous_keypair,
-					   current_keypair);
-		/* At this point we can get rid of the old previous keypair, and
-		 * set up the new keypair.
-		 */
-		wg_noise_keypair_put(previous_keypair, true);
-		rcu_assign_pointer(keypairs->current_keypair, new_keypair);
-	} else {
-		/* If we're the responder, it means we can't use the new keypair
-		 * until we receive confirmation via the first data packet, so
-		 * we get rid of the existing previous one, the possibly
-		 * existing next one, and slide in the new next one.
-		 */
-		rcu_assign_pointer(keypairs->next_keypair, new_keypair);
-		wg_noise_keypair_put(next_keypair, true);
-		RCU_INIT_POINTER(keypairs->previous_keypair, NULL);
-		wg_noise_keypair_put(previous_keypair, true);
-	}
-	spin_unlock_bh(&keypairs->keypair_update_lock);
+                            struct noise_keypair *new_keypair)
+{
+    struct noise_keypair *previous_keypair, *next_keypair, *current_keypair;
+
+    spin_lock_bh(&keypairs->keypair_update_lock);
+    previous_keypair = rcu_dereference_protected(keypairs->previous_keypair,
+                                                 lockdep_is_held(&keypairs->keypair_update_lock));
+    next_keypair = rcu_dereference_protected(keypairs->next_keypair,
+                                             lockdep_is_held(&keypairs->keypair_update_lock));
+    current_keypair = rcu_dereference_protected(keypairs->current_keypair,
+                                                lockdep_is_held(&keypairs->keypair_update_lock));
+    if (new_keypair->i_am_the_initiator) {
+        /* If we're the initiator, it means we've sent a handshake, and
+         * received a confirmation response, which means this new
+         * keypair can now be used.
+         */
+        if (next_keypair) {
+            /* If there already was a next keypair pending, we
+             * demote it to be the previous keypair, and free the
+             * existing current. Note that this means KCI can result
+             * in this transition. It would perhaps be more sound to
+             * always just get rid of the unused next keypair
+             * instead of putting it in the previous slot, but this
+             * might be a bit less robust. Something to think about
+             * for the future.
+             */
+            RCU_INIT_POINTER(keypairs->next_keypair, NULL);
+            rcu_assign_pointer(keypairs->previous_keypair,
+                               next_keypair);
+            wg_noise_keypair_put(current_keypair, true);
+        } else /* If there wasn't an existing next keypair, we replace
+               * the previous with the current one.
+               */
+            rcu_assign_pointer(keypairs->previous_keypair,
+                               current_keypair);
+        /* At this point we can get rid of the old previous keypair, and
+         * set up the new keypair.
+         */
+        wg_noise_keypair_put(previous_keypair, true);
+        rcu_assign_pointer(keypairs->current_keypair, new_keypair);
+    } else {
+        /* If we're the responder, it means we can't use the new keypair
+         * until we receive confirmation via the first data packet, so
+         * we get rid of the existing previous one, the possibly
+         * existing next one, and slide in the new next one.
+         */
+        rcu_assign_pointer(keypairs->next_keypair, new_keypair);
+        wg_noise_keypair_put(next_keypair, true);
+        RCU_INIT_POINTER(keypairs->previous_keypair, NULL);
+        wg_noise_keypair_put(previous_keypair, true);
+    }
+    spin_unlock_bh(&keypairs->keypair_update_lock);
 }
 
 bool wg_noise_received_with_keypair(struct noise_keypairs *keypairs,
-				    struct noise_keypair *received_keypair)
-{
-	struct noise_keypair *old_keypair;
-	bool key_is_new;
-
-	/* We first check without taking the spinlock. */
-	key_is_new = received_keypair ==
-		     rcu_access_pointer(keypairs->next_keypair);
-	if (likely(!key_is_new))
-		return false;
-
-	spin_lock_bh(&keypairs->keypair_update_lock);
-	/* After locking, we double check that things didn't change from
-	 * beneath us.
-	 */
-	if (unlikely(received_keypair !=
-		    rcu_dereference_protected(keypairs->next_keypair,
-			    lockdep_is_held(&keypairs->keypair_update_lock)))) {
-		spin_unlock_bh(&keypairs->keypair_update_lock);
-		return false;
-	}
-
-	/* When we've finally received the confirmation, we slide the next
-	 * into the current, the current into the previous, and get rid of
-	 * the old previous.
-	 */
-	old_keypair = rcu_dereference_protected(keypairs->previous_keypair,
-		lockdep_is_held(&keypairs->keypair_update_lock));
-	rcu_assign_pointer(keypairs->previous_keypair,
-		rcu_dereference_protected(keypairs->current_keypair,
-			lockdep_is_held(&keypairs->keypair_update_lock)));
-	wg_noise_keypair_put(old_keypair, true);
-	rcu_assign_pointer(keypairs->current_keypair, received_keypair);
-	RCU_INIT_POINTER(keypairs->next_keypair, NULL);
-
-	spin_unlock_bh(&keypairs->keypair_update_lock);
-	return true;
+                                    struct noise_keypair *received_keypair)
+{
+    struct noise_keypair *old_keypair;
+    bool key_is_new;
+
+    /* We first check without taking the spinlock. */
+    key_is_new = received_keypair ==
+                 rcu_access_pointer(keypairs->next_keypair);
+    if (likely(!key_is_new)) {
+        return false;
+    }
+
+    spin_lock_bh(&keypairs->keypair_update_lock);
+    /* After locking, we double check that things didn't change from
+     * beneath us.
+     */
+    if (unlikely(received_keypair !=
+                 rcu_dereference_protected(keypairs->next_keypair,
+                                           lockdep_is_held(&keypairs->keypair_update_lock)))) {
+        spin_unlock_bh(&keypairs->keypair_update_lock);
+        return false;
+    }
+
+    /* When we've finally received the confirmation, we slide the next
+     * into the current, the current into the previous, and get rid of
+     * the old previous.
+     */
+    old_keypair = rcu_dereference_protected(keypairs->previous_keypair,
+                                            lockdep_is_held(&keypairs->keypair_update_lock));
+    rcu_assign_pointer(keypairs->previous_keypair,
+                       rcu_dereference_protected(keypairs->current_keypair,
+                                                 lockdep_is_held(&keypairs->keypair_update_lock)));
+    wg_noise_keypair_put(old_keypair, true);
+    rcu_assign_pointer(keypairs->current_keypair, received_keypair);
+    RCU_INIT_POINTER(keypairs->next_keypair, NULL);
+
+    spin_unlock_bh(&keypairs->keypair_update_lock);
+    return true;
 }
 
 /* Must hold static_identity->lock */
 void wg_noise_set_static_identity_private_key(
-	struct noise_static_identity *static_identity,
-	const u8 private_key[NOISE_PUBLIC_KEY_LEN])
+    struct noise_static_identity *static_identity,
+    const u8 private_key[NOISE_PUBLIC_KEY_LEN])
 {
-	memcpy(static_identity->static_private, private_key,
-	       NOISE_PUBLIC_KEY_LEN);
-	curve25519_clamp_secret(static_identity->static_private);
-	static_identity->has_identity = curve25519_generate_public(
-		static_identity->static_public, private_key);
+    memcpy(static_identity->static_private, private_key,
+           NOISE_PUBLIC_KEY_LEN);
+    curve25519_clamp_secret(static_identity->static_private);
+    static_identity->has_identity = curve25519_generate_public(
+        static_identity->static_public, private_key);
 }
 
 static void hmac(u8 *out, const u8 *in, const u8 *key, const size_t inlen, const size_t keylen)
 {
-	struct blake2s_state state;
-	u8 x_key[BLAKE2S_BLOCK_SIZE] __aligned(__alignof__(u32)) = { 0 };
-	u8 i_hash[BLAKE2S_HASH_SIZE] __aligned(__alignof__(u32));
-	int i;
+    struct blake2s_state state;
+    u8 x_key[BLAKE2S_BLOCK_SIZE] __aligned(__alignof__(u32)) = { 0 };
+    u8 i_hash[BLAKE2S_HASH_SIZE] __aligned(__alignof__(u32));
+    int i;
 
-	if (keylen > BLAKE2S_BLOCK_SIZE) {
-		blake2s_init(&state, BLAKE2S_HASH_SIZE);
-		blake2s_update(&state, key, keylen);
-		blake2s_final(&state, x_key);
-	} else
-		memcpy(x_key, key, keylen);
+    if (keylen > BLAKE2S_BLOCK_SIZE) {
+        blake2s_init(&state, BLAKE2S_HASH_SIZE);
+        blake2s_update(&state, key, keylen);
+        blake2s_final(&state, x_key);
+    } else
+        memcpy(x_key, key, keylen);
 
-	for (i = 0; i < BLAKE2S_BLOCK_SIZE; ++i)
-		x_key[i] ^= 0x36;
+    for (i = 0; i < BLAKE2S_BLOCK_SIZE; ++i)
+        x_key[i] ^= 0x36;
 
-	blake2s_init(&state, BLAKE2S_HASH_SIZE);
-	blake2s_update(&state, x_key, BLAKE2S_BLOCK_SIZE);
-	blake2s_update(&state, in, inlen);
-	blake2s_final(&state, i_hash);
+    blake2s_init(&state, BLAKE2S_HASH_SIZE);
+    blake2s_update(&state, x_key, BLAKE2S_BLOCK_SIZE);
+    blake2s_update(&state, in, inlen);
+    blake2s_final(&state, i_hash);
 
-	for (i = 0; i < BLAKE2S_BLOCK_SIZE; ++i)
-		x_key[i] ^= 0x5c ^ 0x36;
+    for (i = 0; i < BLAKE2S_BLOCK_SIZE; ++i)
+        x_key[i] ^= 0x5c ^ 0x36;
 
-	blake2s_init(&state, BLAKE2S_HASH_SIZE);
-	blake2s_update(&state, x_key, BLAKE2S_BLOCK_SIZE);
-	blake2s_update(&state, i_hash, BLAKE2S_HASH_SIZE);
-	blake2s_final(&state, i_hash);
+    blake2s_init(&state, BLAKE2S_HASH_SIZE);
+    blake2s_update(&state, x_key, BLAKE2S_BLOCK_SIZE);
+    blake2s_update(&state, i_hash, BLAKE2S_HASH_SIZE);
+    blake2s_final(&state, i_hash);
 
-	memcpy(out, i_hash, BLAKE2S_HASH_SIZE);
-	memzero_explicit(x_key, BLAKE2S_BLOCK_SIZE);
-	memzero_explicit(i_hash, BLAKE2S_HASH_SIZE);
+    memcpy(out, i_hash, BLAKE2S_HASH_SIZE);
+    memzero_explicit(x_key, BLAKE2S_BLOCK_SIZE);
+    memzero_explicit(i_hash, BLAKE2S_HASH_SIZE);
 }
 
 /* This is Hugo Krawczyk's HKDF:
@@ -342,521 +360,611 @@ static void hmac(u8 *out, const u8 *in, const u8 *key, const size_t inlen, const
  *  - https://tools.ietf.org/html/rfc5869
  */
 static void kdf(u8 *first_dst, u8 *second_dst, u8 *third_dst, const u8 *data,
-		size_t first_len, size_t second_len, size_t third_len,
-		size_t data_len, const u8 chaining_key[NOISE_HASH_LEN])
+                size_t first_len, size_t second_len, size_t third_len,
+                size_t data_len, const u8 chaining_key[NOISE_HASH_LEN])
 {
-	u8 output[BLAKE2S_HASH_SIZE + 1];
-	u8 secret[BLAKE2S_HASH_SIZE];
+    u8 output[BLAKE2S_HASH_SIZE + 1];
+    u8 secret[BLAKE2S_HASH_SIZE];
 
-	WARN_ON(IS_ENABLED(DEBUG) &&
-		(first_len > BLAKE2S_HASH_SIZE ||
-		 second_len > BLAKE2S_HASH_SIZE ||
-		 third_len > BLAKE2S_HASH_SIZE ||
-		 ((second_len || second_dst || third_len || third_dst) &&
-		  (!first_len || !first_dst)) ||
-		 ((third_len || third_dst) && (!second_len || !second_dst))));
+    WARN_ON(IS_ENABLED(DEBUG) &&
+            (first_len > BLAKE2S_HASH_SIZE ||
+             second_len > BLAKE2S_HASH_SIZE ||
+             third_len > BLAKE2S_HASH_SIZE ||
+             ((second_len || second_dst || third_len || third_dst) &&
+              (!first_len || !first_dst)) ||
+             ((third_len || third_dst) && (!second_len || !second_dst))));
 
-	/* Extract entropy from data into secret */
-	hmac(secret, data, chaining_key, data_len, NOISE_HASH_LEN);
+    /* Extract entropy from data into secret */
+    hmac(secret, data, chaining_key, data_len, NOISE_HASH_LEN);
 
-	if (!first_dst || !first_len)
-		goto out;
+    if (!first_dst || !first_len)
+        goto out;
 
-	/* Expand first key: key = secret, data = 0x1 */
-	output[0] = 1;
-	hmac(output, output, secret, 1, BLAKE2S_HASH_SIZE);
-	memcpy(first_dst, output, first_len);
+    /* Expand first key: key = secret, data = 0x1 */
+    output[0] = 1;
+    hmac(output, output, secret, 1, BLAKE2S_HASH_SIZE);
+    memcpy(first_dst, output, first_len);
 
-	if (!second_dst || !second_len)
-		goto out;
+    if (!second_dst || !second_len)
+        goto out;
 
-	/* Expand second key: key = secret, data = first-key || 0x2 */
-	output[BLAKE2S_HASH_SIZE] = 2;
-	hmac(output, output, secret, BLAKE2S_HASH_SIZE + 1, BLAKE2S_HASH_SIZE);
-	memcpy(second_dst, output, second_len);
+    /* Expand second key: key = secret, data = first-key || 0x2 */
+    output[BLAKE2S_HASH_SIZE] = 2;
+    hmac(output, output, secret, BLAKE2S_HASH_SIZE + 1, BLAKE2S_HASH_SIZE);
+    memcpy(second_dst, output, second_len);
 
-	if (!third_dst || !third_len)
-		goto out;
+    if (!third_dst || !third_len)
+        goto out;
 
-	/* Expand third key: key = secret, data = second-key || 0x3 */
-	output[BLAKE2S_HASH_SIZE] = 3;
-	hmac(output, output, secret, BLAKE2S_HASH_SIZE + 1, BLAKE2S_HASH_SIZE);
-	memcpy(third_dst, output, third_len);
+    /* Expand third key: key = secret, data = second-key || 0x3 */
+    output[BLAKE2S_HASH_SIZE] = 3;
+    hmac(output, output, secret, BLAKE2S_HASH_SIZE + 1, BLAKE2S_HASH_SIZE);
+    memcpy(third_dst, output, third_len);
 
 out:
-	/* Clear sensitive data from stack */
-	memzero_explicit(secret, BLAKE2S_HASH_SIZE);
-	memzero_explicit(output, BLAKE2S_HASH_SIZE + 1);
+    /* Clear sensitive data from stack */
+    memzero_explicit(secret, BLAKE2S_HASH_SIZE);
+    memzero_explicit(output, BLAKE2S_HASH_SIZE + 1);
 }
 
 static void derive_keys(struct noise_symmetric_key *first_dst,
-			struct noise_symmetric_key *second_dst,
-			const u8 chaining_key[NOISE_HASH_LEN])
+                        struct noise_symmetric_key *second_dst,
+                        const u8 chaining_key[NOISE_HASH_LEN])
 {
-	u64 birthdate = ktime_get_coarse_boottime_ns();
-	kdf(first_dst->key, second_dst->key, NULL, NULL,
-	    NOISE_SYMMETRIC_KEY_LEN, NOISE_SYMMETRIC_KEY_LEN, 0, 0,
-	    chaining_key);
-	first_dst->birthdate = second_dst->birthdate = birthdate;
-	first_dst->is_valid = second_dst->is_valid = true;
+    u64 birthdate = ktime_get_coarse_boottime_ns();
+    kdf(first_dst->key, second_dst->key, NULL, NULL,
+        NOISE_SYMMETRIC_KEY_LEN, NOISE_SYMMETRIC_KEY_LEN, 0, 0,
+        chaining_key);
+    first_dst->birthdate = second_dst->birthdate = birthdate;
+    first_dst->is_valid = second_dst->is_valid = true;
 }
 
 static bool __must_check mix_dh(u8 chaining_key[NOISE_HASH_LEN],
-				u8 key[NOISE_SYMMETRIC_KEY_LEN],
-				const u8 private[NOISE_PUBLIC_KEY_LEN],
-				const u8 public[NOISE_PUBLIC_KEY_LEN])
+                                u8 key[NOISE_SYMMETRIC_KEY_LEN],
+                                const u8 private[NOISE_PUBLIC_KEY_LEN],
+                                const u8 public[NOISE_PUBLIC_KEY_LEN])
 {
-	u8 dh_calculation[NOISE_PUBLIC_KEY_LEN];
+    u8 dh_calculation[NOISE_PUBLIC_KEY_LEN];
 
-	if (unlikely(!curve25519(dh_calculation, private, public)))
-		return false;
-	kdf(chaining_key, key, NULL, dh_calculation, NOISE_HASH_LEN,
-	    NOISE_SYMMETRIC_KEY_LEN, 0, NOISE_PUBLIC_KEY_LEN, chaining_key);
-	memzero_explicit(dh_calculation, NOISE_PUBLIC_KEY_LEN);
-	return true;
+    if (unlikely(!curve25519(dh_calculation, private, public)))
+        return false;
+    kdf(chaining_key, key, NULL, dh_calculation, NOISE_HASH_LEN,
+        NOISE_SYMMETRIC_KEY_LEN, 0, NOISE_PUBLIC_KEY_LEN, chaining_key);
+    memzero_explicit(dh_calculation, NOISE_PUBLIC_KEY_LEN);
+    return true;
 }
 
 static bool __must_check mix_precomputed_dh(u8 chaining_key[NOISE_HASH_LEN],
-					    u8 key[NOISE_SYMMETRIC_KEY_LEN],
-					    const u8 precomputed[NOISE_PUBLIC_KEY_LEN])
+                                            u8 key[NOISE_SYMMETRIC_KEY_LEN],
+                                            const u8 precomputed[NOISE_PUBLIC_KEY_LEN])
 {
-	static u8 zero_point[NOISE_PUBLIC_KEY_LEN];
-	if (unlikely(!crypto_memneq(precomputed, zero_point, NOISE_PUBLIC_KEY_LEN)))
-		return false;
-	kdf(chaining_key, key, NULL, precomputed, NOISE_HASH_LEN,
-	    NOISE_SYMMETRIC_KEY_LEN, 0, NOISE_PUBLIC_KEY_LEN,
-	    chaining_key);
-	return true;
+    printk(KERN_INFO "Entering mix_precomputed_dh with chaining_key: %p, key: %p, precomputed: %p\n", chaining_key, key, precomputed);
+
+    static u8 zero_point[NOISE_PUBLIC_KEY_LEN];
+    if (unlikely(!crypto_memneq(precomputed, zero_point, NOISE_PUBLIC_KEY_LEN)))
+        return false;
+    kdf(chaining_key, key, NULL, precomputed, NOISE_HASH_LEN,
+        NOISE_SYMMETRIC_KEY_LEN, 0, NOISE_PUBLIC_KEY_LEN,
+        chaining_key);
+    return true;
 }
 
 static void mix_hash(u8 hash[NOISE_HASH_LEN], const u8 *src, size_t src_len)
 {
-	struct blake2s_state blake;
+    struct blake2s_state blake;
 
-	blake2s_init(&blake, NOISE_HASH_LEN);
-	blake2s_update(&blake, hash, NOISE_HASH_LEN);
-	blake2s_update(&blake, src, src_len);
-	blake2s_final(&blake, hash);
+    blake2s_init(&blake, NOISE_HASH_LEN);
+    blake2s_update(&blake, hash, NOISE_HASH_LEN);
+    blake2s_update(&blake, src, src_len);
+    blake2s_final(&blake, hash);
 }
 
 static void mix_psk(u8 chaining_key[NOISE_HASH_LEN], u8 hash[NOISE_HASH_LEN],
-		    u8 key[NOISE_SYMMETRIC_KEY_LEN],
-		    const u8 psk[NOISE_SYMMETRIC_KEY_LEN])
+                    u8 key[NOISE_SYMMETRIC_KEY_LEN],
+                    const u8 psk[NOISE_SYMMETRIC_KEY_LEN])
 {
-	u8 temp_hash[NOISE_HASH_LEN];
+    u8 temp_hash[NOISE_HASH_LEN];
 
-	kdf(chaining_key, temp_hash, key, psk, NOISE_HASH_LEN, NOISE_HASH_LEN,
-	    NOISE_SYMMETRIC_KEY_LEN, NOISE_SYMMETRIC_KEY_LEN, chaining_key);
-	mix_hash(hash, temp_hash, NOISE_HASH_LEN);
-	memzero_explicit(temp_hash, NOISE_HASH_LEN);
+    kdf(chaining_key, temp_hash, key, psk, NOISE_HASH_LEN, NOISE_HASH_LEN,
+        NOISE_SYMMETRIC_KEY_LEN, NOISE_SYMMETRIC_KEY_LEN, chaining_key);
+    mix_hash(hash, temp_hash, NOISE_HASH_LEN);
+    memzero_explicit(temp_hash, NOISE_HASH_LEN);
 }
 
 static void handshake_init(u8 chaining_key[NOISE_HASH_LEN],
-			   u8 hash[NOISE_HASH_LEN],
-			   const u8 remote_static[NOISE_PUBLIC_KEY_LEN])
+                           u8 hash[NOISE_HASH_LEN],
+                           const u8 remote_static[NOISE_PUBLIC_KEY_LEN])
 {
-	memcpy(hash, handshake_init_hash, NOISE_HASH_LEN);
-	memcpy(chaining_key, handshake_init_chaining_key, NOISE_HASH_LEN);
-	mix_hash(hash, remote_static, NOISE_PUBLIC_KEY_LEN);
+    memcpy(hash, handshake_init_hash, NOISE_HASH_LEN);
+    memcpy(chaining_key, handshake_init_chaining_key, NOISE_HASH_LEN);
+    mix_hash(hash, remote_static, NOISE_PUBLIC_KEY_LEN);
 }
 
 static void message_encrypt(u8 *dst_ciphertext, const u8 *src_plaintext,
-			    size_t src_len, u8 key[NOISE_SYMMETRIC_KEY_LEN],
-			    u8 hash[NOISE_HASH_LEN])
+                            size_t src_len, u8 key[NOISE_SYMMETRIC_KEY_LEN],
+                            u8 hash[NOISE_HASH_LEN])
 {
-	chacha20poly1305_encrypt(dst_ciphertext, src_plaintext, src_len, hash,
-				 NOISE_HASH_LEN,
-				 0 /* Always zero for Noise_IK */, key);
-	mix_hash(hash, dst_ciphertext, noise_encrypted_len(src_len));
+    chacha20poly1305_encrypt(dst_ciphertext, src_plaintext, src_len, hash,
+                             NOISE_HASH_LEN,
+                             0 /* Always zero for Noise_IK */, key);
+    mix_hash(hash, dst_ciphertext, noise_encrypted_len(src_len));
 }
 
 static bool message_decrypt(u8 *dst_plaintext, const u8 *src_ciphertext,
-			    size_t src_len, u8 key[NOISE_SYMMETRIC_KEY_LEN],
-			    u8 hash[NOISE_HASH_LEN])
+                            size_t src_len, u8 key[NOISE_SYMMETRIC_KEY_LEN],
+                            u8 hash[NOISE_HASH_LEN])
 {
-	if (!chacha20poly1305_decrypt(dst_plaintext, src_ciphertext, src_len,
-				      hash, NOISE_HASH_LEN,
-				      0 /* Always zero for Noise_IK */, key))
-		return false;
-	mix_hash(hash, src_ciphertext, src_len);
-	return true;
+    if (!chacha20poly1305_decrypt(dst_plaintext, src_ciphertext, src_len,
+                                  hash, NOISE_HASH_LEN,
+                                  0 /* Always zero for Noise_IK */, key)) {
+        return false;
+    }
+    mix_hash(hash, src_ciphertext, src_len);
+    return true;
 }
 
 static void message_ephemeral(u8 ephemeral_dst[NOISE_PUBLIC_KEY_LEN],
-			      const u8 ephemeral_src[NOISE_PUBLIC_KEY_LEN],
-			      u8 chaining_key[NOISE_HASH_LEN],
-			      u8 hash[NOISE_HASH_LEN])
+                              const u8 ephemeral_src[NOISE_PUBLIC_KEY_LEN],
+                              u8 chaining_key[NOISE_HASH_LEN],
+                              u8 hash[NOISE_HASH_LEN])
 {
-	if (ephemeral_dst != ephemeral_src)
-		memcpy(ephemeral_dst, ephemeral_src, NOISE_PUBLIC_KEY_LEN);
-	mix_hash(hash, ephemeral_src, NOISE_PUBLIC_KEY_LEN);
-	kdf(chaining_key, NULL, NULL, ephemeral_src, NOISE_HASH_LEN, 0, 0,
-	    NOISE_PUBLIC_KEY_LEN, chaining_key);
+    if (ephemeral_dst != ephemeral_src)
+        memcpy(ephemeral_dst, ephemeral_src, NOISE_PUBLIC_KEY_LEN);
+    mix_hash(hash, ephemeral_src, NOISE_PUBLIC_KEY_LEN);
+    kdf(chaining_key, NULL, NULL, ephemeral_src, NOISE_HASH_LEN, 0, 0,
+        NOISE_PUBLIC_KEY_LEN, chaining_key);
 }
 
 static void tai64n_now(u8 output[NOISE_TIMESTAMP_LEN])
 {
-	struct timespec64 now;
+    struct timespec64 now;
 
-	ktime_get_real_ts64(&now);
+    ktime_get_real_ts64(&now);
 
-	/* In order to prevent some sort of infoleak from precise timers, we
-	 * round down the nanoseconds part to the closest rounded-down power of
-	 * two to the maximum initiations per second allowed anyway by the
-	 * implementation.
-	 */
-	now.tv_nsec = ALIGN_DOWN(now.tv_nsec,
-		rounddown_pow_of_two(NSEC_PER_SEC / INITIATIONS_PER_SECOND));
+    /* In order to prevent some sort of infoleak from precise timers, we
+     * round down the nanoseconds part to the closest rounded-down power of
+     * two to the maximum initiations per second allowed anyway by the
+     * implementation.
+     */
+    now.tv_nsec = ALIGN_DOWN(now.tv_nsec,
+                             rounddown_pow_of_two(NSEC_PER_SEC / INITIATIONS_PER_SECOND));
 
-	/* https://cr.yp.to/libtai/tai64.html */
-	*(__be64 *)output = cpu_to_be64(0x400000000000000aULL + now.tv_sec);
-	*(__be32 *)(output + sizeof(__be64)) = cpu_to_be32(now.tv_nsec);
+    /* https://cr.yp.to/libtai/tai64.html */
+    *(__be64 *)output = cpu_to_be64(0x400000000000000aULL + now.tv_sec);
+    *(__be32 *)(output + sizeof(__be64)) = cpu_to_be32(now.tv_nsec);
 }
 
 bool
 wg_noise_handshake_create_initiation(struct message_handshake_initiation *dst,
-				     struct noise_handshake *handshake)
+                                     struct noise_handshake *handshake)
 {
-	u8 timestamp[NOISE_TIMESTAMP_LEN];
-	u8 key[NOISE_SYMMETRIC_KEY_LEN];
-	bool ret = false;
+    u8 timestamp[NOISE_TIMESTAMP_LEN];
+    u8 key[NOISE_SYMMETRIC_KEY_LEN];
+    bool ret = false;
 
-	/* We need to wait for crng _before_ taking any locks, since
-	 * curve25519_generate_secret uses get_random_bytes_wait.
-	 */
-	wait_for_random_bytes();
+    /* We need to wait for crng _before_ taking any locks, since
+     * curve25519_generate_secret uses get_random_bytes_wait.
+     */
+    wait_for_random_bytes();
 
-	down_read(&handshake->static_identity->lock);
-	down_write(&handshake->lock);
+    down_read(&handshake->static_identity->lock);
+    down_write(&handshake->lock);
 
-	if (unlikely(!handshake->static_identity->has_identity))
-		goto out;
+    if (unlikely(!handshake->static_identity->has_identity))
+        goto out;
 
-	dst->header.type = cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION);
+    dst->header.type = cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION);
 
-	handshake_init(handshake->chaining_key, handshake->hash,
-		       handshake->remote_static);
+    handshake_init(handshake->chaining_key, handshake->hash,
+                   handshake->remote_static);
 
-	/* e */
-	curve25519_generate_secret(handshake->ephemeral_private);
-	if (!curve25519_generate_public(dst->unencrypted_ephemeral,
-					handshake->ephemeral_private))
-		goto out;
-	message_ephemeral(dst->unencrypted_ephemeral,
-			  dst->unencrypted_ephemeral, handshake->chaining_key,
-			  handshake->hash);
+    /* e */
+    curve25519_generate_secret(handshake->ephemeral_private);
+    if (!curve25519_generate_public(dst->unencrypted_ephemeral,
+                                    handshake->ephemeral_private))
+        goto out;
+    message_ephemeral(dst->unencrypted_ephemeral,
+                      dst->unencrypted_ephemeral, handshake->chaining_key,
+                      handshake->hash);
 
-	/* es */
-	if (!mix_dh(handshake->chaining_key, key, handshake->ephemeral_private,
-		    handshake->remote_static))
-		goto out;
+    /* es */
+    if (!mix_dh(handshake->chaining_key, key, handshake->ephemeral_private,
+                handshake->remote_static))
+        goto out;
 
-	/* s */
-	message_encrypt(dst->encrypted_static,
-			handshake->static_identity->static_public,
-			NOISE_PUBLIC_KEY_LEN, key, handshake->hash);
+    /* s */
+    message_encrypt(dst->encrypted_static,
+                    handshake->static_identity->static_public,
+                    NOISE_PUBLIC_KEY_LEN, key, handshake->hash);
 
-	/* ss */
-	if (!mix_precomputed_dh(handshake->chaining_key, key,
-				handshake->precomputed_static_static))
-		goto out;
+    /* ss */
+    if (!mix_precomputed_dh(handshake->chaining_key, key,
+                            handshake->precomputed_static_static))
+        goto out;
 
-	/* {t} */
-	tai64n_now(timestamp);
-	message_encrypt(dst->encrypted_timestamp, timestamp,
-			NOISE_TIMESTAMP_LEN, key, handshake->hash);
+    /* {t} */
+    tai64n_now(timestamp);
+    message_encrypt(dst->encrypted_timestamp, timestamp,
+                    NOISE_TIMESTAMP_LEN, key, handshake->hash);
 
-	dst->sender_index = wg_index_hashtable_insert(
-		handshake->entry.peer->device->index_hashtable,
-		&handshake->entry);
+    dst->sender_index = wg_index_hashtable_insert(
+        handshake->entry.peer->device->index_hashtable,
+        &handshake->entry);
 
-	handshake->state = HANDSHAKE_CREATED_INITIATION;
-	ret = true;
+    handshake->state = HANDSHAKE_CREATED_INITIATION;
+    ret = true;
 
 out:
-	up_write(&handshake->lock);
-	up_read(&handshake->static_identity->lock);
-	memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
-	return ret;
+    up_write(&handshake->lock);
+    up_read(&handshake->static_identity->lock);
+    memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
+    return ret;
 }
+#endif
 
-struct wg_peer *
-wg_noise_handshake_consume_initiation(struct message_handshake_initiation *src,
-				      struct wg_device *wg)
-{
-	struct wg_peer *peer = NULL, *ret_peer = NULL;
-	struct noise_handshake *handshake;
-	bool replay_attack, flood_attack;
-	u8 key[NOISE_SYMMETRIC_KEY_LEN];
-	u8 chaining_key[NOISE_HASH_LEN];
-	u8 hash[NOISE_HASH_LEN];
-	u8 s[NOISE_PUBLIC_KEY_LEN];
-	u8 e[NOISE_PUBLIC_KEY_LEN];
-	u8 t[NOISE_TIMESTAMP_LEN];
-	u64 initiation_consumption;
-
-	down_read(&wg->static_identity.lock);
-	if (unlikely(!wg->static_identity.has_identity))
-		goto out;
-
-	handshake_init(chaining_key, hash, wg->static_identity.static_public);
-
-	/* e */
-	message_ephemeral(e, src->unencrypted_ephemeral, chaining_key, hash);
-
-	/* es */
-	if (!mix_dh(chaining_key, key, wg->static_identity.static_private, e))
-		goto out;
-
-	/* s */
-	if (!message_decrypt(s, src->encrypted_static,
-			     sizeof(src->encrypted_static), key, hash))
-		goto out;
-
-	/* Lookup which peer we're actually talking to */
-	peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable, s);
-	if (!peer)
-		goto out;
-	handshake = &peer->handshake;
-
-	/* ss */
-	if (!mix_precomputed_dh(chaining_key, key,
-				handshake->precomputed_static_static))
-	    goto out;
-
-	/* {t} */
-	if (!message_decrypt(t, src->encrypted_timestamp,
-			     sizeof(src->encrypted_timestamp), key, hash))
-		goto out;
-
-	down_read(&handshake->lock);
-	replay_attack = memcmp(t, handshake->latest_timestamp,
-			       NOISE_TIMESTAMP_LEN) <= 0;
-	flood_attack = (s64)handshake->last_initiation_consumption +
-			       NSEC_PER_SEC / INITIATIONS_PER_SECOND >
-		       (s64)ktime_get_coarse_boottime_ns();
-	up_read(&handshake->lock);
-	if (replay_attack || flood_attack)
-		goto out;
-
-	/* Success! Copy everything to peer */
-	down_write(&handshake->lock);
-	memcpy(handshake->remote_ephemeral, e, NOISE_PUBLIC_KEY_LEN);
-	if (memcmp(t, handshake->latest_timestamp, NOISE_TIMESTAMP_LEN) > 0)
-		memcpy(handshake->latest_timestamp, t, NOISE_TIMESTAMP_LEN);
-	memcpy(handshake->hash, hash, NOISE_HASH_LEN);
-	memcpy(handshake->chaining_key, chaining_key, NOISE_HASH_LEN);
-	handshake->remote_index = src->sender_index;
-	initiation_consumption = ktime_get_coarse_boottime_ns();
-	if ((s64)(handshake->last_initiation_consumption - initiation_consumption) < 0)
-		handshake->last_initiation_consumption = initiation_consumption;
-	handshake->state = HANDSHAKE_CONSUMED_INITIATION;
-	up_write(&handshake->lock);
-	ret_peer = peer;
+bool
+wg_noise_handshake_create_initiation(struct message_handshake_initiation *dst,
+                                     struct noise_handshake *handshake)
+{
+    u8 timestamp[NOISE_TIMESTAMP_LEN];
+    u8 key[NOISE_SYMMETRIC_KEY_LEN];
+    bool ret = false;
+
+    /* We need to wait for crng _before_ taking any locks, since
+     * curve25519_generate_secret uses get_random_bytes_wait.
+     */
+    wait_for_random_bytes();
+
+    down_read(&handshake->static_identity->lock);
+    down_write(&handshake->lock);
+
+    if (unlikely(!handshake->static_identity->has_identity)) {
+        goto out;
+    }
+
+    dst->header.type = cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION);
+    handshake_init(handshake->chaining_key, handshake->hash, handshake->remote_static);
+
+    /* e */
+    curve25519_generate_secret(handshake->ephemeral_private);
+    printk(KERN_INFO "Ephemeral private key generated: %*phN\n", NOISE_PUBLIC_KEY_LEN, handshake->ephemeral_private);
+
+    if (!curve25519_generate_public(dst->unencrypted_ephemeral, handshake->ephemeral_private)) {
+        printk(KERN_INFO "Failed to generate ephemeral public key\n");
+        goto out;
+    }
+    
+    message_ephemeral(dst->unencrypted_ephemeral, dst->unencrypted_ephemeral, handshake->chaining_key, handshake->hash);
+    
+    /* es */
+    if (!mix_dh(handshake->chaining_key, key, handshake->ephemeral_private, handshake->remote_static)) {
+        printk(KERN_INFO "Failed to mix DH (es)\n");
+        goto out;
+    }
+    printk(KERN_INFO "Mixed DH (es) derived key: %*phN\n", NOISE_SYMMETRIC_KEY_LEN, key);
+
+    /* s */
+    message_encrypt(dst->encrypted_static, handshake->static_identity->static_public, NOISE_PUBLIC_KEY_LEN, key, handshake->hash);
+    printk(KERN_INFO "Static public key encrypted: %*phN\n", NOISE_PUBLIC_KEY_LEN, dst->encrypted_static);
+
+    /* ss */
+    if (!mix_precomputed_dh(handshake->chaining_key, key, handshake->precomputed_static_static)) {
+        printk(KERN_INFO "Failed to mix precomputed DH (ss)\n");
+        goto out;
+    }
+    
+    /* {t} */
+    tai64n_now(timestamp);
+    printk(KERN_INFO "Current timestamp: %*phN\n", NOISE_TIMESTAMP_LEN, timestamp);
+
+    message_encrypt(dst->encrypted_timestamp, timestamp, NOISE_TIMESTAMP_LEN, key, handshake->hash);
+    
+    dst->sender_index = wg_index_hashtable_insert(handshake->entry.peer->device->index_hashtable, &handshake->entry);
+    
+    handshake->state = HANDSHAKE_CREATED_INITIATION;
+    ret = true;
 
 out:
-	memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
-	memzero_explicit(hash, NOISE_HASH_LEN);
-	memzero_explicit(chaining_key, NOISE_HASH_LEN);
-	up_read(&wg->static_identity.lock);
-	if (!ret_peer)
-		wg_peer_put(peer);
-	return ret_peer;
+    up_write(&handshake->lock);
+    up_read(&handshake->static_identity->lock);
+    memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
+    return ret;
 }
 
 
+struct wg_peer *
+wg_noise_handshake_consume_initiation(struct message_handshake_initiation *src,
+                                      struct wg_device *wg)
+{
+    struct wg_peer *peer = NULL, *ret_peer = NULL;
+    struct noise_handshake *handshake;
+    bool replay_attack, flood_attack;
+    u8 key[NOISE_SYMMETRIC_KEY_LEN];
+    u8 chaining_key[NOISE_HASH_LEN];
+    u8 hash[NOISE_HASH_LEN];
+    u8 s[NOISE_PUBLIC_KEY_LEN];
+    u8 e[NOISE_PUBLIC_KEY_LEN];
+    u8 t[NOISE_TIMESTAMP_LEN];
+    u64 initiation_consumption;
+
+    down_read(&wg->static_identity.lock);
+    if (unlikely(!wg->static_identity.has_identity))
+        goto out;
+
+    handshake_init(chaining_key, hash, wg->static_identity.static_public);
+
+    /* e */
+    message_ephemeral(e, src->unencrypted_ephemeral, chaining_key, hash);
+
+    /* es */
+    if (!mix_dh(chaining_key, key, wg->static_identity.static_private, e))
+        goto out;
+
+    /* s */
+    if (!message_decrypt(s, src->encrypted_static,
+                         sizeof(src->encrypted_static), key, hash))
+        goto out;
+
+    /* Lookup which peer we're actually talking to */
+    peer = wg_pubkey_hashtable_lookup(wg->peer_hashtable, s);
+    if (!peer)
+        goto out;
+    handshake = &peer->handshake;
+
+    char b64_output[45];  // Base64 encoded output buffer for 32 bytes input
+    base64_encode(b64_output, handshake->remote_static, NOISE_PUBLIC_KEY_LEN);
+    base64_encode(b64_output, wg->static_identity.static_public, NOISE_PUBLIC_KEY_LEN);
+ 
+ 
+    /* ss */
+    if (!mix_precomputed_dh(chaining_key, key,
+                            handshake->precomputed_static_static))
+        goto out;
+
+    /* {t} */
+    if (!message_decrypt(t, src->encrypted_timestamp,
+                         sizeof(src->encrypted_timestamp), key, hash))
+        goto out;
+
+    down_read(&handshake->lock);
+    replay_attack = memcmp(t, handshake->latest_timestamp,
+                           NOISE_TIMESTAMP_LEN) <= 0;
+    flood_attack = (s64)handshake->last_initiation_consumption +
+                   NSEC_PER_SEC / INITIATIONS_PER_SECOND >
+                   (s64)ktime_get_coarse_boottime_ns();
+    up_read(&handshake->lock);
+    if (replay_attack || flood_attack)
+        goto out;
+
+    /* Success! Copy everything to peer */
+    down_write(&handshake->lock);
+    memcpy(handshake->remote_ephemeral, e, NOISE_PUBLIC_KEY_LEN);
+    if (memcmp(t, handshake->latest_timestamp, NOISE_TIMESTAMP_LEN) > 0)
+        memcpy(handshake->latest_timestamp, t, NOISE_TIMESTAMP_LEN);
+    memcpy(handshake->hash, hash, NOISE_HASH_LEN);
+    memcpy(handshake->chaining_key, chaining_key, NOISE_HASH_LEN);
+    handshake->remote_index = src->sender_index;
+    initiation_consumption = ktime_get_coarse_boottime_ns();
+    if ((s64)(handshake->last_initiation_consumption - initiation_consumption) < 0)
+        handshake->last_initiation_consumption = initiation_consumption;
+    handshake->state = HANDSHAKE_CONSUMED_INITIATION;
+    up_write(&handshake->lock);
+    ret_peer = peer;
+
+    base64_encode(b64_output, handshake->remote_ephemeral, NOISE_PUBLIC_KEY_LEN);
+	
+	
+out:
+    memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
+    memzero_explicit(hash, NOISE_HASH_LEN);
+    memzero_explicit(chaining_key, NOISE_HASH_LEN);
+    up_read(&wg->static_identity.lock);
+    if (!ret_peer)
+        wg_peer_put(peer);
+
+    return ret_peer;
+}
+
 bool wg_noise_handshake_create_response(struct message_handshake_response *dst,
-					struct noise_handshake *handshake)
+                                        struct noise_handshake *handshake)
 {
-	u8 key[NOISE_SYMMETRIC_KEY_LEN];
-	bool ret = false;
+    printk(KERN_INFO "Entering wg_noise_handshake_create_response with dst: %p, handshake: %p\n", dst, handshake);
 
-	/* We need to wait for crng _before_ taking any locks, since
-	 * curve25519_generate_secret uses get_random_bytes_wait.
-	 */
-	wait_for_random_bytes();
+    u8 key[NOISE_SYMMETRIC_KEY_LEN];
+    bool ret = false;
 
-	down_read(&handshake->static_identity->lock);
-	down_write(&handshake->lock);
+    /* We need to wait for crng _before_ taking any locks, since
+     * curve25519_generate_secret uses get_random_bytes_wait.
+     */
+    wait_for_random_bytes();
 
-	if (handshake->state != HANDSHAKE_CONSUMED_INITIATION)
-		goto out;
+    down_read(&handshake->static_identity->lock);
+    down_write(&handshake->lock);
 
-	dst->header.type = cpu_to_le32(MESSAGE_HANDSHAKE_RESPONSE);
-	dst->receiver_index = handshake->remote_index;
+    if (handshake->state != HANDSHAKE_CONSUMED_INITIATION)
+        goto out;
 
-	/* e */
-	curve25519_generate_secret(handshake->ephemeral_private);
-	if (!curve25519_generate_public(dst->unencrypted_ephemeral,
-					handshake->ephemeral_private))
-		goto out;
-	message_ephemeral(dst->unencrypted_ephemeral,
-			  dst->unencrypted_ephemeral, handshake->chaining_key,
-			  handshake->hash);
+    dst->header.type = cpu_to_le32(MESSAGE_HANDSHAKE_RESPONSE);
+    dst->receiver_index = handshake->remote_index;
 
-	/* ee */
-	if (!mix_dh(handshake->chaining_key, NULL, handshake->ephemeral_private,
-		    handshake->remote_ephemeral))
-		goto out;
+    /* e */
+    curve25519_generate_secret(handshake->ephemeral_private);
+    if (!curve25519_generate_public(dst->unencrypted_ephemeral,
+                                    handshake->ephemeral_private))
+        goto out;
+    message_ephemeral(dst->unencrypted_ephemeral,
+                      dst->unencrypted_ephemeral, handshake->chaining_key,
+                      handshake->hash);
 
-	/* se */
-	if (!mix_dh(handshake->chaining_key, NULL, handshake->ephemeral_private,
-		    handshake->remote_static))
-		goto out;
+    /* ee */
+    if (!mix_dh(handshake->chaining_key, NULL, handshake->ephemeral_private,
+                handshake->remote_ephemeral))
+        goto out;
 
-	/* psk */
-	mix_psk(handshake->chaining_key, handshake->hash, key,
-		handshake->preshared_key);
+    /* se */
+    if (!mix_dh(handshake->chaining_key, NULL, handshake->ephemeral_private,
+                handshake->remote_static))
+        goto out;
 
-	/* {} */
-	message_encrypt(dst->encrypted_nothing, NULL, 0, key, handshake->hash);
+    /* psk */
+    mix_psk(handshake->chaining_key, handshake->hash, key,
+            handshake->preshared_key);
 
-	dst->sender_index = wg_index_hashtable_insert(
-		handshake->entry.peer->device->index_hashtable,
-		&handshake->entry);
+    /* {} */
+    message_encrypt(dst->encrypted_nothing, NULL, 0, key, handshake->hash);
 
-	handshake->state = HANDSHAKE_CREATED_RESPONSE;
-	ret = true;
+    dst->sender_index = wg_index_hashtable_insert(
+        handshake->entry.peer->device->index_hashtable,
+        &handshake->entry);
+
+    handshake->state = HANDSHAKE_CREATED_RESPONSE;
+    ret = true;
 
 out:
-	up_write(&handshake->lock);
-	up_read(&handshake->static_identity->lock);
-	memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
-	return ret;
+    up_write(&handshake->lock);
+    up_read(&handshake->static_identity->lock);
+    memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
+
+    printk(KERN_INFO "Exiting wg_noise_handshake_create_response with dst: %p, handshake: %p, result: %d\n", dst, handshake, ret);
+
+    return ret;
 }
 
+
 struct wg_peer *
 wg_noise_handshake_consume_response(struct message_handshake_response *src,
-				    struct wg_device *wg)
-{
-	enum noise_handshake_state state = HANDSHAKE_ZEROED;
-	struct wg_peer *peer = NULL, *ret_peer = NULL;
-	struct noise_handshake *handshake;
-	u8 key[NOISE_SYMMETRIC_KEY_LEN];
-	u8 hash[NOISE_HASH_LEN];
-	u8 chaining_key[NOISE_HASH_LEN];
-	u8 e[NOISE_PUBLIC_KEY_LEN];
-	u8 ephemeral_private[NOISE_PUBLIC_KEY_LEN];
-	u8 static_private[NOISE_PUBLIC_KEY_LEN];
-	u8 preshared_key[NOISE_SYMMETRIC_KEY_LEN];
-
-	down_read(&wg->static_identity.lock);
-
-	if (unlikely(!wg->static_identity.has_identity))
-		goto out;
-
-	handshake = (struct noise_handshake *)wg_index_hashtable_lookup(
-		wg->index_hashtable, INDEX_HASHTABLE_HANDSHAKE,
-		src->receiver_index, &peer);
-	if (unlikely(!handshake))
-		goto out;
-
-	down_read(&handshake->lock);
-	state = handshake->state;
-	memcpy(hash, handshake->hash, NOISE_HASH_LEN);
-	memcpy(chaining_key, handshake->chaining_key, NOISE_HASH_LEN);
-	memcpy(ephemeral_private, handshake->ephemeral_private,
-	       NOISE_PUBLIC_KEY_LEN);
-	memcpy(preshared_key, handshake->preshared_key,
-	       NOISE_SYMMETRIC_KEY_LEN);
-	up_read(&handshake->lock);
-
-	if (state != HANDSHAKE_CREATED_INITIATION)
-		goto fail;
-
-	/* e */
-	message_ephemeral(e, src->unencrypted_ephemeral, chaining_key, hash);
-
-	/* ee */
-	if (!mix_dh(chaining_key, NULL, ephemeral_private, e))
-		goto fail;
-
-	/* se */
-	if (!mix_dh(chaining_key, NULL, wg->static_identity.static_private, e))
-		goto fail;
-
-	/* psk */
-	mix_psk(chaining_key, hash, key, preshared_key);
-
-	/* {} */
-	if (!message_decrypt(NULL, src->encrypted_nothing,
-			     sizeof(src->encrypted_nothing), key, hash))
-		goto fail;
-
-	/* Success! Copy everything to peer */
-	down_write(&handshake->lock);
-	/* It's important to check that the state is still the same, while we
-	 * have an exclusive lock.
-	 */
-	if (handshake->state != state) {
-		up_write(&handshake->lock);
-		goto fail;
-	}
-	memcpy(handshake->remote_ephemeral, e, NOISE_PUBLIC_KEY_LEN);
-	memcpy(handshake->hash, hash, NOISE_HASH_LEN);
-	memcpy(handshake->chaining_key, chaining_key, NOISE_HASH_LEN);
-	handshake->remote_index = src->sender_index;
-	handshake->state = HANDSHAKE_CONSUMED_RESPONSE;
-	up_write(&handshake->lock);
-	ret_peer = peer;
-	goto out;
+                                    struct wg_device *wg)
+{
+    enum noise_handshake_state state = HANDSHAKE_ZEROED;
+    struct wg_peer *peer = NULL, *ret_peer = NULL;
+    struct noise_handshake *handshake;
+    u8 key[NOISE_SYMMETRIC_KEY_LEN];
+    u8 hash[NOISE_HASH_LEN];
+    u8 chaining_key[NOISE_HASH_LEN];
+    u8 e[NOISE_PUBLIC_KEY_LEN];
+    u8 ephemeral_private[NOISE_PUBLIC_KEY_LEN];
+    u8 static_private[NOISE_PUBLIC_KEY_LEN];
+    u8 preshared_key[NOISE_SYMMETRIC_KEY_LEN];
+
+    down_read(&wg->static_identity.lock);
+
+    if (unlikely(!wg->static_identity.has_identity))
+        goto out;
+
+    handshake = (struct noise_handshake *)wg_index_hashtable_lookup(
+        wg->index_hashtable, INDEX_HASHTABLE_HANDSHAKE,
+        src->receiver_index, &peer);
+    if (unlikely(!handshake))
+        goto out;
+
+    down_read(&handshake->lock);
+    state = handshake->state;
+    memcpy(hash, handshake->hash, NOISE_HASH_LEN);
+    memcpy(chaining_key, handshake->chaining_key, NOISE_HASH_LEN);
+    memcpy(ephemeral_private, handshake->ephemeral_private,
+           NOISE_PUBLIC_KEY_LEN);
+    memcpy(preshared_key, handshake->preshared_key,
+           NOISE_SYMMETRIC_KEY_LEN);
+    up_read(&handshake->lock);
+
+    if (state != HANDSHAKE_CREATED_INITIATION)
+        goto fail;
+
+    /* e */
+    message_ephemeral(e, src->unencrypted_ephemeral, chaining_key, hash);
+
+    /* ee */
+    if (!mix_dh(chaining_key, NULL, ephemeral_private, e))
+        goto fail;
+
+    /* se */
+    if (!mix_dh(chaining_key, NULL, wg->static_identity.static_private, e))
+        goto fail;
+
+    /* psk */
+    mix_psk(chaining_key, hash, key, preshared_key);
+
+    /* {} */
+    if (!message_decrypt(NULL, src->encrypted_nothing,
+                         sizeof(src->encrypted_nothing), key, hash))
+        goto fail;
+
+    /* Success! Copy everything to peer */
+    down_write(&handshake->lock);
+    /* It's important to check that the state is still the same, while we
+     * have an exclusive lock.
+     */
+    if (handshake->state != state) {
+        up_write(&handshake->lock);
+        goto fail;
+    }
+    memcpy(handshake->remote_ephemeral, e, NOISE_PUBLIC_KEY_LEN);
+    memcpy(handshake->hash, hash, NOISE_HASH_LEN);
+    memcpy(handshake->chaining_key, chaining_key, NOISE_HASH_LEN);
+    handshake->remote_index = src->sender_index;
+    handshake->state = HANDSHAKE_CONSUMED_RESPONSE;
+    up_write(&handshake->lock);
+    ret_peer = peer;
+    goto out;
 
 fail:
-	wg_peer_put(peer);
+    wg_peer_put(peer);
 out:
-	memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
-	memzero_explicit(hash, NOISE_HASH_LEN);
-	memzero_explicit(chaining_key, NOISE_HASH_LEN);
-	memzero_explicit(ephemeral_private, NOISE_PUBLIC_KEY_LEN);
-	memzero_explicit(static_private, NOISE_PUBLIC_KEY_LEN);
-	memzero_explicit(preshared_key, NOISE_SYMMETRIC_KEY_LEN);
-	up_read(&wg->static_identity.lock);
-	return ret_peer;
+    memzero_explicit(key, NOISE_SYMMETRIC_KEY_LEN);
+    memzero_explicit(hash, NOISE_HASH_LEN);
+    memzero_explicit(chaining_key, NOISE_HASH_LEN);
+    memzero_explicit(ephemeral_private, NOISE_PUBLIC_KEY_LEN);
+    memzero_explicit(static_private, NOISE_PUBLIC_KEY_LEN);
+    memzero_explicit(preshared_key, NOISE_SYMMETRIC_KEY_LEN);
+    up_read(&wg->static_identity.lock);
+
+    return ret_peer;
 }
 
 bool wg_noise_handshake_begin_session(struct noise_handshake *handshake,
-				      struct noise_keypairs *keypairs)
-{
-	struct noise_keypair *new_keypair;
-	bool ret = false;
-
-	down_write(&handshake->lock);
-	if (handshake->state != HANDSHAKE_CREATED_RESPONSE &&
-	    handshake->state != HANDSHAKE_CONSUMED_RESPONSE)
-		goto out;
-
-	new_keypair = keypair_create(handshake->entry.peer);
-	if (!new_keypair)
-		goto out;
-	new_keypair->i_am_the_initiator = handshake->state ==
-					  HANDSHAKE_CONSUMED_RESPONSE;
-	new_keypair->remote_index = handshake->remote_index;
-
-	if (new_keypair->i_am_the_initiator)
-		derive_keys(&new_keypair->sending, &new_keypair->receiving,
-			    handshake->chaining_key);
-	else
-		derive_keys(&new_keypair->receiving, &new_keypair->sending,
-			    handshake->chaining_key);
-
-	handshake_zero(handshake);
-	rcu_read_lock_bh();
-	if (likely(!READ_ONCE(container_of(handshake, struct wg_peer,
-					   handshake)->is_dead))) {
-		add_new_keypair(keypairs, new_keypair);
-		net_dbg_ratelimited("%s: Keypair %llu created for peer %llu\n",
-				    handshake->entry.peer->device->dev->name,
-				    new_keypair->internal_id,
-				    handshake->entry.peer->internal_id);
-		ret = wg_index_hashtable_replace(
-			handshake->entry.peer->device->index_hashtable,
-			&handshake->entry, &new_keypair->entry);
-	} else {
-		kfree_sensitive(new_keypair);
-	}
-	rcu_read_unlock_bh();
+                                      struct noise_keypairs *keypairs)
+{
+    struct noise_keypair *new_keypair;
+    bool ret = false;
+
+    down_write(&handshake->lock);
+    if (handshake->state != HANDSHAKE_CREATED_RESPONSE &&
+        handshake->state != HANDSHAKE_CONSUMED_RESPONSE)
+        goto out;
+
+    new_keypair = keypair_create(handshake->entry.peer);
+    if (!new_keypair)
+        goto out;
+    new_keypair->i_am_the_initiator = handshake->state ==
+                                       HANDSHAKE_CONSUMED_RESPONSE;
+    new_keypair->remote_index = handshake->remote_index;
+
+    if (new_keypair->i_am_the_initiator)
+        derive_keys(&new_keypair->sending, &new_keypair->receiving,
+                    handshake->chaining_key);
+    else
+        derive_keys(&new_keypair->receiving, &new_keypair->sending,
+                    handshake->chaining_key);
+
+    handshake_zero(handshake);
+    rcu_read_lock_bh();
+    if (likely(!READ_ONCE(container_of(handshake, struct wg_peer,
+                                       handshake)->is_dead))) {
+        add_new_keypair(keypairs, new_keypair);
+        net_dbg_ratelimited("%s: Keypair %llu created for peer %llu\n",
+                            handshake->entry.peer->device->dev->name,
+                            new_keypair->internal_id,
+                            handshake->entry.peer->internal_id);
+        ret = wg_index_hashtable_replace(
+            handshake->entry.peer->device->index_hashtable,
+            &handshake->entry, &new_keypair->entry);
+    } else {
+        kfree_sensitive(new_keypair);
+    }
+    rcu_read_unlock_bh();
 
 out:
-	up_write(&handshake->lock);
-	return ret;
+    up_write(&handshake->lock);
+
+    return ret;
 }
diff --git a/wireguard-linux/drivers/net/wireguard/peer.c b/wireguard-linux/drivers/net/wireguard/peer.c
index 1cb502a932e0..c0b5030437b0 100644
--- a/wireguard-linux/drivers/net/wireguard/peer.c
+++ b/wireguard-linux/drivers/net/wireguard/peer.c
@@ -9,15 +9,19 @@
 #include "timers.h"
 #include "peerlookup.h"
 #include "noise.h"
+#include "socket.h"
 
 #include <linux/kref.h>
 #include <linux/lockdep.h>
 #include <linux/rcupdate.h>
 #include <linux/list.h>
+#include <linux/wireguard.h>
 
 static struct kmem_cache *peer_cache;
 static atomic64_t peer_counter = ATOMIC64_INIT(0);
 
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound);
+
 struct wg_peer *wg_peer_create(struct wg_device *wg,
 			       const u8 public_key[NOISE_PUBLIC_KEY_LEN],
 			       const u8 preshared_key[NOISE_SYMMETRIC_KEY_LEN])
@@ -27,14 +31,17 @@ struct wg_peer *wg_peer_create(struct wg_device *wg,
 
 	lockdep_assert_held(&wg->device_update_lock);
 
-	if (wg->num_peers >= MAX_PEERS_PER_DEVICE)
+	if (wg->num_peers >= MAX_PEERS_PER_DEVICE) {
 		return ERR_PTR(ret);
+	}
 
 	peer = kmem_cache_zalloc(peer_cache, GFP_KERNEL);
-	if (unlikely(!peer))
+	if (unlikely(!peer)) {
 		return ERR_PTR(ret);
-	if (unlikely(dst_cache_init(&peer->endpoint_cache, GFP_KERNEL)))
+	}
+	if (unlikely(dst_cache_init(&peer->endpoint_cache, GFP_KERNEL))) {
 		goto err;
+	}
 
 	peer->device = wg;
 	wg_noise_handshake_init(&peer->handshake, &wg->static_identity,
@@ -59,8 +66,90 @@ struct wg_peer *wg_peer_create(struct wg_device *wg,
 	list_add_tail(&peer->peer_list, &wg->peer_list);
 	INIT_LIST_HEAD(&peer->allowedips_list);
 	wg_pubkey_hashtable_add(wg->peer_hashtable, peer);
+
+	// TCP field initialization
+	peer->peer_socket = NULL;  // Initialize the peer socket to NULL
+
+	// Initialize the original socket callbacks to NULL
+	peer->original_outbound_state_change = NULL;
+	peer->original_outbound_write_space = NULL;
+	peer->original_outbound_data_ready = NULL;
+	peer->original_outbound_error_report = NULL;
+	peer->original_outbound_destruct = NULL;
+
+	peer->original_inbound_state_change = NULL;
+	peer->original_inbound_write_space = NULL;
+	peer->original_inbound_data_ready = NULL;
+	peer->original_inbound_error_report = NULL;
+	peer->original_inbound_destruct = NULL;
+
+	peer->partial_skb = NULL;  // Initialize the partial skb pointer to NULL
+	peer->expected_len = 0;    // Initialize expected length to 0
+	peer->received_len = 0;    // Initialize received length to 0
+
+	// Initialize the skb queue for queuing TCP packets
+	skb_queue_head_init(&peer->tcp_packet_queue);
+
+	// Initialize the TCP retry scheduled flag to false
+	peer->tcp_retry_scheduled = false;
+
+	// Initialize the delayed work for TCP connection retry
+	INIT_DELAYED_WORK(&peer->tcp_retry_work, wg_tcp_retry_worker);
+
+	// Initialize the delayed work for TCP socket removal
+	INIT_DELAYED_WORK(&peer->tcp_inbound_remove_work, wg_tcp_inbound_remove_worker);
+	INIT_DELAYED_WORK(&peer->tcp_outbound_remove_work, wg_tcp_outbound_remove_worker);
+	
+	// Initialize TCP connection status flags
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->tcp_inbound_callbacks_set = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->tcp_outbound_remove_scheduled = false;
+	peer->tcp_inbound_remove_scheduled = false;
+	peer->peer_endpoint_set = false;
+
+	// Initialize the spinlock for protecting TCP-related state
+	spin_lock_init(&peer->tcp_lock);
+
+	// Initialize the skb queue for the TX send queue
+	skb_queue_head_init(&peer->send_queue);
+
+	// Initialize the spinlock for the TX send queue
+	spin_lock_init(&peer->send_queue_lock);
+
+	// Initialize the list head for pending connection list
+	INIT_LIST_HEAD(&peer->pending_connection_list);
+
+	// Initialize the work structure, associating it with the worker functions
+	INIT_WORK(&peer->tcp_read_work, wg_tcp_read_worker);
+	// Create a workqueue for processing TCP read data
+	peer->tcp_read_wq = alloc_workqueue("tcp_read_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_read_wq) {
+        	pr_err("Failed to allocate read workqueue\n");
+		goto err;
+	}
+
+	INIT_WORK(&peer->tcp_write_work, wg_tcp_write_worker);
+	// Create a workqueue for processing TCP write data
+	peer->tcp_write_wq = alloc_workqueue("tcp_write_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_write_wq) {
+		pr_err("Failed to allocate write workqueue\n");
+		goto err;
+	}
+
+	spin_lock_init(&peer->tcp_transfer_lock);  // Initialize the lock for the tcp_transfer queue
+	peer->tcp_transfer_wq = alloc_workqueue("wg_tcp_transfer_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_transfer_wq)
+		goto err;
+	
+	// Indicate this is a real peer not a temp peer
+	peer->temp_peer = false;
+	peer->peer_endpoint = peer->endpoint;
+	
 	++wg->num_peers;
-	pr_debug("%s: Peer %llu created\n", wg->dev->name, peer->internal_id);
+	if (wg->transport == WG_TRANSPORT_TCP)
+		wg_tcp_connect(peer);
 	return peer;
 
 err:
@@ -72,22 +161,67 @@ struct wg_peer *wg_peer_get_maybe_zero(struct wg_peer *peer)
 {
 	RCU_LOCKDEP_WARN(!rcu_read_lock_bh_held(),
 			 "Taking peer reference without holding the RCU read lock");
-	if (unlikely(!peer || !kref_get_unless_zero(&peer->refcount)))
+	if (unlikely(!peer || !kref_get_unless_zero(&peer->refcount))) {
 		return NULL;
+	}
 	return peer;
 }
 
+
+
 static void peer_make_dead(struct wg_peer *peer)
 {
+	if(!peer || IS_ERR(peer)){
+		printk(KERN_INFO "Exiting function peer_remove_after_dead, no peer.\n");
+		return;
+	}
+
 	/* Remove from configuration-time lookup structures. */
 	list_del_init(&peer->peer_list);
 	wg_allowedips_remove_by_peer(&peer->device->peer_allowedips, peer,
 				     &peer->device->device_update_lock);
 	wg_pubkey_hashtable_remove(peer->device->peer_hashtable, peer);
-
+	
 	/* Mark as dead, so that we don't allow jumping contexts after. */
 	WRITE_ONCE(peer->is_dead, true);
 
+	// Check if the TCP read work is scheduled before canceling it
+	if (peer->tcp_read_worker_scheduled) {
+        	cancel_work_sync(&peer->tcp_read_work);
+        	peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+	}
+
+	// Destroy the TCP read workqueue if it exists
+	if (peer->tcp_read_wq) {
+		destroy_workqueue(peer->tcp_read_wq);
+		peer->tcp_read_wq = NULL; // Avoid dangling pointers
+	}
+
+	// Check if the TCP write work is scheduled before canceling it
+	if (peer->tcp_write_worker_scheduled) {
+		cancel_work_sync(&peer->tcp_write_work);
+		peer->tcp_write_worker_scheduled = false;  // Reset the flag after canceling
+    	}
+
+	// Destroy the TCP write workqueue if it exists
+	if (peer->tcp_write_wq) {
+		destroy_workqueue(peer->tcp_write_wq);
+		peer->tcp_write_wq = NULL; // Avoid dangling pointers
+	}
+
+
+	// Destroy the TCP transfer workqueue if it exists
+    	if (peer->tcp_transfer_wq) {
+        	destroy_workqueue(peer->tcp_transfer_wq);
+		peer->tcp_transfer_wq = NULL; // Avoid dangling pointers
+    	}
+
+	// clean up any partial TCP data if it exists
+	if (peer->partial_skb) {
+		kfree_skb(peer->partial_skb);
+	    	peer->partial_skb = NULL;
+	}	
+
 	/* The caller must now synchronize_net() for this to take effect. */
 }
 
@@ -155,10 +289,12 @@ static void peer_remove_after_dead(struct wg_peer *peer)
  */
 void wg_peer_remove(struct wg_peer *peer)
 {
-	if (unlikely(!peer))
+	if (unlikely(!peer)) {
 		return;
+	}
 	lockdep_assert_held(&peer->device->device_update_lock);
-
+	wg_clean_peer_socket(peer, true, true, false); // clean up tcp socket stuff
+	wg_clean_peer_socket(peer, true, true, true);  // both inbound and outbound
 	peer_make_dead(peer);
 	synchronize_net();
 	peer_remove_after_dead(peer);
@@ -222,8 +358,10 @@ static void kref_release(struct kref *refcount)
 
 void wg_peer_put(struct wg_peer *peer)
 {
-	if (unlikely(!peer))
+	if (unlikely(!peer)) {
+		printk(KERN_INFO "wg_peer_put: exit (peer is NULL)\n");
 		return;
+	}
 	kref_put(&peer->refcount, kref_release);
 }
 
diff --git a/wireguard-linux/drivers/net/wireguard/peer.h b/wireguard-linux/drivers/net/wireguard/peer.h
index 76e4d3128ad4..96d5648812ab 100644
--- a/wireguard-linux/drivers/net/wireguard/peer.h
+++ b/wireguard-linux/drivers/net/wireguard/peer.h
@@ -18,20 +18,11 @@
 
 struct wg_device;
 
-struct endpoint {
-	union {
-		struct sockaddr addr;
-		struct sockaddr_in addr4;
-		struct sockaddr_in6 addr6;
-	};
-	union {
-		struct {
-			struct in_addr src4;
-			/* Essentially the same as addr6->scope_id */
-			int src_if4;
-		};
-		struct in6_addr src6;
-	};
+
+struct wg_tcp_transfer_work {
+    struct work_struct work;
+    struct sk_buff *skb;
+    struct wg_peer *peer;
 };
 
 struct wg_peer {
@@ -41,7 +32,7 @@ struct wg_peer {
 	int serial_work_cpu;
 	bool is_dead;
 	struct noise_keypairs keypairs;
-	struct endpoint endpoint;
+	struct endpoint endpoint, tcp_reply_endpoint, peer_endpoint;
 	struct dst_cache endpoint_cache;
 	rwlock_t endpoint_lock;
 	struct noise_handshake handshake;
@@ -64,8 +55,74 @@ struct wg_peer {
 	struct list_head allowedips_list;
 	struct napi_struct napi;
 	u64 internal_id;
+
+        // TCP-related members
+	bool peer_endpoint_set;
+	struct socket *peer_socket, *inbound_socket, *outbound_socket;
+	void (*original_outbound_state_change)(struct sock *sk);
+	void (*original_outbound_write_space)(struct sock *sk);
+	void (*original_outbound_data_ready)(struct sock *sk);
+	void (*original_outbound_error_report)(struct sock *sk);
+	void (*original_outbound_destruct)(struct sock *sk);
+	void (*original_inbound_state_change)(struct sock *sk);
+	void (*original_inbound_write_space)(struct sock *sk);
+	void (*original_inbound_data_ready)(struct sock *sk);
+	void (*original_inbound_error_report)(struct sock *sk);
+	void (*original_inbound_destruct)(struct sock *sk);
+	bool tcp_outbound_callbacks_set;			// Flag to track if the inbound socket callbacks have been set
+	bool tcp_inbound_callbacks_set;				// Flag to track if the inbound socket callbacks have been set
+	ktime_t outbound_timestamp, inbound_timestamp;	// timestamps for connections
+	struct sockaddr_storage	inbound_source, outbound_source, inbound_dest, outbound_dest;
+
+	struct sk_buff *partial_skb;
+	size_t expected_len;
+	size_t received_len;
+	struct sk_buff_head tcp_packet_queue;	// For queuing TCP packets
+
+	struct delayed_work tcp_retry_work;	// Work for retrying TCP connection
+	bool tcp_retry_scheduled;		// Flag to track connect retry scheduling
+
+	struct delayed_work tcp_outbound_remove_work;	// Work for removing outbound peer TCP connection
+	bool tcp_outbound_remove_scheduled;		// Flag to track outbound peer removal scheduling
+	struct delayed_work tcp_inbound_remove_work;	// Work for removing inbound peer TCP connection
+	bool tcp_inbound_remove_scheduled;		// Flag to track inbound peer removal scheduling
+
+	struct delayed_work tcp_cleanup_work;	// Work for removing TCP connections in pending list
+	bool tcp_cleanup_scheduled;		// Flag to track removal scheduling
+
+	bool tcp_established;			// Flag to track TCP connection status
+	bool tcp_pending;			// Flag to track outbount pending TCP connection status
+	bool inbound_connected;			// peer connected to us
+	bool outbound_connected;		// we connected to them
+	bool clean_outbound;			// release outbound at next cleanup
+	bool clean_inbound;			// release inbound at next cleanup
+	bool temp_peer;				// is this a temporary peer
+
+
+	struct sk_buff_head send_queue;		// TX queue
+        spinlock_t send_queue_lock;		// TX lock
+
+	struct list_head pending_connection_list;	//peers pending connection handshake
+	spinlock_t tcp_lock;			// Protects TCP-related state
+
+	struct work_struct tcp_read_work;	// Work struct for scheduling the worker
+	struct workqueue_struct *tcp_read_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_read_lock;		// Spinlock to protect access to the socket data
+	bool tcp_read_worker_scheduled;		// Flag to indicate if the TCP read worker is scheduled
+
+	struct work_struct tcp_write_work;      // Work struct for scheduling the worker
+	struct workqueue_struct *tcp_write_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_write_lock;              // Spinlock to protect access to the socket data
+	bool tcp_write_worker_scheduled; 	// Flag to indicate if the TCP write worker is scheduled
+
+	struct work_struct tcp_transfer_work;		// Work struct for scheduling the worker
+	struct workqueue_struct *tcp_transfer_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_transfer_lock;			// Spinlock to protect access to the socket data
+	bool tcp_transfer_worker_scheduled;		// Flag to indicate if the TCP transfer worker is scheduled
 };
 
+
+
 struct wg_peer *wg_peer_create(struct wg_device *wg,
 			       const u8 public_key[NOISE_PUBLIC_KEY_LEN],
 			       const u8 preshared_key[NOISE_SYMMETRIC_KEY_LEN]);
@@ -83,4 +140,12 @@ void wg_peer_remove_all(struct wg_device *wg);
 int wg_peer_init(void);
 void wg_peer_uninit(void);
 
+void wg_peer_tcp_connect(struct work_struct *work);
+void wg_peer_tcp_send(struct work_struct *work);
+void wg_peer_tcp_receive(struct work_struct *work);
+void wg_tcp_inbound_remove_worker(struct work_struct *work);
+void wg_tcp_outbound_remove_worker(struct work_struct *work);
+
+void wg_tcp_retry_worker(struct work_struct *work);
+
 #endif /* _WG_PEER_H */
diff --git a/wireguard-linux/drivers/net/wireguard/peerlookup.c b/wireguard-linux/drivers/net/wireguard/peerlookup.c
index f2783aa7a88f..8986257f97bc 100644
--- a/wireguard-linux/drivers/net/wireguard/peerlookup.c
+++ b/wireguard-linux/drivers/net/wireguard/peerlookup.c
@@ -15,16 +15,18 @@ static struct hlist_head *pubkey_bucket(struct pubkey_hashtable *table,
 	 * bits we need.
 	 */
 	const u64 hash = siphash(pubkey, NOISE_PUBLIC_KEY_LEN, &table->key);
-
 	return &table->hashtable[hash & (HASH_SIZE(table->hashtable) - 1)];
 }
 
 struct pubkey_hashtable *wg_pubkey_hashtable_alloc(void)
 {
+	printk(KERN_INFO "Entering wg_pubkey_hashtable_alloc\n");
 	struct pubkey_hashtable *table = kvmalloc(sizeof(*table), GFP_KERNEL);
 
-	if (!table)
+	if (!table) {
+		printk(KERN_INFO "Exiting wg_pubkey_hashtable_alloc: NULL\n");
 		return NULL;
+	}
 
 	get_random_bytes(&table->key, sizeof(table->key));
 	hash_init(table->hashtable);
@@ -39,6 +41,7 @@ void wg_pubkey_hashtable_add(struct pubkey_hashtable *table,
 	hlist_add_head_rcu(&peer->pubkey_hash,
 			   pubkey_bucket(table, peer->handshake.remote_static));
 	mutex_unlock(&table->lock);
+	printk(KERN_INFO "Exiting wg_pubkey_hashtable_add\n");
 }
 
 void wg_pubkey_hashtable_remove(struct pubkey_hashtable *table,
@@ -84,8 +87,10 @@ struct index_hashtable *wg_index_hashtable_alloc(void)
 {
 	struct index_hashtable *table = kvmalloc(sizeof(*table), GFP_KERNEL);
 
-	if (!table)
+	if (!table) {
+		printk(KERN_INFO "Exiting wg_index_hashtable_alloc: NULL\n");
 		return NULL;
+	}
 
 	hash_init(table->hashtable);
 	spin_lock_init(&table->lock);
@@ -115,7 +120,6 @@ struct index_hashtable *wg_index_hashtable_alloc(void)
  * guessing. this would not, however, help with the growing hash lengths, which
  * is another thing to consider moving forward.
  */
-
 __le32 wg_index_hashtable_insert(struct index_hashtable *table,
 				 struct index_hashtable_entry *entry)
 {
@@ -159,7 +163,6 @@ __le32 wg_index_hashtable_insert(struct index_hashtable *table,
 	spin_unlock_bh(&table->lock);
 
 	rcu_read_unlock_bh();
-
 	return entry->index;
 }
 
diff --git a/wireguard-linux/drivers/net/wireguard/queueing.c b/wireguard-linux/drivers/net/wireguard/queueing.c
index 26d235d15235..940ed3873e92 100644
--- a/wireguard-linux/drivers/net/wireguard/queueing.c
+++ b/wireguard-linux/drivers/net/wireguard/queueing.c
@@ -22,19 +22,21 @@ wg_packet_percpu_multicore_worker_alloc(work_func_t function, void *ptr)
 	return worker;
 }
 
-int wg_packet_queue_init(struct crypt_queue *queue, work_func_t function,
-			 unsigned int len)
+int wg_packet_queue_init(struct crypt_queue *queue, work_func_t function, unsigned int len)
 {
 	int ret;
 
 	memset(queue, 0, sizeof(*queue));
 	queue->last_cpu = -1;
 	ret = ptr_ring_init(&queue->ring, len, GFP_KERNEL);
-	if (ret)
+	if (ret) {
+		printk(KERN_INFO "Exiting: wg_packet_queue_init with ret=%d\n", ret);
 		return ret;
+	}
 	queue->worker = wg_packet_percpu_multicore_worker_alloc(function, queue);
 	if (!queue->worker) {
 		ptr_ring_cleanup(&queue->ring, NULL);
+		printk(KERN_INFO "Exiting: wg_packet_queue_init with ret=%d\n", -ENOMEM);
 		return -ENOMEM;
 	}
 	return 0;
@@ -71,8 +73,10 @@ static void __wg_prev_queue_enqueue(struct prev_queue *queue, struct sk_buff *sk
 
 bool wg_prev_queue_enqueue(struct prev_queue *queue, struct sk_buff *skb)
 {
-	if (!atomic_add_unless(&queue->count, 1, MAX_QUEUED_PACKETS))
+	if (!atomic_add_unless(&queue->count, 1, MAX_QUEUED_PACKETS)) {
+		printk(KERN_INFO "Exiting: wg_prev_queue_enqueue with ret=%d\n", false);
 		return false;
+	}
 	__wg_prev_queue_enqueue(queue, skb);
 	return true;
 }
@@ -80,10 +84,11 @@ bool wg_prev_queue_enqueue(struct prev_queue *queue, struct sk_buff *skb)
 struct sk_buff *wg_prev_queue_dequeue(struct prev_queue *queue)
 {
 	struct sk_buff *tail = queue->tail, *next = smp_load_acquire(&NEXT(tail));
-
 	if (tail == STUB(queue)) {
-		if (!next)
+		if (!next) {
+			printk(KERN_INFO "Exiting: wg_prev_queue_dequeue with ret=%p\n", NULL);
 			return NULL;
+		}
 		queue->tail = next;
 		tail = next;
 		next = smp_load_acquire(&NEXT(next));
@@ -93,13 +98,15 @@ struct sk_buff *wg_prev_queue_dequeue(struct prev_queue *queue)
 		atomic_dec(&queue->count);
 		return tail;
 	}
-	if (tail != READ_ONCE(queue->head))
+	if (tail != READ_ONCE(queue->head)) {
 		return NULL;
+	}
 	__wg_prev_queue_enqueue(queue, STUB(queue));
 	next = smp_load_acquire(&NEXT(tail));
 	if (next) {
 		queue->tail = next;
 		atomic_dec(&queue->count);
+		printk(KERN_INFO "Exiting: wg_prev_queue_dequeue with ret=%p\n", tail);
 		return tail;
 	}
 	return NULL;
diff --git a/wireguard-linux/drivers/net/wireguard/queueing.h b/wireguard-linux/drivers/net/wireguard/queueing.h
index 1ea4f874e367..026bc5e984b3 100644
--- a/wireguard-linux/drivers/net/wireguard/queueing.h
+++ b/wireguard-linux/drivers/net/wireguard/queueing.h
@@ -11,6 +11,7 @@
 #include <linux/skbuff.h>
 #include <linux/ip.h>
 #include <linux/ipv6.h>
+#include <linux/wireguard.h>
 #include <net/ip_tunnels.h>
 
 struct wg_device;
@@ -159,7 +160,6 @@ static inline int wg_queue_enqueue_per_device_and_peer(
 	struct sk_buff *skb, struct workqueue_struct *wq)
 {
 	int cpu;
-
 	atomic_set_release(&PACKET_CB(skb)->state, PACKET_STATE_UNCRYPTED);
 	/* We first queue this up for the peer ingestion, but the consumer
 	 * will wait for the state to change to CRYPTED or DEAD before.
diff --git a/wireguard-linux/drivers/net/wireguard/ratelimiter.c b/wireguard-linux/drivers/net/wireguard/ratelimiter.c
index dd55e5c26f46..8d921dad5978 100644
--- a/wireguard-linux/drivers/net/wireguard/ratelimiter.c
+++ b/wireguard-linux/drivers/net/wireguard/ratelimiter.c
@@ -127,6 +127,7 @@ bool wg_ratelimiter_allow(struct sk_buff *skb, struct net *net)
 			entry->tokens = ret ? tokens - PACKET_COST : tokens;
 			spin_unlock(&entry->lock);
 			rcu_read_unlock();
+			printk(KERN_INFO "wg_ratelimiter_allow: exit ret=%d\n", ret);
 			return ret;
 		}
 	}
@@ -192,6 +193,7 @@ int wg_ratelimiter_init(void)
 	get_random_bytes(&key, sizeof(key));
 out:
 	mutex_unlock(&init_lock);
+	printk(KERN_INFO "wg_ratelimiter_init: exit 0\n");
 	return 0;
 
 err_kmemcache:
diff --git a/wireguard-linux/drivers/net/wireguard/receive.c b/wireguard-linux/drivers/net/wireguard/receive.c
index db01ec03bda0..42bed1208b30 100644
--- a/wireguard-linux/drivers/net/wireguard/receive.c
+++ b/wireguard-linux/drivers/net/wireguard/receive.c
@@ -13,20 +13,51 @@
 
 #include <linux/ip.h>
 #include <linux/ipv6.h>
+#include <linux/tcp.h>
 #include <linux/udp.h>
 #include <net/ip_tunnels.h>
+#include <linux/skbuff.h>
+#include <linux/net.h>
+#include <linux/in.h>
+#include <linux/ipv6.h>
+#include <net/ipv6.h>
+#include <net/ip.h>
+
+#define WG_TRANSPORT_UDP	0
+#define WG_TRANSPORT_TCP	1
+bool endpoint_eq(const struct endpoint *a, const struct endpoint *b);
+void log_wireguard_endpoint(struct endpoint *ep);
+
+struct wg_tcp_socket_list_entry {
+	struct socket *tcp_socket;		// Socket associated with the connection
+	struct sockaddr_storage src_addr;	// Source address for the connection
+	struct wg_peer *temp_peer;		// temporary peer for dataready
+	struct list_head tcp_connection_ll;	// List pointer for the linked list
+	ktime_t timestamp;			// Timestamp when the connection was added
+};
+
+struct wg_socket_data {
+	struct wg_device *device;
+	struct wg_peer *peer;
+	bool inbound;
+};
+
 
 /* Must be called with bh disabled. */
 static void update_rx_stats(struct wg_peer *peer, size_t len)
 {
+	printk(KERN_INFO "Entering update_rx_stats: peer=%p, len=%zu\n", peer, len);
 	dev_sw_netstats_rx_add(peer->device->dev, len);
 	peer->rx_bytes += len;
+	printk(KERN_INFO "Exiting update_rx_stats\n");
 }
 
+
 #define SKB_TYPE_LE32(skb) (((struct message_header *)(skb)->data)->type)
 
 static size_t validate_header_len(struct sk_buff *skb)
 {
+	printk(KERN_INFO "Entering validate_header_len: skb=%p\n", skb);
 	if (unlikely(skb->len < sizeof(struct message_header)))
 		return 0;
 	if (SKB_TYPE_LE32(skb) == cpu_to_le32(MESSAGE_DATA) &&
@@ -41,54 +72,188 @@ static size_t validate_header_len(struct sk_buff *skb)
 	if (SKB_TYPE_LE32(skb) == cpu_to_le32(MESSAGE_HANDSHAKE_COOKIE) &&
 	    skb->len == sizeof(struct message_handshake_cookie))
 		return sizeof(struct message_handshake_cookie);
+	printk(KERN_INFO "Exiting validate_header_len\n");
 	return 0;
 }
 
 static int prepare_skb_header(struct sk_buff *skb, struct wg_device *wg)
 {
 	size_t data_offset, data_len, header_len;
-	struct udphdr *udp;
+	struct udphdr _udp, *udp;
 
+	// Check packet protocol and header validity
 	if (unlikely(!wg_check_packet_protocol(skb) ||
 		     skb_transport_header(skb) < skb->head ||
-		     (skb_transport_header(skb) + sizeof(struct udphdr)) >
-			     skb_tail_pointer(skb)))
+		     (skb_transport_header(skb) + sizeof(struct udphdr)) > skb_tail_pointer(skb))) {
 		return -EINVAL; /* Bogus IP header */
-	udp = udp_hdr(skb);
-	data_offset = (u8 *)udp - skb->data;
-	if (unlikely(data_offset > U16_MAX ||
-		     data_offset + sizeof(struct udphdr) > skb->len))
-		/* Packet has offset at impossible location or isn't big enough
-		 * to have UDP fields.
-		 */
+	}
+
+	// Safely access UDP header using skb_header_pointer
+	udp = skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_udp), &_udp);
+	if (!udp) {
 		return -EINVAL;
+	}
+
+	// Check for valid UDP ports
+	if (unlikely(udp->source == 0 || udp->dest == 0)) {
+		printk(KERN_ERR "Invalid UDP source or destination port: src=%u, dst=%u\n", ntohs(udp->source), ntohs(udp->dest));
+		return -EINVAL;
+	}
+
+	// Calculate data offset and validate
+	data_offset = skb_transport_offset(skb) + sizeof(struct udphdr);
+
+	if (unlikely(data_offset > U16_MAX || data_offset + sizeof(struct udphdr) > skb->len)) {
+		return -EINVAL;
+	}
+
+	// Get the UDP length field
 	data_len = ntohs(udp->len);
-	if (unlikely(data_len < sizeof(struct udphdr) ||
-		     data_len > skb->len - data_offset))
-		/* UDP packet is reporting too small of a size or lying about
-		 * its size.
-		 */
+
+	// Validate data length
+	if (unlikely(data_len < sizeof(struct udphdr) || data_len > skb->len - skb_transport_offset(skb))) {
 		return -EINVAL;
+	}
+
+	// Adjust data length to exclude UDP header
 	data_len -= sizeof(struct udphdr);
-	data_offset = (u8 *)udp + sizeof(struct udphdr) - skb->data;
-	if (unlikely(!pskb_may_pull(skb,
-				data_offset + sizeof(struct message_header)) ||
-		     pskb_trim(skb, data_len + data_offset) < 0))
+	data_offset = skb_transport_offset(skb) + sizeof(struct udphdr);
+
+	// Check pull and trim capabilities
+	if (unlikely(!pskb_may_pull(skb, data_offset + sizeof(struct message_header)) ||
+		     pskb_trim(skb, data_len + data_offset) < 0)) {
 		return -EINVAL;
+	}
+
+	// Diagnostics before pulling SKB data
 	skb_pull(skb, data_offset);
-	if (unlikely(skb->len != data_len))
-		/* Final len does not agree with calculated len */
+	// Diagnostics after pulling SKB data
+	// Validate the SKB length against calculated data length
+	if (unlikely(skb->len != data_len)) {
 		return -EINVAL;
+	}
+
+	// Validate header length
 	header_len = validate_header_len(skb);
-	if (unlikely(!header_len))
+	if (unlikely(!header_len)) {
 		return -EINVAL;
+	}
+
 	__skb_push(skb, data_offset);
-	if (unlikely(!pskb_may_pull(skb, data_offset + header_len)))
+
+	// Check pull capabilities after push
+	if (unlikely(!pskb_may_pull(skb, data_offset + header_len))) {
 		return -EINVAL;
+	}
+
 	__skb_pull(skb, data_offset);
+
+out:
 	return 0;
 }
 
+// Function to extract source and destination sockaddr_storage from an skb
+int extract_sockaddr_from_skb(struct sk_buff *skb, struct sockaddr_storage *source,
+			      struct sockaddr_storage *dest)
+{
+	struct iphdr *ip_header;
+	struct ipv6hdr *ipv6_header;
+	struct tcphdr *tcp_header;
+	struct udphdr *udp_header;
+
+	if (!skb) {
+		return -1; // Invalid skb
+	}
+
+	// Handle IPv4 packets
+	if (skb->protocol == htons(ETH_P_IP)) {
+		ip_header = ip_hdr(skb);
+			if (!ip_header) {
+			return -1; // Failed to get IP header
+		}
+
+		struct sockaddr_in *src_in = (struct sockaddr_in *)source;
+		struct sockaddr_in *dest_in = (struct sockaddr_in *)dest;
+
+		memset(src_in, 0, sizeof(struct sockaddr_in));
+		memset(dest_in, 0, sizeof(struct sockaddr_in));
+
+		src_in->sin_family = AF_INET;
+		dest_in->sin_family = AF_INET;
+
+		src_in->sin_addr.s_addr = ip_header->saddr;
+		dest_in->sin_addr.s_addr = ip_header->daddr;
+
+		// Determine transport protocol
+		if (ip_header->protocol == IPPROTO_TCP) {
+			tcp_header = tcp_hdr(skb);
+			if (!tcp_header) {
+				return -1; // Failed to get TCP header
+			}
+			src_in->sin_port = tcp_header->source;
+			dest_in->sin_port = tcp_header->dest;
+		} else if (ip_header->protocol == IPPROTO_UDP) {
+			udp_header = udp_hdr(skb);
+			if (!udp_header) {
+				return -1; // Failed to get UDP header
+			}
+			src_in->sin_port = udp_header->source;
+			dest_in->sin_port = udp_header->dest;
+		} else {
+			return -1; // Unsupported protocol
+		}
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	// Handle IPv6 packets
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		ipv6_header = ipv6_hdr(skb);
+		if (!ipv6_header) {
+			return -1; // Failed to get IPv6 header
+		}
+
+		struct sockaddr_in6 *src_in6 = (struct sockaddr_in6 *)source;
+		struct sockaddr_in6 *dest_in6 = (struct sockaddr_in6 *)dest;
+
+		memset(src_in6, 0, sizeof(struct sockaddr_in6));
+		memset(dest_in6, 0, sizeof(struct sockaddr_in6));
+
+		src_in6->sin6_family = AF_INET6;
+		dest_in6->sin6_family = AF_INET6;
+
+		src_in6->sin6_addr = ipv6_header->saddr;
+		dest_in6->sin6_addr = ipv6_header->daddr;
+
+		// Determine transport protocol
+		if (ipv6_header->nexthdr == IPPROTO_TCP) {
+			tcp_header = tcp_hdr(skb);
+			if (!tcp_header) {
+				return -1; // Failed to get TCP header
+			}
+			src_in6->sin6_port = tcp_header->source;
+			dest_in6->sin6_port = tcp_header->dest;
+		} else if (ipv6_header->nexthdr == IPPROTO_UDP) {
+			udp_header = udp_hdr(skb);
+			if (!udp_header) {
+				return -1; // Failed to get UDP header
+			}
+			src_in6->sin6_port = udp_header->source;
+			dest_in6->sin6_port = udp_header->dest;
+		} else {
+			return -1; // Unsupported protocol
+		}
+	}
+#endif
+
+	else {
+		return -1; // Unsupported packet type
+	}
+
+	return 0; // Success
+}
+
+void print_peer_socket_info(struct wg_peer *peer);
+
 static void wg_receive_handshake_packet(struct wg_device *wg,
 					struct sk_buff *skb)
 {
@@ -101,99 +266,189 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 	bool packet_needs_cookie;
 	bool under_load;
 
+	if(wg->transport == WG_TRANSPORT_TCP) { 
+		// For TCP, skip cookie check
+		packet_needs_cookie = false;
+		goto nocookie;
+	}
+
+	// Handle handshake cookie response
 	if (SKB_TYPE_LE32(skb) == cpu_to_le32(MESSAGE_HANDSHAKE_COOKIE)) {
-		net_dbg_skb_ratelimited("%s: Receiving cookie response from %pISpfsc\n",
-					wg->dev->name, skb);
-		wg_cookie_message_consume(
-			(struct message_handshake_cookie *)skb->data, wg);
+		wg_cookie_message_consume((struct message_handshake_cookie *)skb->data, wg);
 		return;
 	}
 
-	under_load = atomic_read(&wg->handshake_queue_len) >=
-			MAX_QUEUED_INCOMING_HANDSHAKES / 8;
+	// Load calculation to decide if system is under load
+	under_load = atomic_read(&wg->handshake_queue_len) >= MAX_QUEUED_INCOMING_HANDSHAKES / 8;
 	if (under_load) {
 		last_under_load = ktime_get_coarse_boottime_ns();
 	} else if (last_under_load) {
 		under_load = !wg_birthdate_has_expired(last_under_load, 1);
-		if (!under_load)
+		if (!under_load) {
 			last_under_load = 0;
+		}
 	}
-	mac_state = wg_cookie_validate_packet(&wg->cookie_checker, skb,
-					      under_load);
+
+	// Validate packet's MAC and set packet_needs_cookie flag
+	mac_state = wg_cookie_validate_packet(&wg->cookie_checker, skb, under_load);
+
+	// Determine if a cookie is needed based on load and MAC state
 	if ((under_load && mac_state == VALID_MAC_WITH_COOKIE) ||
 	    (!under_load && mac_state == VALID_MAC_BUT_NO_COOKIE)) {
 		packet_needs_cookie = false;
 	} else if (under_load && mac_state == VALID_MAC_BUT_NO_COOKIE) {
-		packet_needs_cookie = true;
+		if(wg->transport == WG_TRANSPORT_UDP)
+			packet_needs_cookie = false;
 	} else {
-		net_dbg_skb_ratelimited("%s: Invalid MAC of handshake, dropping packet from %pISpfsc\n",
-					wg->dev->name, skb);
+		net_dbg_skb_ratelimited("%s: Invalid MAC of handshake, dropping packet from %pISpfsc\n", wg->dev->name, skb);
 		return;
 	}
 
+nocookie:
+	// Process handshake packets
 	switch (SKB_TYPE_LE32(skb)) {
 	case cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION): {
-		struct message_handshake_initiation *message =
-			(struct message_handshake_initiation *)skb->data;
+		struct message_handshake_initiation *message = (struct message_handshake_initiation *)skb->data;
 
 		if (packet_needs_cookie) {
-			wg_packet_send_handshake_cookie(wg, skb,
-							message->sender_index);
+			wg_packet_send_handshake_cookie(wg, skb, message->sender_index);
 			return;
 		}
+
+		// Handle handshake initiation
 		peer = wg_noise_handshake_consume_initiation(message, wg);
 		if (unlikely(!peer)) {
-			net_dbg_skb_ratelimited("%s: Invalid handshake initiation from %pISpfsc\n",
-						wg->dev->name, skb);
+			net_dbg_skb_ratelimited("%s: Invalid handshake initiation from %pISpfsc\n", wg->dev->name, skb);
 			return;
 		}
+		print_peer_socket_info(peer);
 		wg_socket_set_peer_endpoint_from_skb(peer, skb);
-		net_dbg_ratelimited("%s: Receiving handshake initiation from peer %llu (%pISpfsc)\n",
-				    wg->dev->name, peer->internal_id,
-				    &peer->endpoint.addr);
+		net_dbg_ratelimited("%s: Receiving handshake initiation from peer %llu (%pISpfsc)\n", wg->dev->name, peer->internal_id, &peer->endpoint.addr);
 		wg_packet_send_handshake_response(peer);
 		break;
 	}
 	case cpu_to_le32(MESSAGE_HANDSHAKE_RESPONSE): {
-		struct message_handshake_response *message =
-			(struct message_handshake_response *)skb->data;
+		struct message_handshake_response *message = (struct message_handshake_response *)skb->data;
 
 		if (packet_needs_cookie) {
-			wg_packet_send_handshake_cookie(wg, skb,
-							message->sender_index);
+			wg_packet_send_handshake_cookie(wg, skb, message->sender_index);
 			return;
 		}
+
+		// Handle handshake response
 		peer = wg_noise_handshake_consume_response(message, wg);
 		if (unlikely(!peer)) {
-			net_dbg_skb_ratelimited("%s: Invalid handshake response from %pISpfsc\n",
-						wg->dev->name, skb);
+			net_dbg_skb_ratelimited("%s: Invalid handshake response from %pISpfsc\n", wg->dev->name, skb);
 			return;
 		}
+
+		// TCP-specific endpoint handling
+		struct endpoint ep = peer->endpoint;
+		struct wg_tcp_socket_list_entry *socket_iter;
+		bool found = false;
+		print_peer_socket_info(peer);
 		wg_socket_set_peer_endpoint_from_skb(peer, skb);
-		net_dbg_ratelimited("%s: Receiving handshake response from peer %llu (%pISpfsc)\n",
-				    wg->dev->name, peer->internal_id,
-				    &peer->endpoint.addr);
-		if (wg_noise_handshake_begin_session(&peer->handshake,
-						     &peer->keypairs)) {
+
+		if (!endpoint_eq(&peer->endpoint, &ep) && peer->device->transport == WG_TRANSPORT_TCP) {
+			if(skb->sk) {
+				if(skb->sk->sk_socket) {
+					if ((skb->sk->sk_socket != peer->inbound_socket) && (skb->sk->sk_socket != peer->outbound_socket)) {
+ 						if (!list_empty(&peer->device->tcp_connection_list)) {
+							found = false;
+							list_for_each_entry_rcu(socket_iter, &peer->device->tcp_connection_list, tcp_connection_ll) {
+							// Defensive checks to ensure all relevant fields are populated
+								if (!socket_iter) {
+									continue;
+								}
+								if (!socket_iter->tcp_socket || !socket_iter->tcp_socket->sk) {
+									continue;
+								}
+								if (!socket_iter->temp_peer || IS_ERR(socket_iter->temp_peer)) {
+									continue;
+								}
+	
+								if (endpoint_eq(&peer->endpoint, &socket_iter->temp_peer->endpoint)) {
+									found = true;
+									break;
+								}
+							}
+
+							if (found) {
+								spin_lock_bh(&peer->device->tcp_connection_list_lock);
+								list_del_rcu(&socket_iter->tcp_connection_ll);
+								spin_unlock_bh(&peer->device->tcp_connection_list_lock);
+								synchronize_rcu();
+
+								spin_lock(&peer->tcp_lock);
+								if (socket_iter->temp_peer->tcp_inbound_callbacks_set) {
+									peer->original_inbound_state_change = socket_iter->temp_peer->original_inbound_state_change;
+									peer->original_inbound_write_space = socket_iter->temp_peer->original_inbound_write_space;
+									peer->original_inbound_data_ready = socket_iter->temp_peer->original_inbound_data_ready;
+									peer->tcp_inbound_callbacks_set = true;
+								}
+								peer->inbound_connected = true;
+								peer->inbound_timestamp = ktime_get();
+								peer->tcp_established = true;
+								peer->tcp_inbound_remove_scheduled = false;
+								peer->clean_inbound = false;
+								peer->clean_outbound = true;
+								spin_unlock(&peer->tcp_lock);	
+
+								// Clean up temporary peer structure
+								wg_clean_peer_socket(socket_iter->temp_peer, false, false, false);
+								if (!IS_ERR(socket_iter->temp_peer) && socket_iter->temp_peer)
+									kfree(socket_iter->temp_peer);
+
+								write_lock_bh(&peer->endpoint_lock);
+								peer->tcp_reply_endpoint = socket_iter->temp_peer->endpoint;
+								peer->peer_socket = skb->sk->sk_socket;
+								peer->inbound_socket = skb->sk->sk_socket;
+								extract_sockaddr_from_skb(skb, &peer->inbound_source, &peer->inbound_dest);
+								((struct wg_socket_data *)(peer->peer_socket->sk->sk_user_data))->peer = peer;
+								write_unlock_bh(&peer->endpoint_lock);
+
+								kfree(socket_iter);
+							}
+						}	
+					}
+				}
+			}
+		}
+
+		// Update timestamps and session handling for TCP
+		if (peer->device->transport == WG_TRANSPORT_TCP && skb->sk) {
+			write_lock_bh(&peer->endpoint_lock);
+			peer->peer_socket = skb->sk->sk_socket;
+			write_unlock_bh(&peer->endpoint_lock);
+			if(skb->sk->sk_socket) {
+				if (skb->sk->sk_socket == peer->inbound_socket) {
+					peer->inbound_timestamp = ktime_get();
+				} else {
+					peer->outbound_timestamp = ktime_get();
+				}
+			}
+		}
+		net_dbg_ratelimited("%s: Receiving handshake response from peer %llu (%pISpfsc)\n", wg->dev->name, peer->internal_id, &peer->endpoint.addr);
+
+		if (wg_noise_handshake_begin_session(&peer->handshake, &peer->keypairs)) {
 			wg_timers_session_derived(peer);
 			wg_timers_handshake_complete(peer);
-			/* Calling this function will either send any existing
-			 * packets in the queue and not send a keepalive, which
-			 * is the best case, Or, if there's nothing in the
-			 * queue, it will send a keepalive, in order to give
-			 * immediate confirmation of the session.
-			 */
 			wg_packet_send_keepalive(peer);
 		}
 		break;
 	}
+
+	default:
+		break;
 	}
 
+	// Final check to ensure peer is valid
 	if (unlikely(!peer)) {
-		WARN(1, "Somehow a wrong type of packet wound up in the handshake queue!\n");
+		WARN(1, "Unexpected state: No valid peer found after handshake processing\n");
 		return;
 	}
 
+	// Update statistics and state
 	local_bh_disable();
 	update_rx_stats(peer, skb->len);
 	local_bh_enable();
@@ -203,6 +458,7 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 	wg_peer_put(peer);
 }
 
+
 void wg_packet_handshake_receive_worker(struct work_struct *work)
 {
 	struct crypt_queue *queue = container_of(work, struct multicore_worker, work)->ptr;
@@ -241,18 +497,22 @@ static void keep_key_fresh(struct wg_peer *peer)
 
 static bool decrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 {
+	printk(KERN_INFO "Entering decrypt_packet: skb=%p, keypair=%p\n", skb, keypair);
 	struct scatterlist sg[MAX_SKB_FRAGS + 8];
 	struct sk_buff *trailer;
 	unsigned int offset;
 	int num_frags;
 
-	if (unlikely(!keypair))
+	if (unlikely(!keypair)) {
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
+	}
 
 	if (unlikely(!READ_ONCE(keypair->receiving.is_valid) ||
 		  wg_birthdate_has_expired(keypair->receiving.birthdate, REJECT_AFTER_TIME) ||
 		  READ_ONCE(keypair->receiving_counter.counter) >= REJECT_AFTER_MESSAGES)) {
 		WRITE_ONCE(keypair->receiving.is_valid, false);
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
 	}
 
@@ -268,26 +528,35 @@ static bool decrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 	num_frags = skb_cow_data(skb, 0, &trailer);
 	offset += sizeof(struct message_data);
 	skb_pull(skb, offset);
-	if (unlikely(num_frags < 0 || num_frags > ARRAY_SIZE(sg)))
+	if (unlikely(num_frags < 0 || num_frags > ARRAY_SIZE(sg))) {
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
+	}
 
 	sg_init_table(sg, num_frags);
-	if (skb_to_sgvec(skb, sg, 0, skb->len) <= 0)
+	if (skb_to_sgvec(skb, sg, 0, skb->len) <= 0) {
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
+	}
 
 	if (!chacha20poly1305_decrypt_sg_inplace(sg, skb->len, NULL, 0,
 					         PACKET_CB(skb)->nonce,
-						 keypair->receiving.key))
+						 keypair->receiving.key)) {
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
+	}
 
 	/* Another ugly situation of pushing and pulling the header so as to
 	 * keep endpoint information intact.
 	 */
 	skb_push(skb, offset);
-	if (pskb_trim(skb, skb->len - noise_encrypted_len(0)))
+	if (pskb_trim(skb, skb->len - noise_encrypted_len(0))) {
+		printk(KERN_INFO "Exiting decrypt_packet with false\n");
 		return false;
+	}
 	skb_pull(skb, offset);
 
+	printk(KERN_INFO "Exiting decrypt_packet with true\n");
 	return true;
 }
 
@@ -327,6 +596,7 @@ static bool counter_validate(struct noise_replay_counter *counter, u64 their_cou
 
 out:
 	spin_unlock_bh(&counter->lock);
+	printk(KERN_INFO "Exiting counter_validate with %d\n", ret);
 	return ret;
 }
 
@@ -340,6 +610,11 @@ static void wg_packet_consume_data_done(struct wg_peer *peer,
 	unsigned int len, len_before_trim;
 	struct wg_peer *routed_peer;
 
+	if (unlikely(!endpoint)) {
+    		return;
+	}
+
+	
 	wg_socket_set_peer_endpoint(peer, endpoint);
 
 	if (unlikely(wg_noise_received_with_keypair(&peer->keypairs,
@@ -398,6 +673,14 @@ static void wg_packet_consume_data_done(struct wg_peer *peer,
 	if (unlikely(len > skb->len))
 		goto dishonest_packet_size;
 	len_before_trim = skb->len;
+
+
+	if (unlikely(len == 0 || len_before_trim == 0)) {
+    		printk(KERN_ERR "Invalid packet length detected: len=%u, len_before_trim=%u\n", len, len_before_trim);
+    		return;
+	}
+
+	
 	if (unlikely(pskb_trim(skb, len)))
 		goto packet_processed;
 
@@ -433,6 +716,7 @@ static void wg_packet_consume_data_done(struct wg_peer *peer,
 	goto packet_processed;
 packet_processed:
 	dev_kfree_skb(skb);
+	printk(KERN_INFO "Exiting wg_packet_consume_data_done\n");
 }
 
 int wg_packet_rx_poll(struct napi_struct *napi, int budget)
@@ -445,8 +729,9 @@ int wg_packet_rx_poll(struct napi_struct *napi, int budget)
 	int work_done = 0;
 	bool free;
 
-	if (unlikely(budget <= 0))
+	if (unlikely(budget <= 0)) {
 		return 0;
+	}
 
 	while ((skb = wg_prev_queue_peek(&peer->rx_queue)) != NULL &&
 	       (state = atomic_read_acquire(&PACKET_CB(skb)->state)) !=
@@ -454,7 +739,7 @@ int wg_packet_rx_poll(struct napi_struct *napi, int budget)
 		wg_prev_queue_drop_peeked(&peer->rx_queue);
 		keypair = PACKET_CB(skb)->keypair;
 		free = true;
-
+		
 		if (unlikely(state != PACKET_STATE_CRYPTED))
 			goto next;
 
@@ -539,6 +824,7 @@ static void wg_packet_consume_data(struct wg_device *wg, struct sk_buff *skb)
 	dev_kfree_skb(skb);
 }
 
+#ifdef ORIGINAL
 void wg_packet_receive(struct wg_device *wg, struct sk_buff *skb)
 {
 	if (unlikely(prepare_skb_header(skb, wg) < 0))
@@ -584,3 +870,63 @@ void wg_packet_receive(struct wg_device *wg, struct sk_buff *skb)
 err:
 	dev_kfree_skb(skb);
 }
+#endif // ORIGINAL
+
+void wg_packet_receive(struct wg_device *wg, struct sk_buff *skb)
+{
+	if (unlikely(prepare_skb_header(skb, wg) < 0)) {
+		goto err;
+	}
+
+	/* Determine packet type */
+	uint32_t skb_type = SKB_TYPE_LE32(skb);
+
+	switch (skb_type) {
+	case cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION):
+	case cpu_to_le32(MESSAGE_HANDSHAKE_RESPONSE):
+	case cpu_to_le32(MESSAGE_HANDSHAKE_COOKIE): {
+		int cpu, ret = -EBUSY;
+
+		if (unlikely(!rng_is_initialized())) {
+			goto drop;
+		}
+
+		int queue_len = atomic_read(&wg->handshake_queue_len);
+
+		if (queue_len > MAX_QUEUED_INCOMING_HANDSHAKES / 2) {
+			if (spin_trylock_bh(&wg->handshake_queue.ring.producer_lock)) {
+				ret = __ptr_ring_produce(&wg->handshake_queue.ring, skb);
+				spin_unlock_bh(&wg->handshake_queue.ring.producer_lock);
+			} 
+		} else {
+			ret = ptr_ring_produce_bh(&wg->handshake_queue.ring, skb);
+		}
+
+		if (ret) {
+		drop:
+			net_dbg_skb_ratelimited("%s: Dropping handshake packet from %pISpfsc\n",
+						wg->dev->name, skb);
+			goto err;
+		}
+
+		atomic_inc(&wg->handshake_queue_len);
+		cpu = wg_cpumask_next_online(&wg->handshake_queue.last_cpu);
+		/* Queues up a call to packet_process_queued_handshake_packets(skb): */
+		queue_work_on(cpu, wg->handshake_receive_wq,
+			      &per_cpu_ptr(wg->handshake_queue.worker, cpu)->work);
+		break;
+	}
+	case cpu_to_le32(MESSAGE_DATA):
+		PACKET_CB(skb)->ds = ip_tunnel_get_dsfield(ip_hdr(skb), skb);
+		wg_packet_consume_data(wg, skb);
+		break;
+	default:
+		WARN(1, "Non-exhaustive parsing of packet header lead to unknown packet type!\n");
+		goto err;
+	}
+
+	return;
+
+err:
+	dev_kfree_skb(skb);
+}
diff --git a/wireguard-linux/drivers/net/wireguard/send.c b/wireguard-linux/drivers/net/wireguard/send.c
index 0d48e0f4a1ba..a5d19ad3c975 100644
--- a/wireguard-linux/drivers/net/wireguard/send.c
+++ b/wireguard-linux/drivers/net/wireguard/send.c
@@ -14,17 +14,21 @@
 #include <linux/uio.h>
 #include <linux/inetdevice.h>
 #include <linux/socket.h>
+#include <linux/wireguard.h>
 #include <net/ip_tunnels.h>
 #include <net/udp.h>
 #include <net/sock.h>
 
 static void wg_packet_send_handshake_initiation(struct wg_peer *peer)
 {
+	printk(KERN_INFO "Entering wg_packet_send_handshake_initiation with peer=%p\n", peer);
 	struct message_handshake_initiation packet;
 
 	if (!wg_birthdate_has_expired(atomic64_read(&peer->last_sent_handshake),
-				      REKEY_TIMEOUT))
+				      REKEY_TIMEOUT)) {
+		printk(KERN_INFO "Exiting wg_packet_send_handshake_initiation\n");
 		return; /* This function is rate limited. */
+	}
 
 	atomic64_set(&peer->last_sent_handshake, ktime_get_coarse_boottime_ns());
 	net_dbg_ratelimited("%s: Sending handshake initiation to peer %llu (%pISpfsc)\n",
@@ -33,6 +37,13 @@ static void wg_packet_send_handshake_initiation(struct wg_peer *peer)
 
 	if (wg_noise_handshake_create_initiation(&packet, &peer->handshake)) {
 		wg_cookie_add_mac_to_packet(&packet, sizeof(packet), peer);
+		
+		printk(KERN_INFO "MAC added to handshake initiation packet\n");
+		printk(KERN_INFO "Handshake Initiation Packet: %*ph\n",
+			(int)sizeof(packet), &packet);
+		printk(KERN_INFO "Peer Cookie Parameters: peer=%p, handshake=%p, index=%u\n",
+			peer, &peer->handshake, packet.sender_index);
+ 
 		wg_timers_any_authenticated_packet_traversal(peer);
 		wg_timers_any_authenticated_packet_sent(peer);
 		atomic64_set(&peer->last_sent_handshake,
@@ -41,15 +52,17 @@ static void wg_packet_send_handshake_initiation(struct wg_peer *peer)
 					      HANDSHAKE_DSCP);
 		wg_timers_handshake_initiated(peer);
 	}
+	printk(KERN_INFO "Exiting wg_packet_send_handshake_initiation\n");
 }
-
 void wg_packet_handshake_send_worker(struct work_struct *work)
 {
+	printk(KERN_INFO "Entering wg_packet_handshake_send_worker with work=%p\n", work);
 	struct wg_peer *peer = container_of(work, struct wg_peer,
 					    transmit_handshake_work);
 
 	wg_packet_send_handshake_initiation(peer);
 	wg_peer_put(peer);
+	printk(KERN_INFO "Exiting wg_packet_handshake_send_worker\n");
 }
 
 void wg_packet_send_queued_handshake_initiation(struct wg_peer *peer,
@@ -93,6 +106,7 @@ void wg_packet_send_handshake_response(struct wg_peer *peer)
 
 	if (wg_noise_handshake_create_response(&packet, &peer->handshake)) {
 		wg_cookie_add_mac_to_packet(&packet, sizeof(packet), peer);
+		
 		if (wg_noise_handshake_begin_session(&peer->handshake,
 						     &peer->keypairs)) {
 			wg_timers_session_derived(peer);
@@ -113,10 +127,13 @@ void wg_packet_send_handshake_cookie(struct wg_device *wg,
 {
 	struct message_handshake_cookie packet;
 
+	(int)initiating_skb->len, initiating_skb->data);
+	
 	net_dbg_skb_ratelimited("%s: Sending cookie response for denied handshake message for %pISpfsc\n",
 				wg->dev->name, initiating_skb);
 	wg_cookie_message_create(&packet, initiating_skb, sender_index,
 				 &wg->cookie_checker);
+	
 	wg_socket_send_buffer_as_reply_to_skb(wg, initiating_skb, &packet,
 					      sizeof(packet));
 }
@@ -142,8 +159,9 @@ static unsigned int calculate_skb_padding(struct sk_buff *skb)
 {
 	unsigned int padded_size, last_unit = skb->len;
 
-	if (unlikely(!PACKET_CB(skb)->mtu))
+	if (unlikely(!PACKET_CB(skb)->mtu)) {
 		return ALIGN(last_unit, MESSAGE_PADDING_MULTIPLE) - last_unit;
+	}
 
 	/* We do this modulo business with the MTU, just in case the networking
 	 * layer gives us a packet that's bigger than the MTU. In that case, we
@@ -159,6 +177,7 @@ static unsigned int calculate_skb_padding(struct sk_buff *skb)
 	return padded_size - last_unit;
 }
 
+#ifdef ORIGINAL
 static bool encrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 {
 	unsigned int padding_len, plaintext_len, trailer_len;
@@ -179,8 +198,10 @@ static bool encrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 
 	/* Expand data section to have room for padding and auth tag. */
 	num_frags = skb_cow_data(skb, trailer_len, &trailer);
-	if (unlikely(num_frags < 0 || num_frags > ARRAY_SIZE(sg)))
+	if (unlikely(num_frags < 0 || num_frags > ARRAY_SIZE(sg))) {
+		printk(KERN_INFO "Exiting encrypt_packet\n");
 		return false;
+	}
 
 	/* Set the padding to zeros, and make sure it and the auth tag are part
 	 * of the skb.
@@ -190,13 +211,17 @@ static bool encrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 	/* Expand head section to have room for our header and the network
 	 * stack's headers.
 	 */
-	if (unlikely(skb_cow_head(skb, DATA_PACKET_HEAD_ROOM) < 0))
+	if (unlikely(skb_cow_head(skb, DATA_PACKET_HEAD_ROOM) < 0)) {
+		printk(KERN_INFO "Exiting encrypt_packet\n");
 		return false;
+	}
 
 	/* Finalize checksum calculation for the inner packet, if required. */
 	if (unlikely(skb->ip_summed == CHECKSUM_PARTIAL &&
-		     skb_checksum_help(skb)))
+		     skb_checksum_help(skb))) {
+		printk(KERN_INFO "Exiting encrypt_packet\n");
 		return false;
+	}
 
 	/* Only after checksumming can we safely add on the padding at the end
 	 * and the header.
@@ -211,12 +236,70 @@ static bool encrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
 	/* Now we can encrypt the scattergather segments */
 	sg_init_table(sg, num_frags);
 	if (skb_to_sgvec(skb, sg, sizeof(struct message_data),
-			 noise_encrypted_len(plaintext_len)) <= 0)
+			 noise_encrypted_len(plaintext_len)) <= 0) {
 		return false;
+	}
 	return chacha20poly1305_encrypt_sg_inplace(sg, plaintext_len, NULL, 0,
 						   PACKET_CB(skb)->nonce,
 						   keypair->sending.key);
 }
+#endif // ORIGINAL
+
+static bool encrypt_packet(struct sk_buff *skb, struct noise_keypair *keypair)
+{
+    unsigned int padding_len, plaintext_len, trailer_len;
+    struct scatterlist sg[MAX_SKB_FRAGS + 8];
+    struct message_data *header;
+    struct sk_buff *trailer;
+    int num_frags;
+
+    // Force hash calculation before encryption
+    skb_get_hash(skb);
+
+    // Calculate lengths
+    padding_len = calculate_skb_padding(skb);
+    trailer_len = padding_len + noise_encrypted_len(0);
+    plaintext_len = skb->len + padding_len;
+
+    // Expand data section
+    num_frags = skb_cow_data(skb, trailer_len, &trailer);
+    if (unlikely(num_frags < 0 || num_frags > ARRAY_SIZE(sg))) {
+        return false;
+    }
+
+    // Set the padding to zeros
+    memset(skb_tail_pointer(trailer), 0, padding_len);
+
+    // Expand head section
+    if (unlikely(skb_cow_head(skb, DATA_PACKET_HEAD_ROOM) < 0)) {
+        return false;
+    }
+
+    // Finalize checksum calculation
+    if (unlikely(skb->ip_summed == CHECKSUM_PARTIAL && skb_checksum_help(skb))) {
+        return false;
+    }
+
+    // Add padding and header
+    skb_set_inner_network_header(skb, 0);
+    header = (struct message_data *)skb_push(skb, sizeof(*header));
+    header->header.type = cpu_to_le32(MESSAGE_DATA);
+    header->key_idx = keypair->remote_index;
+    header->counter = cpu_to_le64(PACKET_CB(skb)->nonce);
+#define NOISE_KEY_LEN 32
+    pskb_put(skb, trailer, trailer_len);
+
+    // Encrypt the scattergather segments
+    sg_init_table(sg, num_frags);
+    if (skb_to_sgvec(skb, sg, sizeof(struct message_data), noise_encrypted_len(plaintext_len)) <= 0) {
+        return false;
+    }
+
+    bool success = chacha20poly1305_encrypt_sg_inplace(sg, plaintext_len, NULL, 0,
+                                                       PACKET_CB(skb)->nonce,
+                                                       keypair->sending.key);
+    return success;
+}
 
 void wg_packet_send_keepalive(struct wg_peer *peer)
 {
@@ -225,8 +308,10 @@ void wg_packet_send_keepalive(struct wg_peer *peer)
 	if (skb_queue_empty(&peer->staged_packet_queue)) {
 		skb = alloc_skb(DATA_PACKET_HEAD_ROOM + MESSAGE_MINIMUM_LENGTH,
 				GFP_ATOMIC);
-		if (unlikely(!skb))
+		if (unlikely(!skb)) {
+			printk(KERN_INFO "Exiting wg_packet_send_keepalive\n");
 			return;
+		}
 		skb_reserve(skb, DATA_PACKET_HEAD_ROOM);
 		skb->dev = peer->device->dev;
 		PACKET_CB(skb)->mtu = skb->dev->mtu;
@@ -308,6 +393,8 @@ void wg_packet_encrypt_worker(struct work_struct *work)
 	}
 }
 
+
+
 static void wg_packet_create_data(struct wg_peer *peer, struct sk_buff *first)
 {
 	struct wg_device *wg = peer->device;
@@ -317,14 +404,18 @@ static void wg_packet_create_data(struct wg_peer *peer, struct sk_buff *first)
 	if (unlikely(READ_ONCE(peer->is_dead)))
 		goto err;
 
+	
 	ret = wg_queue_enqueue_per_device_and_peer(&wg->encrypt_queue, &peer->tx_queue, first,
 						   wg->packet_crypt_wq);
 	if (unlikely(ret == -EPIPE))
 		wg_queue_enqueue_per_peer_tx(first, PACKET_STATE_DEAD);
+
 err:
 	rcu_read_unlock_bh();
-	if (likely(!ret || ret == -EPIPE))
+	if (likely(!ret || ret == -EPIPE)) {
+		printk(KERN_INFO "Exiting wg_packet_create_data\n");
 		return;
+	}
 	wg_noise_keypair_put(PACKET_CB(first)->keypair, false);
 	wg_peer_put(peer);
 	kfree_skb_list(first);
@@ -350,8 +441,10 @@ void wg_packet_send_staged_packets(struct wg_peer *peer)
 	spin_lock_bh(&peer->staged_packet_queue.lock);
 	skb_queue_splice_init(&peer->staged_packet_queue, &packets);
 	spin_unlock_bh(&peer->staged_packet_queue.lock);
-	if (unlikely(skb_queue_empty(&packets)))
+	if (unlikely(skb_queue_empty(&packets))) {
+		printk(KERN_INFO "Exiting wg_packet_send_staged_packets\n");
 		return;
+	}
 
 	/* First we make sure we have a valid reference to a valid key. */
 	rcu_read_lock_bh();
diff --git a/wireguard-linux/drivers/net/wireguard/socket.c b/wireguard-linux/drivers/net/wireguard/socket.c
index 0414d7a6ce74..157a520386d0 100644
--- a/wireguard-linux/drivers/net/wireguard/socket.c
+++ b/wireguard-linux/drivers/net/wireguard/socket.c
@@ -3,23 +3,350 @@
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
+/* TCP Support by Jeff Nathan and Dragos Ruiu 2024-03-16 */
+
 #include "device.h"
 #include "peer.h"
 #include "socket.h"
 #include "queueing.h"
 #include "messages.h"
 
+#include <asm/byteorder.h> // For ntohl
 #include <linux/ctype.h>
-#include <linux/net.h>
 #include <linux/if_vlan.h>
 #include <linux/if_ether.h>
 #include <linux/inetdevice.h>
+#include <linux/wireguard.h>
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/net.h>
+#include <linux/tcp.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include <linux/socket.h>
+#include <linux/in6.h>
+#include <net/checksum.h>
 #include <net/udp_tunnel.h>
 #include <net/ipv6.h>
+#include <net/sock.h>
+#include <net/udp.h>
+#include <net/inet_sock.h>
+#include <net/inet_common.h>
+
+
+
+struct wg_tcp_socket_list_entry {
+    struct socket *tcp_socket;        // Socket associated with the connection
+    struct sockaddr_storage src_addr; // Source address for the connection
+    struct wg_peer *temp_peer;	      // temporary peer for dataready
+    struct list_head tcp_connection_ll;  // List pointer for the linked list
+    ktime_t timestamp;                // Timestamp when the connection was added
+};
+
+struct wg_socket_data {
+	struct wg_device *device;
+	struct wg_peer *peer;
+	bool inbound;
+};
+
+// Global structure to hold default network interface information
+struct default_interface_info {
+    struct net_device *dev;   // Default network interface
+    __be32 ipv4_address;      // IPv4 address of the default interface
+    struct in6_addr ipv6_address; // IPv6 address of the default interface
+    bool ipv4_available;      // Flag indicating if IPv4 is available
+    bool ipv6_available;      // Flag indicating if IPv6 is available
+};
+
+extern struct default_interface_info default_iface_info;
+
+void wg_setup_tcp_socket_callbacks(struct wg_peer *peer, bool inbound);
+void wg_reset_tcp_socket_callbacks(struct wg_peer *peer, bool inbound);
+void wg_get_endpoint_from_socket(struct socket *epsocket, struct endpoint *ep);
+void log_wireguard_endpoint(struct endpoint *ep);
+
+// ******** DIAGNOSTIC CODE ********
+
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/rcupdate.h>
+#include <linux/kref.h>
+#include <linux/syslog.h>
+
+// Function to print details of sk_buff_head for diagnostic purposes
+void print_skbuff_head_info(const char *label, struct sk_buff_head *queue);
+
+void print_skbuff_head_info(const char *label, struct sk_buff_head *queue)
+{
+	const struct sk_buff *skb;
+	unsigned long flags;
+
+	printk(KERN_INFO "%s:\n", label);
+	if (!queue) {
+		printk(KERN_INFO "Queue is NULL\n");
+		return;
+	}
+
+	spin_lock_irqsave(&queue->lock, flags);
+	skb_queue_walk(queue, skb) {
+		printk(KERN_INFO "Packet: len=%u, data_len=%u, users=%d\n",
+		        skb->len, skb->data_len, refcount_read(&skb->users));
+	}
+	spin_unlock_irqrestore(&queue->lock, flags);
+}
+
+void print_wg_peer(struct wg_peer *peer);
+
+void print_wg_peer(struct wg_peer *peer)
+{
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_ERR "NULL wg_peer provided\n");
+		return;
+	}
+
+	printk(KERN_INFO "WG Peer Complete Diagnostic Info:\n");
+	printk(KERN_INFO "Device Pointer: %p, Serial Work CPU: %d, "
+			"Is Dead: %d, (Device) Transport Mode: %u\n",
+			peer->device, peer->serial_work_cpu, peer->is_dead,
+			peer->device->transport);
+	printk(KERN_INFO "RX Bytes: %llu, TX Bytes: %llu, Internal ID: %llu\n",
+			peer->rx_bytes, peer->tx_bytes, peer->internal_id);
+	printk(KERN_INFO "Last Sent Handshake: %llu\n",
+			atomic64_read(&peer->last_sent_handshake));
+
+	// Endpoint info
+	printk(KERN_INFO "Endpoint Address Family: %u\n",
+			peer->endpoint.addr.sa_family);
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+        printk(KERN_INFO "IPv4 Address: %pI4, IPv4 Source: %pI4, "
+			"Interface: %d\n",
+			&peer->endpoint.addr4.sin_addr, &peer->endpoint.src4,
+			peer->endpoint.src_if4);
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		printk(KERN_INFO "IPv6 Address: %pI6c, IPv6 Source: %pI6c\n",
+				&peer->endpoint.addr6.sin6_addr, &peer->endpoint.src6);
+	}
+
+	// Correctly accessing sk_buff_head queues
+	if (!skb_queue_empty(&peer->staged_packet_queue)) {
+		print_skbuff_head_info("Staged Packet Queue",
+				&peer->staged_packet_queue);
+	} else {
+		printk(KERN_INFO "Staged Packet Queue: NULL\n");
+	}
+
+	// Additional diagnostics and corrections for TCP
+	if (peer->peer_socket) {
+		printk(KERN_INFO "TCP Socket: %p, Established: %d\n",
+				peer->peer_socket, peer->tcp_established);
+		if (!skb_queue_empty(&peer->tcp_packet_queue)) {
+			print_skbuff_head_info("TCP Packet Queue",
+					&peer->tcp_packet_queue);
+		} else {
+			printk(KERN_INFO "TCP Packet Queue: NULL\n");
+		}
+	} else {
+		printk(KERN_INFO "TCP Socket: NULL\n");
+	}
+
+	// Timer diagnostics
+	printk(KERN_INFO "Timer for Retransmit Handshake Expires: %ld\n",
+			peer->timer_retransmit_handshake.expires);
+	printk(KERN_INFO "Timer for Sending Keepalive Expires: %ld\n",
+			peer->timer_send_keepalive.expires);
+	printk(KERN_INFO "Timer for New Handshake Expires: %ld\n",
+			peer->timer_new_handshake.expires);
+	printk(KERN_INFO "Timer for Zero Key Material Expires: %ld\n",
+			peer->timer_zero_key_material.expires);
+	printk(KERN_INFO "Timer for Persistent Keepalive Expires: %ld\n",
+			peer->timer_persistent_keepalive.expires);
+
+	// RCU and reference count
+	printk(KERN_INFO "RCU Head Address: %p, Reference Count: %d\n",
+			&peer->rcu, kref_read(&peer->refcount));
+}
+
+// Function to print information about crypt_queue
+void print_crypt_queue(const char *label, struct crypt_queue *queue);
+
+void print_crypt_queue(const char *label, struct crypt_queue *queue)
+{
+	if (!queue) {
+		printk(KERN_INFO "%s: NULL\n", label);
+		return;
+	}
+
+	printk(KERN_INFO "%s:\n", label);
+	printk(KERN_INFO "  Last CPU used: %d\n", queue->last_cpu);
+	// Assuming you have a way to inspect ptr_ring structure:
+	// printk(KERN_INFO "  Ring capacity: %d\n", queue->ring.size);
+	if (queue->worker)
+		printk(KERN_INFO "  Worker pointer: %p\n", queue->worker);
+	else
+		printk(KERN_INFO "  Worker: NULL\n");
+}
+
+// Diagnostic function for wg_device
+void print_wg_device(struct wg_device *device);
+
+void print_wg_device(struct wg_device *device)
+{
+	if (!device) {
+		printk(KERN_ERR "NULL wg_device provided\n");
+		return;
+	}
+
+	printk(KERN_INFO "WG Device Diagnostic Info:\n");
+
+	if (device->dev)
+		printk(KERN_INFO "Net device: %s\n", device->dev->name);
+	else
+		printk(KERN_INFO "Net device: NULL\n");
+
+	print_crypt_queue("Encrypt Queue", &(device->encrypt_queue));
+	print_crypt_queue("Decrypt Queue", &(device->decrypt_queue));
+	print_crypt_queue("Handshake Queue", &(device->handshake_queue));
+
+	if (rcu_access_pointer(device->tcp_listen_socket4))
+		printk(KERN_INFO "IPv4 Socket: %p\n", device->tcp_listen_socket4);
+	else
+		printk(KERN_INFO "IPv4 Socket: NULL\n");
+
+	if (rcu_access_pointer(device->tcp_listen_socket6))
+		printk(KERN_INFO "IPv6 Socket: %p\n", device->tcp_listen_socket6);
+	else
+		printk(KERN_INFO "IPv6 Socket: NULL\n");
+
+	if (rcu_access_pointer(device->tcp_listen_socket4))
+		printk(KERN_INFO "TCP Listener IPv4 Socket: %p\n",
+				device->tcp_listen_socket4);
+	else
+		printk(KERN_INFO "TCP Listener IPv4 Socket: NULL\n");
+
+	if (rcu_access_pointer(device->tcp_listen_socket6))
+		printk(KERN_INFO "TCP Listener IPv6 Socket: %p\n",
+				device->tcp_listen_socket6);
+	else
+		printk(KERN_INFO "TCP Listener IPv6 Socket: NULL\n");
+
+	if (device->creating_net)
+		printk(KERN_INFO "Creating net namespace: %p\n",
+				device->creating_net);
+	else
+		printk(KERN_INFO "Creating net namespace: NULL\n");
+
+	// Assuming noise_static_identity and other structures have similar diagnostic print functions
+	printk(KERN_INFO "Static Identity: (printing details not implemented)\n");
+	printk(KERN_INFO "Workqueues and other components would similarly have their details printed based on available data.\n");
+
+	printk(KERN_INFO "FW Mark: %u, Incoming Port: %u, Transport: %u\n", device->fwmark, device->incoming_port, device->transport);
+	printk(KERN_INFO "Handshake queue length: %d\n", atomic_read(&device->handshake_queue_len));
+	printk(KERN_INFO "Number of Peers: %u, Device Update Generation: %u\n", device->num_peers, device->device_update_gen);
+}
+
+
+// Diagnostic function to print TCP state and sk_user_data
+void print_tcp_socket_info(struct socket *sock, const char *label) {
+    struct sock *sk;
+    struct wg_socket_data *user_data;
+    int tcp_state = -1;
+
+    if (sock && sock->sk) {
+        sk = sock->sk;
+        user_data = (struct wg_socket_data *)sk->sk_user_data;
+        tcp_state = (sk->sk_protocol == IPPROTO_TCP) ? sk->sk_state : -1;
+        if (user_data) {
+            printk(KERN_INFO "%s: socket=%p, sk_user_data=%p (device=%p, peer=%p, inbound=%d), TCP state=%d\n",
+                   label, sock, user_data, user_data->device, user_data->peer, user_data->inbound, tcp_state);
+        } else {
+            printk(KERN_INFO "%s: socket=%p, sk_user_data=NULL, TCP state=%d\n",
+                   label, sock, tcp_state);
+        }
+    } else {
+        printk(KERN_INFO "%s: Socket or sk is NULL\n", label);
+    }
+}
+
+// Function to print compact diagnostic information for all sockets in a peer
+void print_peer_socket_info(struct wg_peer *peer) {
+    if (!peer) {
+        printk(KERN_INFO "print_peer_socket_info: peer is NULL\n");
+        return;
+    }
+
+    // Print the pointers to the main sockets in the peer
+    printk(KERN_INFO "Peer: %p, peer_socket=%p, inbound_socket=%p, outbound_socket=%p\n",
+           peer, peer->peer_socket, peer->inbound_socket, peer->outbound_socket);
+
+    // Print inbound timestamp
+    printk(KERN_INFO "Inbound timestamp: %llu ns\n", ktime_to_ns(peer->inbound_timestamp));
+
+    // Print outbound timestamp
+    printk(KERN_INFO "Outbound timestamp: %llu ns\n", ktime_to_ns(peer->outbound_timestamp));
+
+    // Print combined information for inbound socket
+    if (peer->inbound_socket) {
+        print_tcp_socket_info(peer->inbound_socket, "Inbound Socket");
+    } else {
+        printk(KERN_INFO "Inbound Socket is NULL\n");
+    }
+
+    // Print combined information for outbound socket
+    if (peer->outbound_socket) {
+        print_tcp_socket_info(peer->outbound_socket, "Outbound Socket");
+    } else {
+        printk(KERN_INFO "Outbound Socket is NULL\n");
+    }
+
+    // Additional validation check
+    if (peer->peer_socket == peer->inbound_socket) {
+        printk(KERN_INFO "peer_socket matches inbound_socket\n");
+    } else if (peer->peer_socket == peer->outbound_socket) {
+        printk(KERN_INFO "peer_socket matches outbound_socket\n");
+    } else {
+        printk(KERN_WARNING "peer_socket does not match inbound_socket or outbound_socket\n");
+    }
+}
+// ******** END OF DIAGNOSTIC CODE ********
+
+
+
+// Function to create and return an endpoint from source and destination sockaddr_in
+struct endpoint create_endpoint(const struct sockaddr_in *source, const struct sockaddr_in *destination) {
+    struct endpoint ep;
+
+    // Initialize the endpoint to zero
+    memset(&ep, 0, sizeof(struct endpoint));
+
+    // Set the address family to AF_INET
+    ep.addr.sa_family = AF_INET;
+
+    // Copy the destination address to the endpoint's addr4 field
+    ep.addr4.sin_family = AF_INET;
+    ep.addr4.sin_port = destination->sin_port;       // Destination port (network byte order)
+    ep.addr4.sin_addr = destination->sin_addr;       // Destination IP address
+
+    // Copy the source address to the endpoint's source fields
+    ep.src4 = source->sin_addr;                      // Source IP address
+    // ep.src_if4 can be set here if you have an interface index
+
+    return ep; // Return the populated endpoint structure
+}
+
 
 static int send4(struct wg_device *wg, struct sk_buff *skb,
 		 struct endpoint *endpoint, u8 ds, struct dst_cache *cache)
 {
+	printk(KERN_INFO "Entering function send4\n");
+
+
 	struct flowi4 fl = {
 		.saddr = endpoint->src4.s_addr,
 		.daddr = endpoint->addr4.sin_addr.s_addr,
@@ -92,12 +419,16 @@ static int send4(struct wg_device *wg, struct sk_buff *skb,
 out:
 	rcu_read_unlock_bh();
 	return ret;
+	printk(KERN_INFO "Exiting function send4\n");
 }
 
 static int send6(struct wg_device *wg, struct sk_buff *skb,
 		 struct endpoint *endpoint, u8 ds, struct dst_cache *cache)
 {
+	printk(KERN_INFO "Entering function send6\n");
 #if IS_ENABLED(CONFIG_IPV6)
+
+
 	struct flowi6 fl = {
 		.saddr = endpoint->src6,
 		.daddr = endpoint->addr6.sin6_addr,
@@ -158,52 +489,183 @@ static int send6(struct wg_device *wg, struct sk_buff *skb,
 	kfree_skb(skb);
 out:
 	rcu_read_unlock_bh();
+	printk(KERN_INFO "Exiting function send6\n");
 	return ret;
 #else
 	kfree_skb(skb);
+	printk(KERN_INFO "Exiting function send6\n");
 	return -EAFNOSUPPORT;
 #endif
+	printk(KERN_INFO "Exiting function send6\n");
+}
+
+void wg_tcp_transfer_worker(struct work_struct *work);
+
+void wg_tcp_transfer_worker(struct work_struct *work)
+{
+	printk(KERN_INFO "Entering wg_tcp_transfer_worker with work=%p\n", work);
+
+	// Validate the work structure
+	if (!work) {
+		printk(KERN_ERR "wg_tcp_transfer_worker: work is NULL\n");
+		return;
+	}
+
+	// Get the wg_tcp_transfer_work container
+	struct wg_tcp_transfer_work *work_item = container_of(work, struct wg_tcp_transfer_work, work);
+
+	// Validate the work_item structure
+	if (!work_item) {
+		printk(KERN_ERR "wg_tcp_transfer_worker: work_item is NULL\n");
+		return;
+	}
+
+	// Extract peer and skb from the work item
+	struct wg_peer *peer = work_item->peer;
+	struct sk_buff *skb = work_item->skb;
+
+	// Validate the peer structure
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_ERR "wg_tcp_transfer_worker: peer is NULL or invalid\n");
+		goto out;
+	}
+	print_peer_socket_info(peer);
+	// Validate the sk_buff structure before processing
+	if (skb) {
+		// Log diagnostic information about the skb
+		printk(KERN_INFO "wg_tcp_transfer_worker: Processing skb with len=%u peer=%p\n", skb->len, peer);
+
+		// Process the packet by passing it to wg_tcp_queuepkt
+		wg_tcp_queuepkt(peer, skb->data, skb->len);
+	} else {
+		printk(KERN_ERR "wg_tcp_transfer_worker: skb is NULL\n");
+	}
+
+out:
+	// Free the skb if it was allocated
+	if (skb) {
+		kfree_skb(skb);
+	}
+
+	// Free the work item structure
+	if (work_item) {
+		kfree(work_item);
+	}
+
+	// Validate and update the peer's tcp_transfer_worker_scheduled flag
+	if (peer && !IS_ERR(peer)) {
+		spin_lock_bh(&peer->tcp_transfer_lock);
+		peer->tcp_transfer_worker_scheduled = false;
+		spin_unlock_bh(&peer->tcp_transfer_lock);
+	} else {
+		printk(KERN_ERR "wg_tcp_transfer_worker: peer is NULL or invalid during cleanup\n");
+	}
+
+	printk(KERN_INFO "Exiting wg_tcp_transfer_worker\n");
 }
 
+
 int wg_socket_send_skb_to_peer(struct wg_peer *peer, struct sk_buff *skb, u8 ds)
 {
+	printk(KERN_INFO "Entering function wg_socket_send_skb_to_peer\n");
 	size_t skb_len = skb->len;
 	int ret = -EAFNOSUPPORT;
 
-	read_lock_bh(&peer->endpoint_lock);
-	if (peer->endpoint.addr.sa_family == AF_INET)
-		ret = send4(peer->device, skb, &peer->endpoint, ds,
-			    &peer->endpoint_cache);
-	else if (peer->endpoint.addr.sa_family == AF_INET6)
-		ret = send6(peer->device, skb, &peer->endpoint, ds,
-			    &peer->endpoint_cache);
-	else
-		dev_kfree_skb(skb);
-	if (likely(!ret))
-		peer->tx_bytes += skb_len;
-	read_unlock_bh(&peer->endpoint_lock);
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		ret = -EINVAL;
+		goto out;
+	}
+	if (unlikely(!skb)){
+		ret = -ENOMEM;
+		goto out;
+	}
+	
+	print_peer_socket_info(peer);
+	
+        if (peer->device->transport == WG_TRANSPORT_TCP) {
+		struct wg_tcp_transfer_work *work_item;
 
+		/* Allocate the work item */
+		work_item = kmalloc(sizeof(*work_item), GFP_ATOMIC);
+		if (unlikely(!work_item)) {
+			printk(KERN_INFO "Exiting wg_queue_enqueue_per_peer_tx - UNABLE TO ALLOCATE WORK ITEM \n");
+//			wg_peer_put(peer);
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		/* Initialize the work item */
+		INIT_WORK(&work_item->work, wg_tcp_transfer_worker);
+		work_item->skb = skb;
+		work_item->peer = peer;
+
+		printk(KERN_INFO "Work item scheduled peer=%p skb=%p\n", peer, skb);
+		// Check if the worker is already scheduled
+		if (!peer->tcp_transfer_worker_scheduled) {
+			peer->tcp_transfer_worker_scheduled = true;
+			queue_work(peer->tcp_transfer_wq, &work_item->work);
+		}
+		ret = 0;
+	} else {
+		read_lock_bh(&peer->endpoint_lock);	
+		if (peer->endpoint.addr.sa_family == AF_INET)
+			ret = send4(peer->device, skb, &peer->endpoint, ds,
+		    		&peer->endpoint_cache);
+		else if (peer->endpoint.addr.sa_family == AF_INET6)
+			ret = send6(peer->device, skb, &peer->endpoint, ds,
+			    	&peer->endpoint_cache);
+		else {
+			read_unlock_bh(&peer->endpoint_lock);
+			dev_kfree_skb(skb);
+			printk(KERN_INFO "Exiting function wg_socket_send_skb_to_peer\n");
+			return -EAGAIN;
+		}
+		read_unlock_bh(&peer->endpoint_lock);
+	}
+	peer->tx_bytes += skb_len;
+out:
+	printk(KERN_INFO "Exiting function wg_socket_send_skb_to_peer\n");
 	return ret;
+
 }
 
 int wg_socket_send_buffer_to_peer(struct wg_peer *peer, void *buffer,
 				  size_t len, u8 ds)
 {
+	int ret;
+	
+	printk(KERN_INFO "Entering function wg_socket_send_buffer_to_peer peer=%p\n", peer);
+	log_wireguard_endpoint(&peer->endpoint);
 	struct sk_buff *skb = alloc_skb(len + SKB_HEADER_LEN, GFP_ATOMIC);
 
-	if (unlikely(!skb))
-		return -ENOMEM;
+	printk(KERN_INFO "Sending buffer to peer - Length: %zu, Data: %*ph\n",
+		len, (int)len, buffer);
 
+	
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))) {
+		ret = -EINVAL;
+		goto out;
+	}
+	if (unlikely(!skb)){
+		ret = -ENOMEM;
+		goto out;
+	}
+	
 	skb_reserve(skb, SKB_HEADER_LEN);
 	skb_set_inner_network_header(skb, 0);
 	skb_put_data(skb, buffer, len);
-	return wg_socket_send_skb_to_peer(peer, skb, ds);
+	ret = wg_socket_send_skb_to_peer(peer, skb, ds);
+
+out:
+	printk(KERN_INFO "Exiting function wg_socket_send_buffer_to_peer\n");
+	return ret;
 }
 
 int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg,
 					  struct sk_buff *in_skb, void *buffer,
 					  size_t len)
 {
+	printk(KERN_INFO "Entering function wg_socket_send_buffer_as_reply_to_skb\n");
 	int ret = 0;
 	struct sk_buff *skb;
 	struct endpoint endpoint;
@@ -229,12 +691,17 @@ int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg,
 	 * as we checked above.
 	 */
 
+	printk(KERN_INFO "Exiting function wg_socket_send_buffer_as_reply_to_skb\n");
 	return ret;
 }
 
-int wg_socket_endpoint_from_skb(struct endpoint *endpoint,
-				const struct sk_buff *skb)
+
+int wg_socket_endpoint_from_skb(struct endpoint *endpoint, const struct sk_buff *skb)
 {
+	printk(KERN_INFO "Entering function wg_socket_endpoint_from_skb\n");
+
+	printk(KERN_INFO "skb data: %*ph\n", skb->len, skb->data);
+	
 	memset(endpoint, 0, sizeof(*endpoint));
 	if (skb->protocol == htons(ETH_P_IP)) {
 		endpoint->addr4.sin_family = AF_INET;
@@ -242,21 +709,29 @@ int wg_socket_endpoint_from_skb(struct endpoint *endpoint,
 		endpoint->addr4.sin_addr.s_addr = ip_hdr(skb)->saddr;
 		endpoint->src4.s_addr = ip_hdr(skb)->daddr;
 		endpoint->src_if4 = skb->skb_iif;
+		printk(KERN_INFO "wg_socket_endpoint_from_skb: Extracted IPv4 address %pI4:%d\n",
+		       &endpoint->addr4.sin_addr, ntohs(endpoint->addr4.sin_port));
 	} else if (IS_ENABLED(CONFIG_IPV6) && skb->protocol == htons(ETH_P_IPV6)) {
 		endpoint->addr6.sin6_family = AF_INET6;
 		endpoint->addr6.sin6_port = udp_hdr(skb)->source;
 		endpoint->addr6.sin6_addr = ipv6_hdr(skb)->saddr;
-		endpoint->addr6.sin6_scope_id = ipv6_iface_scope_id(
-			&ipv6_hdr(skb)->saddr, skb->skb_iif);
+		endpoint->addr6.sin6_scope_id = ipv6_iface_scope_id(&ipv6_hdr(skb)->saddr, skb->skb_iif);
 		endpoint->src6 = ipv6_hdr(skb)->daddr;
+		printk(KERN_INFO "wg_socket_endpoint_from_skb: Extracted IPv6 address %pI6c:%d\n",
+		       &endpoint->addr6.sin6_addr, ntohs(endpoint->addr6.sin6_port));
 	} else {
 		return -EINVAL;
 	}
+
+	
+	printk(KERN_INFO "Exiting function wg_socket_endpoint_from_skb\n");
 	return 0;
 }
 
-static bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
+bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
 {
+	printk(KERN_INFO "Entering function endpoint_eq\n");
+	printk(KERN_INFO "Exiting function endpoint_eq\n");
 	return (a->addr.sa_family == AF_INET && b->addr.sa_family == AF_INET &&
 		a->addr4.sin_port == b->addr4.sin_port &&
 		a->addr4.sin_addr.s_addr == b->addr4.sin_addr.s_addr &&
@@ -270,88 +745,158 @@ static bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
 	       unlikely(!a->addr.sa_family && !b->addr.sa_family);
 }
 
-void wg_socket_set_peer_endpoint(struct wg_peer *peer,
-				 const struct endpoint *endpoint)
+static void wg_release_peer_tcp_connection(struct wg_peer *peer);
+
+void wg_socket_set_peer_endpoint(struct wg_peer *peer, const struct endpoint *endpoint)
 {
+	char addr_str[INET6_ADDRSTRLEN];
+	printk(KERN_INFO "Entering function wg_socket_set_peer_endpoint peer=%p\n", peer);
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		goto out;
+	}
+
 	/* First we check unlocked, in order to optimize, since it's pretty rare
 	 * that an endpoint will change. If we happen to be mid-write, and two
 	 * CPUs wind up writing the same thing or something slightly different,
 	 * it doesn't really matter much either.
 	 */
-	if (endpoint_eq(endpoint, &peer->endpoint))
+	if (endpoint_eq(endpoint, &peer->endpoint)) {
+		printk(KERN_INFO "Exiting function wg_socket_set_peer_endpoint (no change in endpoint)\n");
 		return;
-	write_lock_bh(&peer->endpoint_lock);
+	}
+
+	print_peer_socket_info(peer);
+	
 	if (endpoint->addr.sa_family == AF_INET) {
+		snprintf(addr_str, INET_ADDRSTRLEN, "%pI4", &endpoint->addr4.sin_addr);
+		printk(KERN_INFO "Setting endpoint address: %s:%d\n", addr_str,
+		       ntohs(endpoint->addr4.sin_port));
+		write_lock_bh(&peer->endpoint_lock);
 		peer->endpoint.addr4 = endpoint->addr4;
 		peer->endpoint.src4 = endpoint->src4;
 		peer->endpoint.src_if4 = endpoint->src_if4;
+		write_unlock_bh(&peer->endpoint_lock);  // Unlock before making connection
 	} else if (IS_ENABLED(CONFIG_IPV6) && endpoint->addr.sa_family == AF_INET6) {
+		snprintf(addr_str, INET6_ADDRSTRLEN, "%pI6", &endpoint->addr6.sin6_addr);
+		printk(KERN_INFO "Setting endpoint address: [%s]:%d\n",
+		       addr_str, ntohs(endpoint->addr6.sin6_port));
+		write_lock_bh(&peer->endpoint_lock);
 		peer->endpoint.addr6 = endpoint->addr6;
 		peer->endpoint.src6 = endpoint->src6;
+		write_unlock_bh(&peer->endpoint_lock);  // Unlock before making connection
+
 	} else {
 		goto out;
 	}
 	dst_cache_reset(&peer->endpoint_cache);
+	peer->tcp_reply_endpoint = peer->endpoint;
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		// For IPv4 address
+		peer->endpoint.addr4.sin_port = htons(peer->device->incoming_port);
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		// For IPv6 address
+		peer->endpoint.addr6.sin6_port = htons(peer->device->incoming_port);
+	} else {
+		pr_err("Unsupported address family\n");
+	}
+
+	printk(KERN_INFO "Peer Endpoint:\n");
+	log_wireguard_endpoint(&peer->endpoint);
+	printk(KERN_INFO "TCP Reply Endpoint:\n");
+	log_wireguard_endpoint(&peer->tcp_reply_endpoint);
+
+	if (!peer->peer_endpoint_set) {
+		peer->peer_endpoint = peer->endpoint;
+		peer->peer_endpoint_set;
+		printk(KERN_INFO "wg_set_peer_endpoint: setting peer->peer_endpoint\n");
+	}
+	if (peer->device->transport == WG_TRANSPORT_TCP && !peer->tcp_established)
+		wg_tcp_connect(peer);
+
 out:
-	write_unlock_bh(&peer->endpoint_lock);
+	printk(KERN_INFO "Exiting function wg_socket_set_peer_endpoint\n");
 }
 
 void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer,
 					  const struct sk_buff *skb)
 {
+	printk(KERN_INFO "Entering function wg_socket_set_peer_endpoint_from_skb peer=%p\n", peer);
 	struct endpoint endpoint;
 
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		goto out;
+	}
+	
 	if (!wg_socket_endpoint_from_skb(&endpoint, skb))
 		wg_socket_set_peer_endpoint(peer, &endpoint);
+	log_wireguard_endpoint(&peer->endpoint);
+	print_peer_socket_info(peer);
+out:
+	printk(KERN_INFO "Exiting function wg_socket_set_peer_endpoint_from_skb\n");
 }
 
 void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer)
 {
+	printk(KERN_INFO "Entering function wg_socket_clear_peer_endpoint_src\n");
 	write_lock_bh(&peer->endpoint_lock);
 	memset(&peer->endpoint.src6, 0, sizeof(peer->endpoint.src6));
 	dst_cache_reset_now(&peer->endpoint_cache);
 	write_unlock_bh(&peer->endpoint_lock);
+	printk(KERN_INFO "Exiting function wg_socket_clear_peer_endpoint_src\n");
 }
 
 static int wg_receive(struct sock *sk, struct sk_buff *skb)
 {
+	printk(KERN_INFO "Entering function wg_receive\n");
 	struct wg_device *wg;
-
+	struct wg_socket_data *socket_data;
+	
 	if (unlikely(!sk))
 		goto err;
-	wg = sk->sk_user_data;
-	if (unlikely(!wg))
+	socket_data = sk->sk_user_data;
+	if (unlikely(!socket_data))
 		goto err;
+	if (unlikely(!socket_data->device))
+ 		goto err;
+	wg = socket_data->device;
 	skb_mark_not_on_list(skb);
 	wg_packet_receive(wg, skb);
+	printk(KERN_INFO "Exiting function wg_receive\n");
 	return 0;
 
 err:
 	kfree_skb(skb);
+	printk(KERN_INFO "Exiting function wg_receive with error.\n");
 	return 0;
 }
 
 static void sock_free(struct sock *sock)
 {
+	printk(KERN_INFO "Entering function sock_free\n");
 	if (unlikely(!sock))
 		return;
 	sk_clear_memalloc(sock);
 	udp_tunnel_sock_release(sock->sk_socket);
+	printk(KERN_INFO "Exiting function sock_free\n");
 }
 
 static void set_sock_opts(struct socket *sock)
 {
+	printk(KERN_INFO "Entering function set_sock_opts\n");
 	sock->sk->sk_allocation = GFP_ATOMIC;
 	sock->sk->sk_sndbuf = INT_MAX;
 	sk_set_memalloc(sock->sk);
+	printk(KERN_INFO "Exiting function set_sock_opts\n");
 }
 
 int wg_socket_init(struct wg_device *wg, u16 port)
 {
+	printk(KERN_INFO "Entering function wg_socket_init\n");
 	struct net *net;
+	struct wg_socket_data *socket_data;
 	int ret;
 	struct udp_tunnel_sock_cfg cfg = {
-		.sk_user_data = wg,
+		.sk_user_data = NULL,  // will set later
 		.encap_type = 1,
 		.encap_rcv = wg_receive
 	};
@@ -379,7 +924,17 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 	rcu_read_unlock();
 	if (unlikely(!net))
 		return -ENONET;
+	
+	socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+	if (!socket_data) {
+		put_net(net);
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		return -ENOMEM;
+	}
 
+ 	socket_data->device = wg;
+ 	socket_data->peer = NULL; // Set this to a valid peer where appropriate
+	
 #if IS_ENABLED(CONFIG_IPV6)
 retry:
 #endif
@@ -390,6 +945,11 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 		goto out;
 	}
 	set_sock_opts(new4);
+
+	// Set the socket data in the cfg structure
+	cfg.sk_user_data = socket_data;
+
+	// Now setup the UDP tunnel socket with the updated cfg
 	setup_udp_tunnel_sock(net, new4, &cfg);
 
 #if IS_ENABLED(CONFIG_IPV6)
@@ -405,6 +965,8 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 			goto out;
 		}
 		set_sock_opts(new6);
+
+		// Setup the IPv6 UDP tunnel socket with the same socket data
 		setup_udp_tunnel_sock(net, new6, &cfg);
 	}
 #endif
@@ -413,12 +975,15 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 	ret = 0;
 out:
 	put_net(net);
+	printk(KERN_INFO "Exiting function wg_socket_init\n");
 	return ret;
 }
 
+
 void wg_socket_reinit(struct wg_device *wg, struct sock *new4,
 		      struct sock *new6)
 {
+	printk(KERN_INFO "Entering function wg_socket_reinit\n");
 	struct sock *old4, *old6;
 
 	mutex_lock(&wg->socket_update_lock);
@@ -434,4 +999,2767 @@ void wg_socket_reinit(struct wg_device *wg, struct sock *new4,
 	synchronize_net();
 	sock_free(old4);
 	sock_free(old6);
+	printk(KERN_INFO "Exiting function wg_socket_reinit\n");
+}
+
+static int wg_set_socket_timeouts(struct socket *sock, unsigned long snd_timeout,
+				  unsigned long rcv_timeout)
+{
+	printk(KERN_INFO "Entering function wg_set_socket_timeouts\n");
+	if (!sock || !sock->sk) {
+		pr_err("Invalid socket or sock is NULL\n");
+		return -EINVAL;
+	}
+
+	struct sock *sk = sock->sk;
+
+	sk->sk_sndtimeo = snd_timeout*30;
+	sk->sk_rcvtimeo = rcv_timeout*30;
+
+	printk(KERN_INFO "Exiting function wg_set_socket_timeouts\n");
+	return 0;
+}
+
+static bool wg_endpoints_match(const struct endpoint *a,
+                               const struct endpoint *b)
+{
+	printk(KERN_INFO "Entering function wg_endpoints_match\n");
+	// Compare endpoints
+	if (a->addr.sa_family != b->addr.sa_family) {
+		printk(KERN_INFO "Exiting function wg_endpoints_match\n");
+		return false;
+	}
+
+	if (a->addr.sa_family == AF_INET) {
+		return a->addr4.sin_port == b->addr4.sin_port &&
+		a->addr4.sin_addr.s_addr == b->addr4.sin_addr.s_addr;
+	} else if (a->addr.sa_family == AF_INET6) {
+		// For IPv6, also compare the scope ID if the address is link-local
+		bool is_link_local_a = ipv6_addr_type(&a->addr6.sin6_addr) & IPV6_ADDR_LINKLOCAL;
+		bool is_link_local_b = ipv6_addr_type(&b->addr6.sin6_addr) & IPV6_ADDR_LINKLOCAL;
+        
+		return a->addr6.sin6_port == b->addr6.sin6_port &&
+		ipv6_addr_equal(&a->addr6.sin6_addr, &b->addr6.sin6_addr) &&
+		(!is_link_local_a || !is_link_local_b || a->addr6.sin6_scope_id == b->addr6.sin6_scope_id);
+	}
+	printk(KERN_INFO "Exiting function wg_endpoints_match\n");
+	return false;
+}
+
+void wg_free_peer_socket_data(struct wg_peer *peer);
+
+void wg_free_peer_socket_data(struct wg_peer *peer)
+{
+	if (peer && !IS_ERR(peer))
+		if (peer->peer_socket)
+			if (peer->peer_socket->sk)
+				if (peer->peer_socket->sk->sk_user_data)
+					kfree(peer->peer_socket->sk->sk_user_data);
+}
+
+
+
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound)
+{
+	printk(KERN_INFO "Entering function wg_clean_peer_socket peer=%p, inbound=%d\n", peer, inbound);
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_INFO "wg_clean_peer_socket: No peer or invalid peer.\n");
+		goto out;
+	}
+	print_peer_socket_info(peer);
+	if ((inbound && peer->peer_socket == peer->inbound_socket) ||
+	    (!inbound && peer->peer_socket == peer->outbound_socket)) {
+		// Cleanup partial skb buffer
+		if (peer->partial_skb) {
+			kfree_skb(peer->partial_skb);
+			peer->partial_skb = NULL;
+		}
+	
+		// Cancel and flush the TCP read workqueue
+		if (peer->tcp_read_worker_scheduled) {
+			cancel_work_sync(&peer->tcp_read_work);
+			peer->tcp_read_worker_scheduled = false;
+		}
+		if (peer->tcp_read_wq && destroy) {
+			destroy_workqueue(peer->tcp_read_wq);
+			peer->tcp_read_wq = NULL;
+		}
+	
+		// Cancel and flush the TCP write workqueue
+		if (peer->tcp_write_worker_scheduled) {
+			cancel_work_sync(&peer->tcp_write_work);
+			peer->tcp_write_worker_scheduled = false;
+		}
+		if (peer->tcp_write_wq && destroy) {
+			destroy_workqueue(peer->tcp_write_wq);
+			peer->tcp_write_wq = NULL;
+		}
+	
+		// Clean up packet queues
+		if (!skb_queue_empty(&peer->tcp_packet_queue))
+			skb_queue_purge(&peer->tcp_packet_queue);
+		if (!skb_queue_empty(&peer->send_queue))
+			skb_queue_purge(&peer->send_queue);
+	
+		// Reset TCP state
+		peer->received_len = 0;
+		peer->expected_len = 0;
+		peer->tcp_established = false;
+		peer->tcp_pending = false;
+		peer->tcp_retry_scheduled = false;
+	}
+	
+	// Determine which socket and related resources to clean based on the 'inbound' flag
+	struct socket **socket_to_clean = inbound ? &peer->inbound_socket : &peer->outbound_socket;
+	bool *callbacks_set_flag = inbound ? &peer->tcp_inbound_callbacks_set : &peer->tcp_outbound_callbacks_set;
+	bool *connected_flag = inbound ? &peer->inbound_connected : &peer->outbound_connected;
+	ktime_t *timestamp = inbound ? &peer->inbound_timestamp : &peer->outbound_timestamp;
+	struct delayed_work *remove_work = inbound ? &peer->tcp_inbound_remove_work : &peer->tcp_outbound_remove_work;
+	bool *remove_scheduled_flag = inbound ? &peer->tcp_inbound_remove_scheduled : &peer->tcp_outbound_remove_scheduled;
+
+	// Cleanup socket if necessary
+	if (*socket_to_clean) {
+		if (release) {
+			// Directly free peer socket data as per wg_free_peer_socket_data logic
+			if (*socket_to_clean && (*socket_to_clean)->sk) {
+				if ((*socket_to_clean)->sk->sk_user_data) {
+					kfree((*socket_to_clean)->sk->sk_user_data);
+					(*socket_to_clean)->sk->sk_user_data = NULL;
+				}
+			}
+			kernel_sock_shutdown(*socket_to_clean, SHUT_RDWR);
+			sock_release(*socket_to_clean);
+		}
+		*socket_to_clean = NULL;
+	}
+
+	// Reset callbacks set flag
+	*callbacks_set_flag = false;
+
+	// Reset connection status and timestamp
+	*connected_flag = false;
+	*timestamp = 0;
+
+	// Cancel and clean up remove work if scheduled
+	if (*remove_scheduled_flag) {
+		*remove_scheduled_flag = false;
+		cancel_delayed_work_sync(remove_work);
+	}
+
+	
+	// Check if a retry is scheduled and clean up
+	if (peer->tcp_retry_scheduled && !inbound) {
+		peer->tcp_retry_scheduled = false;
+		cancel_delayed_work_sync(&peer->tcp_retry_work);
+	}
+
+
+out:
+	print_peer_socket_info(peer);
+	printk(KERN_INFO "Exiting wg_clean_peer_socket\n");
+}
+
+
+struct wg_peer *wg_temp_peer_create(struct wg_device *wg);
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *receive_socket);
+
+
+// Function to copy source and destination addresses from a TCP socket
+
+
+int copy_sock_addresses(struct socket *tcp_socket, struct sockaddr_storage *inbound_source, struct sockaddr_storage *inbound_dest) {
+    struct sock *sk;
+    struct inet_sock *inet;
+    int family;
+
+    // Check if the socket is valid
+    if (!tcp_socket || !tcp_socket->sk) {
+        printk(KERN_INFO "Invalid TCP socket or socket's sk structure.\n");
+        return -1; // Invalid socket
+    }
+
+    sk = tcp_socket->sk; // Retrieve the socket's 'sock' structure
+    family = sk->sk_family;
+
+    if (family == AF_INET) {
+        struct inet_sock *inet = inet_sk(sk);
+
+        // Validate inet_sk
+        if (!inet) {
+            printk(KERN_ERR "inet_sk is NULL for IPv4 socket\n");
+            return -1;
+        }
+
+        // Clear the sockaddr_storage structures
+        memset(inbound_source, 0, sizeof(struct sockaddr_storage));
+        memset(inbound_dest, 0, sizeof(struct sockaddr_storage));
+
+        // Cast to sockaddr_in for IPv4
+        struct sockaddr_in *source_in = (struct sockaddr_in *)inbound_source;
+        struct sockaddr_in *dest_in = (struct sockaddr_in *)inbound_dest;
+
+        // Set the address family to AF_INET
+        source_in->sin_family = AF_INET;
+        dest_in->sin_family = AF_INET;
+
+        // Populate the source and destination information
+        source_in->sin_addr.s_addr = inet->inet_rcv_saddr; // Local IP address
+        source_in->sin_port = inet->inet_sport;            // Local port (already in network byte order)
+        dest_in->sin_addr.s_addr = inet->inet_daddr;       // Remote IP address
+        dest_in->sin_port = inet->inet_dport;              // Remote port (already in network byte order)
+
+        // Diagnostic printouts
+        printk(KERN_INFO "IPv4 Source IP: %pI4, Source Port: %u\n", &source_in->sin_addr, ntohs(source_in->sin_port));
+        printk(KERN_INFO "IPv4 Destination IP: %pI4, Destination Port: %u\n", &dest_in->sin_addr, ntohs(dest_in->sin_port));
+        printk(KERN_INFO "Hexdump of IP and UDP headers: %*ph\n", (int)sizeof(struct sockaddr_in), (void *)source_in);
+
+    } 
+#if IS_ENABLED(CONFIG_IPV6)
+    else if (family == AF_INET6) {
+        struct ipv6_pinfo *np = inet6_sk(sk);
+
+        // Validate ipv6_pinfo
+        if (!np) {
+            printk(KERN_ERR "ipv6_pinfo is NULL for IPv6 socket\n");
+            return -1;
+        }
+
+        // Clear the sockaddr_storage structures
+        memset(inbound_source, 0, sizeof(struct sockaddr_storage));
+        memset(inbound_dest, 0, sizeof(struct sockaddr_storage));
+
+        // Cast to sockaddr_in6 for IPv6
+        struct sockaddr_in6 *source_in6 = (struct sockaddr_in6 *)inbound_source;
+        struct sockaddr_in6 *dest_in6 = (struct sockaddr_in6 *)inbound_dest;
+
+        // Set the address family to AF_INET6
+        source_in6->sin6_family = AF_INET6;
+        dest_in6->sin6_family = AF_INET6;
+
+        // Populate the source and destination information
+        source_in6->sin6_addr = sk->sk_v6_rcv_saddr;       // Local IPv6 address
+        source_in6->sin6_port = inet->inet_sport;          // Local port (already in network byte order)
+        dest_in6->sin6_addr = sk->sk_v6_daddr;             // Remote IPv6 address
+        dest_in6->sin6_port = inet->inet_dport;            // Remote port (already in network byte order)
+        source_in6->sin6_scope_id = ipv6_iface_scope_id(&sk->sk_v6_rcv_saddr, sk->sk_bound_dev_if);
+
+        // Diagnostic printouts
+        printk(KERN_INFO "IPv6 Source IP: %pI6c, Source Port: %u\n", &source_in6->sin6_addr, ntohs(source_in6->sin6_port));
+        printk(KERN_INFO "IPv6 Destination IP: %pI6c, Destination Port: %u\n", &dest_in6->sin6_addr, ntohs(dest_in6->sin6_port));
+        printk(KERN_INFO "Hexdump of IP and UDP headers: %*ph\n", (int)sizeof(struct sockaddr_in6), (void *)source_in6);
+
+    }
+#endif
+    else {
+        printk(KERN_ERR "Unsupported address family: %d\n", family);
+        return -1; // Unsupported address family
+    }
+
+    return 0; // Success
+}
+
+struct wg_peer *wg_find_peer_by_endpoints(struct wg_device *wg, const struct endpoint *endpoint)
+{
+    struct wg_peer *peer = NULL;
+    struct wg_peer *matched_peer = NULL;
+
+    if (!wg || !endpoint) {
+        printk(KERN_ERR "wg_find_peer_by_endpoints: Invalid arguments, wg or endpoint is NULL\n");
+        return NULL;
+    }
+
+    printk(KERN_INFO "Entering function wg_find_peer_by_endpoints\n");
+
+    rcu_read_lock();
+    list_for_each_entry_rcu(peer, &wg->peer_list, peer_list) {
+        // Check if the current peer's endpoint, peer_endpoint, or tcp_reply_endpoint matches the provided endpoint
+        if (endpoint_eq(&peer->endpoint, endpoint) ||
+            endpoint_eq(&peer->peer_endpoint, endpoint) ||
+            endpoint_eq(&peer->tcp_reply_endpoint, endpoint)) {
+            matched_peer = peer;
+            printk(KERN_INFO "wg_find_peer_by_endpoints: Found matching peer %p\n", matched_peer);
+            break;
+        }
+    }
+    rcu_read_unlock();
+
+    if (!matched_peer) {
+        printk(KERN_INFO "wg_find_peer_by_endpoints: No matching peer found\n");
+    }
+
+    printk(KERN_INFO "Exiting function wg_find_peer_by_endpoints peer=%p\n", matched_peer);
+    return matched_peer;
+}
+
+
+int wg_tcp_listener_worker(struct wg_device *wg, struct socket *tcp_socket)
+{
+	bool found = false;
+	printk(KERN_INFO "Entering function wg_tcp_listener_worker\n");
+	struct socket *new_peer_connection = NULL;
+
+	if (!tcp_socket) {
+        	pr_err("tcp_socket is NULL\n");
+        	return -EINVAL;
+    	}
+	if (wg->listener_active) {
+		pr_err("Device TCP listener is already active.");
+		return -EINVAL;
+	}
+	wg->listener_active = true;
+	while (!kthread_should_stop()) {
+		int err;
+
+		err = kernel_accept(tcp_socket, &new_peer_connection, 0);
+		if (err < 0) {
+			if (err == -EAGAIN || err == -ERESTARTSYS)
+				continue;
+			pr_err("Error accepting new connection: %d\n", err);
+        		continue;
+		}
+
+		if (!new_peer_connection) {
+			pr_err("new_peer_connection is NULL after kernel_accept\n");
+			continue;
+		}
+		printk(KERN_INFO "wg_tcp_listener_worker accepted socket: %p new_peer_connection: %p\n", tcp_socket, &new_peer_connection);
+
+	        struct wg_peer *matched_peer = NULL;
+		struct wg_peer *new_temp_peer = NULL;
+	        struct endpoint new_endpoint;
+		struct wg_tcp_socket_list_entry *socket_iter = NULL;
+		struct wg_socket_data *socket_data = NULL;  // New structure for sk_user_data
+
+		
+		new_peer_connection->ops->getname(new_peer_connection, (struct sockaddr *)&new_endpoint, 1);
+
+		if (!list_empty(&wg->peer_list)) {
+			// search device peer list to see if inbound connection is from an established peer address.
+	      	 	rcu_read_lock();
+	       	 	list_for_each_entry_rcu(matched_peer, &wg->peer_list, peer_list) {
+				if (wg_endpoints_match(&matched_peer->endpoint, &new_endpoint)) {
+					// read data if there is any available
+					found = true;
+					printk(KERN_INFO "wg_tcp_listener_worker matched existing endpoint\n");
+					break;
+	        		}
+	       		 }
+			rcu_read_unlock();
+		}
+		if (found)
+			if (!skb_queue_empty(&new_peer_connection->sk->sk_receive_queue)) {
+				printk(KERN_INFO "wg_tcp_listener_worker found lingering data, calling wg_tcp_data_ready()\n");
+				wg_tcp_data_ready(new_peer_connection->sk);
+			}
+
+		
+		// if no matched_peer then this is a new connection and we need to process the packet to see if this is an esiting peer roaming.    
+		if (!matched_peer) {
+			// some kind of martian, toss it.
+			// Perform a graceful shutdown and release the socket
+			printk(KERN_INFO "wg_tcp_listener_worker did not find a matching peer\n");
+			kernel_sock_shutdown(new_peer_connection, SHUT_RDWR);
+			sock_release(new_peer_connection);
+	        } else {
+			printk(KERN_INFO "wg_tcp_listener_worker handling a new connection\n");
+			// We need to search if there is already a pendign connection and remove the old pending connection if there.
+			  // Check if the peer list is empty before proceeding
+			if (!list_empty(&wg->peer_list)) {
+				rcu_read_lock();
+				// check device pending connections in tcp_connection_list
+				list_for_each_entry_rcu(socket_iter, &wg->tcp_connection_list, tcp_connection_ll) {
+					// Defensive checks to ensure all relevant fields are populated
+					// Skip to the next entry if any critical field is NULL
+					if (!socket_iter) {
+						printk(KERN_INFO "socket_iter is NULL\n");
+						continue;
+					}	
+					if (!socket_iter->tcp_socket) {
+						printk(KERN_INFO "socket_iter->tcp_socket is NULL\n");
+						continue;
+					}
+					if (!socket_iter->tcp_socket->sk) {
+						printk(KERN_INFO "socket_iter->tcp_socket->sk is NULL\n");
+						continue;
+					}	
+
+					if (endpoint_eq(&new_endpoint, (struct endpoint *)&socket_iter->src_addr)) {
+						found = true;
+						break;
+					}
+				}
+				rcu_read_unlock();
+			}
+			if (found) {
+				printk(KERN_INFO "wg_tcp_listener_worker new connection was for an existing peer\n");
+				// Remove the older socket from the pending TCP connection list
+				spin_lock_bh(&wg->tcp_connection_list_lock);
+				list_del_rcu(&socket_iter->tcp_connection_ll);
+				spin_unlock_bh(&wg->tcp_connection_list_lock);
+				synchronize_rcu(); // Ensure safe removal
+				// nuke old socket
+				wg_clean_peer_socket(socket_iter->temp_peer, true, true, true);
+				// clean up old temp_peer
+				if (!IS_ERR(socket_iter->temp_peer) && socket_iter->temp_peer) {
+					kfree(socket_iter->temp_peer);
+				}
+				// Free the old entry
+				kfree(socket_iter);
+			}	
+				
+			// we have a new peer end point roaming potentially, 
+			// add to pending connection list and hand packets to upper layer for verificaiton
+	
+			new_temp_peer = wg_temp_peer_create(wg);
+			printk(KERN_INFO "wg_tcp_listener_worker created temp peer for inbound new connection temp_peer=%p\n", new_temp_peer);
+			if (!IS_ERR(new_temp_peer) && new_temp_peer) {
+				// Allocate memory for the new socket data structure
+				socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+				if (!socket_data) {
+					pr_err("Failed to allocate memory for socket_data\n");
+					kernel_sock_shutdown(new_peer_connection, SHUT_RDWR);
+					sock_release(new_peer_connection);
+					continue;
+				}
+
+				// Initialize the socket data with the device and the temp peer
+				socket_data->device = wg;
+				socket_data->peer = new_temp_peer;
+				socket_data->inbound = true;
+
+				// Set the socket data as sk_user_data
+				new_peer_connection->sk->sk_user_data = socket_data;
+				
+				new_temp_peer->peer_socket = new_peer_connection;
+				new_temp_peer->inbound_socket = new_peer_connection;
+				wg_get_endpoint_from_socket(new_peer_connection, &new_temp_peer->tcp_reply_endpoint);
+				new_temp_peer->endpoint = new_temp_peer->tcp_reply_endpoint;
+				
+				new_temp_peer->tcp_established = true;
+				new_temp_peer->inbound_connected = true;
+				new_temp_peer->inbound_timestamp = ktime_get();
+				new_temp_peer->clean_inbound = false;
+				new_temp_peer->tcp_inbound_callbacks_set = false;
+				copy_sock_addresses(new_peer_connection, &new_temp_peer->inbound_source, &new_temp_peer->inbound_dest);
+				printk(KERN_INFO "new_temp_peer Peer endpoint:");
+				log_wireguard_endpoint(&new_temp_peer->endpoint);
+				new_temp_peer->peer_endpoint = new_temp_peer->endpoint;
+				new_temp_peer->peer_endpoint_set = true;
+				// Set the port to incoming port
+				if (new_temp_peer->peer_endpoint.addr.sa_family == AF_INET) {
+        			// IPv4 address
+        				new_temp_peer->peer_endpoint.addr4.sin_port = htons(peer->device->incoming_port);
+    				} else if (new_temp_peer->peer_endpoint.addr.sa_family == AF_INET6) {
+        			// IPv6 address
+        				new_temp_peer->peer_endpoint.addr6.sin6_port = htons(peer->device->incoming_port);
+    				} else {
+        				// Unsupported address family, handle error if necessary
+        				printk(KERN_WARNING "Unsupported address family in WireGuard peer endpoint.\n");
+    				}
+				printk(KERN_INFO "new_temp_peer Peer endpoint:");
+				log_wireguard_endpoint(&new_temp_peer->endpoint);
+				
+				wg_add_tcp_socket_to_list(wg, new_peer_connection);
+				//  we need to set up a data reader for pending connections
+				wg_setup_tcp_socket_callbacks(new_temp_peer, true);  // ready to read data from pending connection and hand handshake to upper layers
+				// read data if there is some pending
+				if (!skb_queue_empty(&new_peer_connection->sk->sk_receive_queue)) {
+					printk(KERN_INFO "wg_tcp_listener_worker calling wg_tcp_data_ready() for temp peer\n");
+					wg_tcp_data_ready(new_peer_connection->sk);
+				print_peer_socket_info(new_temp_peer);
+				}
+			}
+		}
+	}	
+	printk(KERN_INFO "Exiting function wg_tcp_listener_worker\n");
+	return 0;
+}
+	
+int wg_tcp_listener4_thread(void *data)
+{
+	printk(KERN_INFO "Entering function wg_tcp_listener4_thread\n");
+	struct wg_device *wg = data;
+	struct socket *listen_socket;
+
+	// Check if tcp_socket4_ready is set
+	if (!wg->tcp_socket4_ready) {
+		printk(KERN_INFO "tcp_socket4 is not ready, exiting wg_tcp_listener4_thread\n");
+		return 0;
+	}
+	listen_socket = wg->tcp_listen_socket4;
+
+	printk(KERN_INFO "Exiting function wg_tcp_listener4_thread\n");
+	return wg_tcp_listener_worker(wg, listen_socket);
+}
+
+int wg_tcp_listener6_thread(void *data)
+{
+	printk(KERN_INFO "Entering function wg_tcp_listener6_thread\n");
+	struct wg_device *wg = data;
+	struct socket *listen_socket;
+
+	if (!wg->tcp_socket6_ready) {
+		printk(KERN_INFO "tcp_socket6 is not ready, exiting wg_tcp_listener6_thread\n");
+		return 0;
+	}
+
+	listen_socket = wg->tcp_listen_socket6;
+
+	printk(KERN_INFO "Exiting function wg_tcp_listener6_thread\n");
+	return wg_tcp_listener_worker(wg, listen_socket);
+}
+
+void wg_tcp_listener_socket_release(struct wg_device *wg)
+{
+	printk(KERN_INFO "Entering function wg_tcp_socket_release\n");
+
+	wg->listener_active = false;
+	// Signal listener threads to stop
+	if (wg->tcp_listener4_thread) {
+		printk(KERN_INFO "Stopping IPv4 listener thread\n");
+        	kthread_stop(wg->tcp_listener4_thread);
+        	wg->tcp_listener4_thread = NULL;
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+    	if (wg->tcp_listener6_thread) {
+        	printk(KERN_INFO "Stopping IPv6 listener thread\n");
+        	kthread_stop(wg->tcp_listener6_thread);
+        	wg->tcp_listener6_thread = NULL;
+    	}
+#endif
+
+	// Release IPv4 socket
+    	if (wg->tcp_listen_socket4) {
+        	printk(KERN_INFO "Releasing IPv4 socket\n");
+        	sock_release(wg->tcp_listen_socket4);
+        	wg->tcp_listen_socket4 = NULL;
+        	wg->tcp_socket4_ready = false;
+    	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+    	// Release IPv6 socket
+    	if (wg->tcp_listen_socket6) {
+		printk(KERN_INFO "Releasing IPv6 socket\n");
+        	sock_release(wg->tcp_listen_socket6);
+        	wg->tcp_listen_socket6 = NULL;
+        	wg->tcp_socket6_ready = false;
+    	}
+#endif
+
+	printk(KERN_INFO "Exiting function wg_tcp_socket_release\n");
+}
+
+struct socket *wg_setup_tcp_listen4(struct wg_device *wg, struct net *net, u16 port)
+{
+	if (!wg || !net || port == 0) {
+		printk(KERN_ERR "wg_setup_tcp_listen4: Invalid arguments\n");
+		return NULL;
+	}
+	printk(KERN_INFO "Entering function wg_setup_tcp_listen4\n");
+
+	int ret = -EINVAL; // Initialize ret with an invalid argument error
+	struct socket *listen_socket4 = NULL;
+	struct sockaddr_in addr4 = {
+		.sin_family = AF_INET,
+		.sin_port = htons(port),
+		.sin_addr = { htonl(INADDR_ANY) }
+	};
+
+	printk(KERN_INFO "Creating IPv4 socket\n");
+	ret = sock_create_kern(net, AF_INET, SOCK_STREAM, IPPROTO_TCP, &listen_socket4);
+	if (ret < 0) {
+		pr_err("%s: Could not create IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+	printk(KERN_INFO "IPv4 socket created successfully\n");
+
+	// Set socket options to reuse port
+	sock_set_reuseport(listen_socket4->sk);
+
+	printk(KERN_INFO "Binding IPv4 socket\n");
+	ret = kernel_bind(listen_socket4, (struct sockaddr *)&addr4, sizeof(addr4));
+	if (ret < 0) {
+		pr_err("%s: Could not bind IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+	printk(KERN_INFO "IPv4 socket bound successfully\n");
+
+	printk(KERN_INFO "Starting to listen on IPv4 socket\n");
+	ret = kernel_listen(listen_socket4, SOMAXCONN);
+	if (ret < 0) {
+		pr_err("%s: Could not listen on IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+	printk(KERN_INFO "IPv4 socket is now listening\n");
+	goto out;
+
+
+release_ipv4:
+	if (ret < 0 && listen_socket4) {
+		sock_release(listen_socket4);
+		printk(KERN_INFO "Exiting function wg_setup_tcp_listen4 with ret=%d\n", ret);
+		return NULL;
+	}
+
+out:
+	put_net(net);
+	printk(KERN_INFO "Exiting function wg_setup_tcp_listen4 with ret=%d\n", ret);
+	return listen_socket4;
+}
+
+struct socket *wg_setup_tcp_listen6(struct wg_device *wg, struct net *net, u16 port)
+{
+	if (!wg || !net || port == 0) {
+		printk(KERN_ERR "wg_setup_tcp_listen6: Invalid arguments\n");
+		return NULL;
+	}
+	printk(KERN_INFO "Entering function wg_setup_tcp_listen6\n");
+
+#if IS_ENABLED(CONFIG_IPV6)
+	int ret = -EINVAL; // Initialize ret with an invalid argument error
+	struct socket *listen_socket6 = NULL;
+	struct sockaddr_in6 addr6 = {
+		.sin6_family = AF_INET6,
+		.sin6_port = htons(port),
+		.sin6_addr = IN6ADDR_ANY_INIT,
+	};
+	printk(KERN_INFO "Creating IPv6 socket\n");
+	ret = sock_create_kern(net, AF_INET6, SOCK_STREAM, IPPROTO_TCP, &listen_socket6);
+	if (ret < 0) {
+		pr_err("%s: Could not create IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+	printk(KERN_INFO "IPv6 socket created successfully\n");
+
+	printk(KERN_INFO "Binding IPv6 socket\n");
+	ret = kernel_bind(listen_socket6, (struct sockaddr *)&addr6, sizeof(addr6));
+	if (ret < 0) {
+		pr_err("%s: Could not bind IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+	printk(KERN_INFO "IPv6 socket bound successfully\n");
+
+	printk(KERN_INFO "Starting to listen on IPv6 socket\n");
+	ret = kernel_listen(listen_socket6, SOMAXCONN);
+	if (ret < 0) {
+		pr_err("%s: Could not listen on IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+	printk(KERN_INFO "IPv6 socket is now listening\n");
+	goto out;
+
+release_ipv6:
+	if (ret < 0 && listen_socket6) {
+		sock_release(listen_socket6);
+		printk(KERN_INFO "Exiting function wg_setup_tcp_listen6 with ret=%d\n", ret);
+		return NULL;
+	}
+
+out:
+	put_net(net);
+	printk(KERN_INFO "Exiting function wg_setup_tcp_listen6 with ret=%d\n", ret);
+	return listen_socket6;
+#endif
+}
+
+int wg_tcp_listener_socket_init(struct wg_device *wg, u16 port)
+{
+	if (!wg || port == 0) {
+		printk(KERN_ERR "wg_tcp_listener_socket_init: Invalid arguments\n");
+		return -EINVAL;
+	}
+	printk(KERN_INFO "Entering function wg_tcp_listener_socket_init\n");
+
+	if (wg->tcp_socket4_ready || wg->tcp_socket6_ready) {
+		printk(KERN_INFO "TCP sockets are already initialized, exiting\n");
+		return 0;
+	}
+
+	if (!wg->dev) {
+		printk(KERN_INFO "Net Device not initialized in wg_device, exiting\n");
+		return -EINVAL;
+	}
+	
+	struct in_device *dev_v4 = __in_dev_get_rtnl(wg->dev);
+	struct inet6_dev *dev_v6 = __in6_dev_get(wg->dev);
+	struct net *net;
+	bool ipv4_configured = false, ipv6_configured = false;
+
+	printk(KERN_INFO "Locking RCU and dereferencing wg->creating_net\n");
+	rcu_read_lock();
+	net = rcu_dereference(wg->creating_net);
+	net = net ? maybe_get_net(net) : NULL;
+	rcu_read_unlock();
+	printk(KERN_INFO "RCU lock released\n");
+
+	if (unlikely(!net)) {
+		printk(KERN_ERR "Error: net is NULL, exiting wg_tcp_listener_socket_init\n");
+		return -ENONET;
+	}
+
+
+
+
+	// Use the default interface info to set up the IPv4 listener
+	if (default_iface_info.ipv4_available) {
+		wg->tcp_listen_socket4 = wg_setup_tcp_listen4(wg, net, port);
+		if (wg->tcp_listen_socket4) {
+			wg->tcp_socket4_ready = true;
+			ipv4_configured = true;
+
+			// Set the device endpoint using the global structure
+			wg->device_endpoint.addr4.sin_family = AF_INET;
+			wg->device_endpoint.addr4.sin_addr.s_addr = default_iface_info.ipv4_address;
+			wg->device_endpoint.addr4.sin_port = htons(port);
+			wg->device_endpoint.src4.s_addr = default_iface_info.ipv4_address;
+			wg->device_endpoint.src_if4 = default_iface_info.dev->ifindex; // Interface index
+			printk(KERN_INFO "Set default IPv4 device endpoint: %pI4\n", &wg->device_endpoint.addr4.sin_addr);
+		}
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	// Use the default interface info to set up the IPv6 listener
+	if (default_iface_info.ipv6_available) {
+		wg->tcp_listen_socket6 = wg_setup_tcp_listen6(wg, net, port);
+		if (wg->tcp_listen_socket6) {
+			wg->tcp_socket6_ready = true;
+			ipv6_configured = true;
+
+			// Set the device endpoint using the global structure
+			wg->device_endpoint.addr6.sin6_family = AF_INET6;
+			wg->device_endpoint.addr6.sin6_addr = default_iface_info.ipv6_address;
+			wg->device_endpoint.addr6.sin6_port = htons(port);
+			wg->device_endpoint.src6 = default_iface_info.ipv6_address;
+			printk(KERN_INFO "Set default IPv6 device endpoint: %pI6\n", &wg->device_endpoint.addr6.sin6_addr);
+		}
+	}
+#endif
+
+
+	printk(KERN_INFO "Listener Endpoint:\n");
+	log_wireguard_endpoint(&wg->device_endpoint);
+	
+	// Start the IPv4 listener thread if IPv4 is configured
+	if (dev_v4 && ipv4_configured && !wg->tcp_listener4_thread) {
+		printk(KERN_INFO "Starting IPv4 listener thread\n");
+		wg->tcp_listener4_thread = kthread_run(wg_tcp_listener4_thread, (void *)wg, "wg_listener");
+		if (IS_ERR(wg->tcp_listener4_thread)) {
+		pr_err("Failed to establish IPv4 TCP listener thread\n");
+		} else {
+			printk(KERN_INFO "IPv4 listener thread started successfully\n");
+		}
+	}
+
+    // Start the IPv6 listener thread if IPv6 is configured
+#if IS_ENABLED(CONFIG_IPV6)
+	if (dev_v6 && ipv6_configured && !wg->tcp_listener6_thread) {
+		printk(KERN_INFO "Starting IPv6 listener thread\n");
+		wg->tcp_listener6_thread = kthread_run(wg_tcp_listener6_thread, (void *)wg, "wg_listener");
+		if (IS_ERR(wg->tcp_listener6_thread)) {
+			pr_err("Failed to establish IPv6 TCP listener thread\n");
+		} else {
+			printk(KERN_INFO "IPv6 listener thread started successfully\n");
+		}
+	}
+#endif
+
+	// Schedule TCP cleanup work if not already scheduled
+#ifdef NOTTEST
+        if (!wg->tcp_cleanup_scheduled) {
+                pr_info("Scheduling TCP cleanup work.\n");
+		spin_lock_bh(&wg->tcp_cleanup_lock);
+                wg->tcp_cleanup_scheduled = true;
+	        spin_unlock_bh(&wg->tcp_cleanup_lock);
+		schedule_delayed_work(&wg->tcp_cleanup_work, msecs_to_jiffies(5000));
+	 	printk(KERN_INFO "Delayed work scheduled\n");
+	}
+#endif
+	printk(KERN_INFO "Exiting function wg_tcp_listener_socket_init\n");
+	return 0;
+}
+
+// Attempt to establish a TCP connection
+int wg_tcp_connect(struct wg_peer *peer)
+{
+    	struct wg_socket_data *socket_data;
+	struct sockaddr_storage src_addr_storage;
+	struct sockaddr *src_addr = (struct sockaddr *)&src_addr_storage; // Correctly define src_addr pointer
+
+	printk(KERN_INFO "Entering function wg_tcp_connect peer=%p\n", peer);
+	print_peer_socket_info(peer);
+	// Ensure wg_tcp_listener_socket_init is called
+	if (!peer->device->tcp_socket4_ready && !peer->device->tcp_socket6_ready) {
+		int ret = wg_tcp_listener_socket_init(peer->device, peer->device->incoming_port);
+        	if (ret < 0) {
+            		printk(KERN_ERR "Failed to initialize TCP sockets, exiting wg_tcp_connect\n");
+            		return ret;
+        	}
+    	}
+
+	// Check if the connection has already been established or is pending
+	if (peer->peer_socket || peer->tcp_pending || peer->inbound_connected) {
+        	printk(KERN_INFO "TCP connection already established or pending\n");
+        	return 0;
+    	}
+
+	// Print initial diagnostics
+	printk(KERN_INFO "(Device) Peer transport: %d, TCP established: %d\n", peer->device->transport, peer->tcp_established);
+	printk(KERN_INFO "Peer endpoint address family: %d\n", peer->endpoint.addr.sa_family);
+	printk(KERN_INFO "Endpoint ");
+	log_wireguard_endpoint(&peer->endpoint);
+	printk(KERN_INFO "Peer Endpoint");
+	log_wireguard_endpoint(&peer->peer_endpoint);
+
+	// Check if endpoint is properly set before attempting to connect
+	if (peer->peer_endpoint.addr.sa_family != AF_INET && peer->peer_endpoint.addr.sa_family != AF_INET6) {
+        	printk(KERN_ERR "Invalid address family for connection: %d\n", peer->peer_endpoint.addr.sa_family);
+       		return -EAFNOSUPPORT;
+	}
+
+	struct sockaddr_storage addr_storage;
+    	struct sockaddr *addr = (struct sockaddr *)&addr_storage;
+	unsigned long timeout = 30 * HZ; // 5 seconds in jiffies
+	int ret;
+
+	if (peer->device->transport != WG_TRANSPORT_TCP || peer->tcp_established || peer->outbound_connected) {
+		pr_err("Invalid state for TCP connection attempt.\n");
+		printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+		return -EINVAL;
+    	}
+
+	memset(&addr_storage, 0, sizeof(addr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+        	struct sockaddr_in *addr4 = (struct sockaddr_in *)&addr_storage;
+		addr4->sin_family = AF_INET;
+		addr4->sin_port = peer->peer_endpoint.addr4.sin_port; // Use correct port from endpoint
+		addr4->sin_addr.s_addr = peer->peer_endpoint.addr4.sin_addr.s_addr;
+		addr = (struct sockaddr *)addr4;
+		printk(KERN_INFO "Setting up IPv4 connection to %pI4:%d\n", &addr4->sin_addr, ntohs(addr4->sin_port));
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *addr6 = (struct sockaddr_in6 *)&addr_storage;
+		addr6->sin6_family = AF_INET6;
+		addr6->sin6_port = peer->peer_endpoint.addr6.sin6_port; // Use correct port from endpoint
+		memcpy(&addr6->sin6_addr, &peer->peer_endpoint.addr6.sin6_addr, sizeof(peer->peer_endpoint.addr6.sin6_addr));
+		addr = (struct sockaddr *)addr6;
+		printk(KERN_INFO "Setting up IPv6 connection to [%pI6c]:%d\n", &addr6->sin6_addr, ntohs(addr6->sin6_port));
+    	}
+#endif
+	else {
+        	pr_err("Unsupported address family: %d\n", peer->endpoint.addr.sa_family);
+		printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+		return -EAFNOSUPPORT;
+    	}
+
+	// Create the socket
+	printk(KERN_INFO "Creating socket for address family: %d\n", peer->endpoint.addr.sa_family);
+	ret = sock_create_kern(&init_net, peer->peer_endpoint.addr.sa_family, SOCK_STREAM, IPPROTO_TCP, &peer->peer_socket);
+	if (ret) {
+        	pr_err("Failed to create TCP socket for address family %d: %d\n", peer->peer_endpoint.addr.sa_family, ret);
+		printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+		return ret;
+	}
+
+	
+	// ** New code to bind the socket to the default interface's IP address **
+	memset(&src_addr_storage, 0, sizeof(src_addr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+		struct sockaddr_in *src_addr4 = (struct sockaddr_in *)&src_addr_storage;
+		src_addr4->sin_family = AF_INET;
+		src_addr4->sin_port = 0; // Let the system choose the port
+		src_addr4->sin_addr.s_addr = default_iface_info.ipv4_address; // Use the default interface's IP address
+		src_addr = (struct sockaddr *)src_addr4;
+		printk(KERN_INFO "Binding socket to source address %pI4\n", &src_addr4->sin_addr);
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *src_addr6 = (struct sockaddr_in6 *)&src_addr_storage;
+		src_addr6->sin6_family = AF_INET6;
+		src_addr6->sin6_port = 0; // Let the system choose the port
+		src_addr6->sin6_addr = default_iface_info.ipv6_address; // Use the default interface's IPv6 address
+		src_addr = (struct sockaddr *)src_addr6;
+		printk(KERN_INFO "Binding socket to source address [%pI6c]\n", &src_addr6->sin6_addr);
+	}
+ #endif
+
+
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->outbound_connected = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->outbound_timestamp = ktime_set(0, 0);
+	peer->outbound_socket = peer->peer_socket;
+
+	struct inet_sock *inet = inet_sk(peer->peer_socket->sk);
+
+	// Set up outbound source and destination using sockaddr_storage
+	memset(&peer->outbound_source, 0, sizeof(struct sockaddr_storage));
+	memset(&peer->outbound_dest, 0, sizeof(struct sockaddr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+        	struct sockaddr_in *source = (struct sockaddr_in *)&peer->outbound_source;
+		struct sockaddr_in *dest = (struct sockaddr_in *)&peer->outbound_dest;
+
+		source->sin_family = AF_INET;
+		source->sin_port = inet->inet_sport;  // Source port from socket
+		source->sin_addr.s_addr = inet->inet_saddr; // Source IP from socket
+
+		dest->sin_family = AF_INET;
+		dest->sin_port = peer->peer_endpoint.addr4.sin_port; // Destination port from endpoint
+		dest->sin_addr = peer->peer_endpoint.addr4.sin_addr; // Destination IP from endpoint
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *source6 = (struct sockaddr_in6 *)&peer->outbound_source;
+		struct sockaddr_in6 *dest6 = (struct sockaddr_in6 *)&peer->outbound_dest;
+
+		source6->sin6_family = AF_INET6;
+		source6->sin6_port = inet->inet_sport;  // Source port from socket
+		memcpy(&source6->sin6_addr, &inet6_sk(peer->peer_socket->sk)->saddr, sizeof(struct in6_addr)); // Source IP from socket
+
+		dest6->sin6_family = AF_INET6;
+		dest6->sin6_port = peer->peer_endpoint.addr6.sin6_port; // Destination port from endpoint
+		memcpy(&dest6->sin6_addr, &peer->endpoint.addr6.sin6_addr, sizeof(struct in6_addr)); // Destination IP from endpoint
+    	}
+#endif
+
+	printk(KERN_INFO "Allocating socket data\n");
+	socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+	if (!socket_data) {
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		sock_release(peer->peer_socket);
+		peer->peer_socket = NULL;
+		printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+		return -ENOMEM;
+    	}
+	socket_data->device = peer->device;
+	socket_data->peer = peer;
+	socket_data->inbound = false;
+	peer->peer_socket->sk->sk_user_data = socket_data;
+
+	// Print diagnostic information about the created socket
+	printk(KERN_INFO "Socket created, sk=%p, family=%d, state=%d\n", 
+        peer->peer_socket->sk, peer->peer_socket->sk->sk_family, peer->peer_socket->sk->sk_state);
+
+	// Set up the socket callbacks before initiating the connect
+	printk(KERN_INFO "Setting up socket callbacks\n");
+	wg_setup_tcp_socket_callbacks(peer, false); // set outbound callbacks
+
+	// Set socket timeouts for send and receive operations
+	printk(KERN_INFO "Setting socket timeouts\n");
+    	ret = wg_set_socket_timeouts(peer->peer_socket, timeout, timeout);
+    	if (ret) {
+        	pr_err("Failed to set socket timeouts: %d\n", ret);
+        	sock_release(peer->peer_socket);
+        	peer->peer_socket = NULL;
+        	printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+        	return ret;
+    	}
+
+    	// Print diagnostic information before initiating the connect
+    	printk(KERN_INFO "Ready to initiate connection, sk_state=%d\n", peer->peer_socket->sk->sk_state);
+
+	// Initiate the non-blocking connect
+    	printk(KERN_INFO "Initiating non-blocking connect\n");
+	ret = kernel_connect(peer->peer_socket, addr, addr->sa_family == AF_INET ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6), O_NONBLOCK);
+	if (ret != -EINPROGRESS && ret != 0) {
+		pr_err("TCP connection attempt failed: %d\n", ret);
+		sock_release(peer->peer_socket);
+		peer->peer_socket = NULL;
+        	printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+		return ret;
+    	}
+
+	pr_info("TCP connection attempt initiated\n");
+	spin_lock_bh(&peer->tcp_lock);
+	peer->tcp_pending = true;
+	spin_unlock_bh(&peer->tcp_lock);
+
+	if (!peer->tcp_retry_scheduled) {
+		printk(KERN_INFO "Scheduling TCP retry work.\n");
+		peer->tcp_retry_scheduled = true;
+		schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(10000));
+	}
+
+	printk(KERN_INFO "Exiting function wg_tcp_connect\n");
+	return 0;
+}
+
+
+// Function to release and clean up an old peer TCP connection - clean the active connection
+static void wg_release_peer_tcp_connection(struct wg_peer *peer)
+{
+	bool inbound = false;
+	printk(KERN_INFO "Entering function wg_release_old_peer_tcp_connection\n");
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		printk(KERN_INFO "Exiting function wg_release_old_peer_tcp_connection - no peer to tear down.\n");
+		goto out;
+	}
+	print_peer_socket_info(peer);
+	if (!peer->peer_socket || !(peer->tcp_established || peer->tcp_pending)){
+		printk(KERN_INFO "Exiting function wg_release_old_peer_tcp_connection - no connection to tear down.\n");
+		goto out;
+	}
+	if (peer->peer_socket == peer->inbound_socket)
+		inbound = true;
+	// Reset socket callbacks and release the socket
+	wg_reset_tcp_socket_callbacks(peer, inbound);
+
+	// Perform a graceful shutdown and release the socket
+	kernel_sock_shutdown(peer->peer_socket, SHUT_RDWR);
+	sock_release(peer->peer_socket);
+	
+	// Lock to safely modify the peer's TCP connection state
+	spin_lock_bh(&peer->tcp_lock);
+	peer->peer_socket = NULL;
+	if (inbound)
+		peer->inbound_socket = NULL;
+	else
+		peer->outbound_socket = NULL;
+	// Clear TCP connection flags
+	peer->tcp_pending = true;
+	peer->tcp_established = false;
+	spin_unlock_bh(&peer->tcp_lock);
+	// flush any partial data before we switch and free the held buffer
+	if (peer->partial_skb) {
+                kfree_skb(peer->partial_skb);
+		peer->partial_skb = NULL;
+	}
+		
+	// Check if a retry is scheduled and clean up
+    	if (peer->tcp_retry_scheduled) {
+        	peer->tcp_retry_scheduled = false;
+        	cancel_delayed_work_sync(&peer->tcp_retry_work);
+	}
+
+	// Clean up packet queues
+    	skb_queue_purge(&peer->tcp_packet_queue);
+    	skb_queue_purge(&peer->send_queue);
+
+
+out:
+	printk(KERN_INFO "Exiting function wg_release_old_peer_tcp_connection\n");
+}
+
+
+void wg_extract_endpoint_from_sock(struct sock *sk,
+                                   struct endpoint *endpoint)
+{
+	printk(KERN_INFO "Entering function wg_extract_endpoint_from_sock\n");
+	if (!sk || !endpoint) {
+		pr_warn("Socket or endpoint is NULL.\n");
+		return;
+	}
+	memset(endpoint, 0, sizeof(*endpoint)); // Clear the endpoint structure
+
+	if (sk->sk_family == AF_INET) {
+		// IPv4
+		struct inet_sock *inet = inet_sk(sk);
+
+		endpoint->addr4.sin_family = AF_INET;
+		endpoint->addr4.sin_port = inet->inet_dport; // Destination port
+		endpoint->addr4.sin_addr.s_addr = inet->inet_daddr; // Destination IP address
+	} else if (sk->sk_family == AF_INET6) {
+#if IS_ENABLED(CONFIG_IPV6)
+		// IPv6
+		endpoint->addr6.sin6_family = AF_INET6;
+		endpoint->addr6.sin6_port = sk->sk_dport; // Destination port
+		endpoint->addr6.sin6_addr = sk->sk_v6_daddr; // Destination IP address
+
+		if (ipv6_addr_type((struct in6_addr *)&sk->sk_v6_daddr) & IPV6_ADDR_LINKLOCAL) {
+			// The destination address is link-local; use the socket's bound device for the scope ID
+			endpoint->addr6.sin6_scope_id = sk->sk_bound_dev_if;
+		} else {
+			// Not a link-local address; no scope ID required
+			endpoint->addr6.sin6_scope_id = 0;
+		}
+	} else {
+#endif
+		pr_warn("Unsupported socket family: %d.\n", sk->sk_family);
+	}
+	printk(KERN_INFO "Exiting function wg_extract_endpoint_from_sock\n");
+}
+
+
+void wg_tcp_state_change(struct sock *sk)
+{
+	printk(KERN_INFO "Entering function wg_tcp_state_change\n");
+
+	// Check if the socket is valid
+	if (!sk || IS_ERR(sk)) {
+		pr_err("wg_tcp_state_change: Invalid socket passed to the function\n");
+		goto out;
+	}
+
+	// Retrieve the socket user data
+	struct wg_socket_data *socket_data = sk->sk_user_data;
+
+	// Check if socket_data is valid
+	if (!socket_data || IS_ERR(socket_data)) {
+		pr_err("wg_tcp_state_change: Invalid or NULL socket_data for socket %p\n", sk);
+		goto out;
+	}
+
+	// Retrieve the peer from the socket_data
+	struct wg_peer *peer = socket_data->peer;
+
+	// Check if peer is valid
+	if (!peer || IS_ERR(peer)) {
+		pr_err("wg_tcp_state_change: Invalid or NULL peer in socket_data for socket %p\n", sk);
+		goto out;
+	}
+	print_peer_socket_info(peer);
+	// Diagnostic information about the current state
+	pr_info("wg_tcp_state_change: Socket state=%d, Socket error=%d\n", sk->sk_state, sk->sk_err);
+	pr_info("wg_tcp_state_change: Peer=%p, Device=%p\n", peer, socket_data->device);
+
+	// Additional diagnostic information for peer-specific data
+	pr_info("wg_tcp_state_change: Peer TCP established=%d, TCP pending=%d\n",
+	        peer->tcp_established, peer->tcp_pending);
+
+
+	
+	// Log detailed state information
+	pr_info("wg_tcp_state_change: sk=%p, sk_state=%d, sk_err=%d, sk_shutdown=%d, sk_send_head=%p\n", 
+	 	sk, sk->sk_state, sk->sk_err, sk->sk_shutdown, sk->sk_send_head);
+	// Log TCP specific state information if available
+	const char *tcp_state_name;
+
+	switch (sk->sk_state) {
+    		case TCP_ESTABLISHED:
+        		tcp_state_name = "TCP_ESTABLISHED";
+			break;
+		case TCP_SYN_SENT:
+			tcp_state_name = "TCP_SYN_SENT";
+			break;
+		case TCP_SYN_RECV:
+			tcp_state_name = "TCP_SYN_RECV";
+			break;
+		case TCP_FIN_WAIT1:
+			tcp_state_name = "TCP_FIN_WAIT1";
+			break;
+		case TCP_FIN_WAIT2:
+			tcp_state_name = "TCP_FIN_WAIT2";
+			break;
+		case TCP_TIME_WAIT:
+			tcp_state_name = "TCP_TIME_WAIT";
+			break;
+		case TCP_CLOSE:
+			tcp_state_name = "TCP_CLOSE";
+			break;
+		case TCP_CLOSE_WAIT:
+			tcp_state_name = "TCP_CLOSE_WAIT";
+			break;
+		case TCP_LAST_ACK:
+			tcp_state_name = "TCP_LAST_ACK";
+			break;
+		case TCP_LISTEN:
+			tcp_state_name = "TCP_LISTEN";
+			break;
+		case TCP_CLOSING:
+			tcp_state_name = "TCP_CLOSING";
+			break;
+		case TCP_NEW_SYN_RECV:
+			tcp_state_name = "TCP_NEW_SYN_RECV";
+			break;
+		default:
+			tcp_state_name = "UNKNOWN_STATE";
+        		break;
+	}
+
+	printk(KERN_INFO "TCP state: %s (%d)\n", tcp_state_name, sk->sk_state);
+
+	if (sk->sk_state == TCP_ESTABLISHED) {
+		struct tcp_sock *tp = tcp_sk(sk);
+		printk(KERN_INFO "TCP_ESTABLISHED: snd_una=%u, snd_nxt=%u, snd_wnd=%u, rcv_wnd=%u, rcv_nxt=%u\n", 
+			tp->snd_una, tp->snd_nxt, tp->snd_wnd, tp->rcv_wnd, tp->rcv_nxt);
+	}
+
+	// first lets figure out if this is an inbound connect
+	
+	switch (sk->sk_state) {
+    		case TCP_ESTABLISHED:
+			if (peer->temp_peer) {
+				pr_err("Wireguard: Inbound peer connection previously established.\n");
+				break;
+			}
+			if (!peer->tcp_established && !peer->outbound_connected) {
+				peer->tcp_pending = false;
+        			peer->tcp_established = true;
+				peer->outbound_connected = true;
+				peer->outbound_timestamp = ktime_get();
+				peer->tcp_outbound_remove_scheduled = false;
+				// Check if a retry is scheduled and clean up
+    				if (peer->tcp_retry_scheduled) {
+       		 			peer->tcp_retry_scheduled = false;
+        				cancel_delayed_work_sync(&peer->tcp_retry_work);
+				}
+				peer->tcp_retry_scheduled = false;  // Clear the retry flag upon successful connection
+				pr_info("TCP connection established.\n");
+				break;
+			} else
+				pr_err("Wireguard: Outbound connection previously established.\n");
+			break;
+		case TCP_CLOSE:
+		case TCP_CLOSE_WAIT:
+		case TCP_CLOSING:
+		case TCP_FIN_WAIT1:
+		case TCP_FIN_WAIT2:
+		case TCP_LAST_ACK:
+			if (peer->tcp_established || peer->tcp_pending) {
+                		// Connection failed or closed unexpectedly
+                		pr_info("TCP connection failed or closed, handling state.\n");
+				// first we have to figure out if this is inbound or outbound connection;
+				struct endpoint *ep;
+				bool inbound = false;
+				if (sk->sk_user_data)
+					if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+						inbound = true;
+						if (peer->tcp_established && !peer->outbound_connected)
+							peer->tcp_established = false;
+						peer->inbound_timestamp = ktime_set(0, 0);
+						peer->inbound_connected = false;
+					} else {
+						if (peer->tcp_established && !peer->inbound_connected)
+							peer->tcp_established = false;
+						peer->outbound_timestamp = ktime_set(0, 0);
+						peer->outbound_connected = false;
+						peer->tcp_pending - false;
+					}
+				else
+					pr_err("Wireguard: TCP State Change, malformed socket state user data.\n");
+                		
+				// if this is a real peer and both connections down schedule a connection retry
+				if (!peer->tcp_established && !peer->tcp_retry_scheduled && !peer->temp_peer) {
+                    			printk(KERN_INFO "Scheduling TCP retry work.\n");
+                    			peer->tcp_retry_scheduled = true;
+                    			schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(10000));
+                		}
+				
+				// Schedule TCP socket removal work if not already scheduled	
+ 				if (inbound) {
+					if (!peer->tcp_inbound_remove_scheduled) {
+						printk(KERN_INFO "Setting up socket remove work.\n");
+						peer->tcp_inbound_remove_scheduled = true;
+						schedule_delayed_work(&peer->tcp_inbound_remove_work, 0);
+					}
+				else
+					if (!peer->tcp_outbound_remove_scheduled) {
+						printk(KERN_INFO "Setting up socket remove work.\n");
+						peer->tcp_outbound_remove_scheduled = true;
+						schedule_delayed_work(&peer->tcp_inbound_remove_work, 0);
+					}
+				}
+				if (!peer->inbound_connected && !peer->outbound_connected)
+					peer->tcp_established = false;
+			}
+		default:
+			break;
+    	}
+out:
+	// Call the original state change callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_state_change) {
+        		peer->original_inbound_state_change(sk);
+   		}
+	} else {
+		if (peer->original_outbound_state_change) {
+        		peer->original_outbound_state_change(sk);
+   		}
+	}
+	printk(KERN_INFO "Exiting function wg_tcp_state_change\n");
+}
+
+
+
+void log_wireguard_endpoint(struct endpoint *ep)
+{
+    char addr_str[INET6_ADDRSTRLEN];
+
+    if (!ep) {
+        printk(KERN_INFO "WireGuard: Endpoint is NULL.\n");
+        return;
+    }
+
+    switch (ep->addr.sa_family) {
+    case AF_INET: {
+        // Handle IPv4 address
+        struct sockaddr_in *sin = &ep->addr4;
+        snprintf(addr_str, sizeof(addr_str), "%pI4", &sin->sin_addr);
+        printk(KERN_INFO "Endpoint IPv4: %s:%u\n",
+               addr_str, ntohs(sin->sin_port));
+        if (ep->src_if4 != 0) {
+            snprintf(addr_str, sizeof(addr_str), "%pI4", &ep->src4);
+            printk(KERN_INFO "Source IPv4: %s, Source Interface: %d\n",
+                   addr_str, ep->src_if4);
+        }
+        break;
+    }
+    case AF_INET6: {
+        // Handle IPv6 address
+        struct sockaddr_in6 *sin6 = &ep->addr6;
+        snprintf(addr_str, sizeof(addr_str), "%pI6", &sin6->sin6_addr);
+        printk(KERN_INFO "Endpoint IPv6: [%s]:%u, Scope ID: %u\n",
+               addr_str, ntohs(sin6->sin6_port), sin6->sin6_scope_id);
+        snprintf(addr_str, sizeof(addr_str), "%pI6", &ep->src6);
+        printk(KERN_INFO "Source IPv6: [%s]\n", addr_str);
+        break;
+    }
+    default:
+        printk(KERN_INFO "Unsupported address family: %d\n", ep->addr.sa_family);
+        break;
+    }
+}
+
+
+
+void wg_get_endpoint_from_socket(struct socket *epsocket, struct endpoint *ep)
+{
+    // Validate input parameters
+    if (!epsocket || !ep) {
+        printk(KERN_ERR "Invalid input: epsocket or ep is NULL\n");
+        return;
+    }
+
+    // Validate the socket's `sock` structure
+    if (!epsocket->sk) {
+        printk(KERN_ERR "Invalid socket: epsocket->sk is NULL\n");
+        return;
+    }
+
+    struct sock *sk = epsocket->sk;
+    int family = sk->sk_family;
+
+    if (family == AF_INET) {
+        struct inet_sock *inet = inet_sk(sk);
+
+        // Validate inet_sk
+        if (!inet) {
+            printk(KERN_ERR "inet_sk is NULL for IPv4 socket\n");
+            return;
+        }
+
+        // Ensure that the inet_daddr and inet_dport are valid before accessing
+        if (inet->inet_daddr == 0 || inet->inet_dport == 0) {
+            printk(KERN_ERR "Invalid IPv4 address or port\n");
+            return;
+        }
+
+        // Populate the endpoint with IPv4 address and port
+        ep->addr4.sin_family = AF_INET;
+        ep->addr4.sin_addr.s_addr = inet->inet_daddr; // Remote IPv4 address
+        ep->addr4.sin_port = inet->inet_dport; // Remote port
+
+        // Populate src4 fields with local information
+        ep->src4.s_addr = inet->inet_saddr; // Local IPv4 address
+        ep->src_if4 = sk->sk_bound_dev_if; // Interface index
+
+        // Diagnostics
+        printk(KERN_INFO "IPv4 endpoint: remote %pI4:%u, local %pI4:%u\n",
+               &ep->addr4.sin_addr.s_addr, ntohs(ep->addr4.sin_port),
+               &ep->src4.s_addr, ntohs(inet->inet_sport));
+
+    }
+#if IS_ENABLED(CONFIG_IPV6)
+    else if (family == AF_INET6) {
+        struct ipv6_pinfo *np = inet6_sk(sk);
+
+        // Validate ipv6_pinfo
+        if (!np) {
+            printk(KERN_ERR "ipv6_pinfo is NULL for IPv6 socket\n");
+            return;
+        }
+
+        // Ensure that the IPv6 address and port are valid before accessing
+        if (ipv6_addr_any(&sk->sk_v6_daddr) || inet_sk(sk)->inet_dport == 0) {
+            printk(KERN_ERR "Invalid IPv6 address or port\n");
+            return;
+        }
+
+        // Populate the endpoint with IPv6 address and port
+        ep->addr6.sin6_family = AF_INET6;
+        ep->addr6.sin6_addr = sk->sk_v6_daddr; // Remote IPv6 address
+        ep->addr6.sin6_port = inet_sk(sk)->inet_dport; // Remote port
+        ep->addr6.sin6_scope_id = ipv6_iface_scope_id(&sk->sk_v6_rcv_saddr, sk->sk_bound_dev_if);
+
+        // Populate src6 fields with local information
+        ep->src6 = sk->sk_v6_rcv_saddr; // Local IPv6 address
+
+        // Diagnostics
+        printk(KERN_INFO "IPv6 endpoint: remote %pI6c:%u, local %pI6c:%u\n",
+               &ep->addr6.sin6_addr, ntohs(ep->addr6.sin6_port),
+               &ep->src6, ntohs(inet_sk(sk)->inet_sport));
+    }
+#endif
+    else {
+        printk(KERN_ERR "Unsupported address family: %d\n", family);
+        return;
+    }
+}
+
+int wg_tcp_queuepkt(struct wg_peer *peer, const void *data,
+                           size_t len)
+{
+	printk(KERN_INFO "Entering function wg_tcp_queuepkt peer=%p\n", peer);
+
+	struct endpoint current_endpoint;
+	struct wg_tcp_socket_list_entry *socket_iter;
+	bool found = false;
+	bool inbound = false;
+
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_INFO "Exiting function wg_tcp_queuepkt, no peer.\n");
+		return -EINVAL;
+	}	
+	print_peer_socket_info(peer);
+	if (!data || len == 0) {
+		printk(KERN_INFO "Exiting function wg_tcp_queuepkt, invalid parameters\n");
+		return -EINVAL;
+	}	
+
+	/* Print TCP-related flags */
+	printk(KERN_INFO "wg_peer: temp_peer = %d\n", peer->temp_peer);
+	printk(KERN_INFO "wg_peer: tcp_established = %d\n", peer->tcp_established);
+	printk(KERN_INFO "wg_peer: tcp_pending = %d\n", peer->tcp_pending);
+	printk(KERN_INFO "wg_peer: outbound_connected = %d\n", peer->outbound_connected);
+	printk(KERN_INFO "wg_peer: inbound_connected = %d\n", peer->inbound_connected);
+	printk(KERN_INFO "wg_peer: tcp_outbound_callbacks_set = %d\n", peer->tcp_outbound_callbacks_set);
+	printk(KERN_INFO "wg_peer: tcp_inbound_callbacks_set = %d\n", peer->tcp_inbound_callbacks_set);
+	log_wireguard_endpoint(&peer->endpoint);
+
+    	// Find the peer matching the endpoint, peer_endpoint, or tcp_reply_endpoint
+	peer = wg_find_peer_by_endpoints(peer->device, &peer->endpoint);
+    	if (!peer || IS_ERR(peer)) {
+        	printk(KERN_INFO "wg_queuepkt: No matching peer found for endpoint\n");
+        	return -ENOENT;
+    	}
+	
+	struct sk_buff *skb = alloc_skb(len + SKB_HEADER_LEN, GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_INFO "Exiting function wg_tcp_queuepkt\n");
+		return -ENOMEM;
+	}
+
+	skb_reserve(skb, SKB_HEADER_LEN);
+	skb_put_data(skb, data, len);
+
+	if (!peer->peer_socket) {
+		// peer connenction is down reconnect
+		if (wg_tcp_connect(peer) < 0) {
+			kfree_skb(skb);
+			printk(KERN_INFO "Exiting function wg_tcp_queuepkt due to connection failure\n");
+			return -ECONNREFUSED; // Connection attempt failed
+		}
+	}	
+
+	// Check if the current destination matches the peer's destination address, if not check pending connections
+	printk(KERN_INFO "Current endpoint:");
+	log_wireguard_endpoint(&current_endpoint);
+	printk(KERN_INFO "Peer endpoint:");
+	log_wireguard_endpoint(&peer->endpoint);
+	printk(KERN_INFO "Peer peer_endpoint:");
+	log_wireguard_endpoint(&peer->peer_endpoint);
+
+	if (!peer->tcp_established) {
+		// peer connenction is down reconnect
+		if (wg_tcp_connect(peer) < 0) {
+			kfree_skb(skb);
+			printk(KERN_INFO "Exiting function wg_tcp_queuepkt due to connection failure\n");
+			return -ECONNREFUSED; // Connection attempt failed
+		}
+	}
+	spin_lock_bh(&peer->send_queue_lock);
+	skb_queue_tail(&peer->send_queue, skb);
+	spin_unlock_bh(&peer->send_queue_lock);
+	// Trigger sending if possible
+	if (peer->peer_socket && peer->tcp_established) {
+		if (sk_stream_is_writeable(peer->peer_socket->sk)) {
+			wg_tcp_write_space(peer->peer_socket->sk);
+		} 
+	} 	
+	print_peer_socket_info(peer);
+	printk(KERN_INFO "Exiting function wg_tcp_queuepkt\n");
+	return 0;
+}
+
+// Simple checksum function for TCP encapsulation header
+static __be16 wg_header_checksum(const struct wg_tcp_encap_header *hdr)
+{
+	printk(KERN_INFO "Entering function wg_header_checksum\n");
+    	uint16_t checksum = 0;
+    	uint32_t length = ntohl(hdr->length); // Ensure network byte order is converted to host byte order for calculation
+
+    	// Break the length into two 16-bit halves and XOR them with the flags and type
+    	checksum ^= (length >> 16) & 0xFFFF;
+    	checksum ^= length & 0xFFFF;
+    	checksum ^= (hdr->flags << 8) | hdr->type;
+
+    	// Simple rotate to mix bits a bit more
+    	checksum = (checksum << 5) | (checksum >> (16 - 5));
+
+	// XOR the checksum with a constant to prevent trivial values like all zeros or all ones passing the checksum
+	const uint16_t constant = 0xA5A5;  // constant pattern
+	checksum ^= constant;
+	
+    	return htons(checksum); // Convert back to network byte order
+	printk(KERN_INFO "Exiting function wg_header_checksum\n");
+}
+
+// Function to validate the header checksum
+static bool wg_validate_header_checksum(const struct wg_tcp_encap_header *hdr)
+{
+	printk(KERN_INFO "Entering function wg_validate_header_checksum\n");
+	printk(KERN_INFO "Exiting function wg_validate_header_checksum\n");
+    	return wg_header_checksum(hdr) == hdr->checksum;
+}
+
+
+static int wg_tcp_send(struct socket *sock, const void *buff, size_t len,
+		       __u8 type, __u8 flags)
+{
+	struct wg_tcp_encap_header header;
+    	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL };
+    	struct kvec vec[2];
+    	int sent;
+
+    	// Logging entry into the function
+    	printk(KERN_INFO "wg_tcp_send: Entering function\n");
+
+    	// Prepare the header
+	header.length = htonl(len + WG_TCP_ENCAP_HDR_LEN); // Include the payload length and header length
+    	header.type = type;
+    	header.flags = flags;
+    	header.checksum = wg_header_checksum(&header); // Compute checksum for the header
+
+	// Log header information
+	printk(KERN_INFO "wg_tcp_send: Header - Length: %u, Type: %u, Flags: %u, Checksum: 0x%x\n",
+		ntohl(header.length), header.type, header.flags, ntohs(header.checksum));
+
+	// Set up the vector for the header and the payload
+    	vec[0].iov_base = &header;
+	vec[0].iov_len = WG_TCP_ENCAP_HDR_LEN;
+    	vec[1].iov_base = (void *)buff; // Cast away const
+    	vec[1].iov_len = len;
+
+	// Log payload information
+	printk(KERN_INFO "wg_tcp_send: Payload - Length: %zu, First 16 Bytes: %*ph\n", len, 16, buff);
+
+	// Send the message including the header and the payload
+	sent = kernel_sendmsg(sock, &msg, vec, 2, WG_TCP_ENCAP_HDR_LEN + len);
+	if (sent >= 0) {
+		// Successfully sent some or all data
+		printk(KERN_INFO "wg_tcp_send: Sent %d bytes\n", sent);
+		printk(KERN_INFO "wg_tcp_send: Exiting function successfully\n");
+		return sent;
+	} else {
+		// An error occurred; return the error code
+		switch (sent) {
+			case -EAGAIN:
+				printk(KERN_WARNING "wg_tcp_send: Send would block, socket buffer full (EAGAIN)\n");
+				break;
+			case -EPIPE:
+				printk(KERN_ERR "wg_tcp_send: Broken pipe (EPIPE)\n");
+				break;
+			case -EINVAL:
+				printk(KERN_ERR "wg_tcp_send: Invalid argument (EINVAL)\n");
+				break;
+			case -ENOMEM:
+				printk(KERN_ERR "wg_tcp_send: Out of memory (ENOMEM)\n");
+				break;
+			case -ENOTCONN:
+				printk(KERN_ERR "wg_tcp_send: Socket is not connected (ENOTCONN)\n");
+				break;
+			default:
+				printk(KERN_ERR "wg_tcp_send: Error %d\n", sent);
+				break;
+		}
+		printk(KERN_INFO "wg_tcp_send: Exiting function with error\n");
+		return sent;
+	}
+}
+void wg_tcp_write_worker(struct work_struct *work)
+{
+	
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_write_work);
+	struct sock *sk = peer->peer_socket->sk;    
+    	struct sk_buff *skb;
+    	int sent;
+
+	printk(KERN_INFO "Entering function wg_tcp_write_worker\n");
+
+	if (!peer) {
+		printk(KERN_INFO "wg_tcp_write_worker peer is NULL\n");
+		goto out;
+	}
+	if (!peer->peer_socket) {
+		printk(KERN_INFO "wg_tcp_write_worker peer->peer_socker is NULL\n");
+		goto out;
+	}
+	
+    	if (!sk_stream_is_writeable(sk)) {
+        	// Socket is not ready for writing, exit and wait for sk_write_space activation
+		printk(KERN_INFO "wg_tcp_write_worker sk stream is NOT writeable\n");
+        	goto out;
+	}
+
+    	while ((skb = skb_peek(&peer->send_queue)) != NULL && sk_stream_is_writeable(sk)) {
+		sent = wg_tcp_send(peer->peer_socket, skb->data, skb->len, 0, 0);  // no type or flags for now
+		printk(KERN_INFO "wg_tcp_write_worker sent %d bytes\n", sent);
+        	if (sent > 0) {
+            		if (sent < skb->len) {
+                		// Handle partial send by trimming the skb and leaving it in the queue
+                		skb_pull(skb, sent);
+			} else {
+				// Full send successful, dequeue and free the skb
+				spin_lock_bh(&peer->send_queue_lock);
+				__skb_unlink(skb, &peer->tcp_packet_queue);
+				spin_unlock_bh(&peer->send_queue_lock);
+				kfree_skb(skb);
+			}
+        	} else if (sent == 0) {
+            		// Socket buffer is full, stop sending and wait for sk_write_space
+			printk(KERN_INFO "wg_tcp_write_worker socket buffer is full\n");
+            		break;
+        	} else {
+			// An error occurred, dequeue and free the skb
+			spin_lock_bh(&peer->send_queue_lock);
+			__skb_unlink(skb, &peer->tcp_packet_queue);
+			spin_unlock_bh(&peer->send_queue_lock);
+            		kfree_skb(skb);
+            			break;
+        	}
+    	}
+
+out:
+	spin_lock_bh(&peer->tcp_write_lock);
+	peer->tcp_write_worker_scheduled = false;
+	spin_unlock_bh(&peer->tcp_write_lock);
+	printk(KERN_INFO "Exiting function wg_tcp_write_work\n");
+}
+
+void wg_peer_discard_partial_read(struct wg_peer *peer);
+
+void wg_peer_discard_partial_read(struct wg_peer *peer)
+{
+	if (peer->partial_skb)
+		kfree_skb(peer->partial_skb);
+	peer->partial_skb = NULL;
+	peer->expected_len = 0;
+	peer->received_len = 0;
+}
+
+bool wg_sync_header(struct wg_peer *peer);
+
+bool wg_sync_header(struct wg_peer *peer)
+{
+	struct sk_buff *read_skb = NULL;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec vec;
+	int read_bytes;
+	bool found = false;
+	// Attempt to read as much data as available from the socket
+	printk(KERN_INFO "Entering function wg_sync_header\n");
+
+	// Now attempt to find the next valid header within the data we already have
+	printk(KERN_INFO "wg_sync_header: Trying to synchonize to new header.\n");
+	if (peer->partial_skb && peer->received_len > WG_TCP_ENCAP_HDR_LEN) {
+		// If there are less then 8 bytes left, give up (there's no room for a wg_tcp_encap_header)
+		for (size_t i = 0; i <= peer->received_len - WG_TCP_ENCAP_HDR_LEN; ++i) {
+			// Attempt to validate the header starting from the current byte
+			struct wg_tcp_encap_header *potential_hdr = (struct wg_tcp_encap_header *)(peer->partial_skb + i);
+			if (wg_validate_header_checksum(potential_hdr)) {
+				printk(KERN_INFO "wg_sync_header: Found new header.\n");
+				found = true;
+				// Adjust the skb to start from the found valid header
+				skb_pull(peer->partial_skb, i);
+				peer->received_len -= i; // Update received_len to remaining data length
+				peer->expected_len = ntohl(potential_hdr->length); // Set expected length from valid header
+				goto out; // Exit as we've found a starting point
+			}		
+		}
+	}
+	// not in the existing buffer, try to read more
+	read_skb = alloc_skb(WG_MAX_PACKET_SIZE + NET_IP_ALIGN, GFP_ATOMIC); // Allocate buffer for bulk read
+	if (!read_skb) {
+					pr_err("WireGuard: Failed to allocate skb for bulk data read\n");
+	}
+	skb_reserve(read_skb, NET_IP_ALIGN);
+	
+	// Perform the read operation
+	vec.iov_base = skb_put(read_skb,0); // Prepare space
+	vec.iov_len = skb_tailroom(read_skb);
+	read_bytes = kernel_recvmsg(peer->peer_socket, &msg, &vec, 1, vec.iov_len,
+						MSG_DONTWAIT );
+
+	// Print the number of bytes read and the actual data
+	printk(KERN_INFO "wg_sync_header: kernel_recvmsg read %d bytes: %*ph\n", (int)read_bytes, (int)read_bytes, vec.iov_base);
+	
+	if (read_bytes <= 0) {
+		if (read_bytes == -EAGAIN) {
+			// No more data available, exit
+			kfree_skb(read_skb);
+			goto out;
+		}
+		pr_err("WireGuard: Error receiving bulk data from socket\n");
+		kfree_skb(read_skb);
+		goto out;
+	}
+	skb_trim(read_skb, read_bytes); // Trim skb to actual size of received data
+	
+	// Now attempt to find the next valid header within the newly read data
+	printk(KERN_INFO "wg_sync_header: Trying to synchonize to new header.\n");
+	for (size_t i = 0; i <= read_skb->len - WG_TCP_ENCAP_HDR_LEN; ++i) {
+		// Attempt to validate the header starting from the current byte
+		struct wg_tcp_encap_header *potential_hdr = (struct wg_tcp_encap_header *)(read_skb->data + i);
+		if (wg_validate_header_checksum(potential_hdr)) {
+			printk(KERN_INFO "wg_sync_header: Found new header.\n");
+			found = true;
+			// Adjust the skb to start from the found valid header
+			skb_pull(read_skb, i);
+			if (peer->partial_skb)
+					kfree_skb(peer->partial_skb);  // free discarded data buffer
+			peer->partial_skb = read_skb; // Transfer ownership of the buffer to partial_skb for further processing
+			peer->received_len = read_bytes - i; // Update received_len to remaining data length
+			peer->expected_len = ntohl(potential_hdr->length); // Set expected length from valid header
+			break; // Exit the loop as we've found a starting point
+		}		
+	}
+	if (!found){
+		wg_peer_discard_partial_read(peer);
+		kfree_skb(read_skb);
+	}
+out:
+	printk(KERN_INFO "Exiting function wg_sync_header\n");
+	return found;
+}
+
+// Function to check if the given data pointer has a valid WireGuard TCP encapsulation header
+bool wg_check_potential_header_validity(struct wg_tcp_encap_header *hdr, size_t remaining_len)
+{
+    printk(KERN_INFO "Entering function wg_check_potential_header_validity\n");
+
+    if (remaining_len < WG_TCP_ENCAP_HDR_LEN) {
+        printk(KERN_INFO "Not enough data for a header, remaining length: %zu\n", remaining_len);
+        printk(KERN_INFO "Exiting function wg_check_potential_header_validity\n");
+        return false;
+    }
+
+    // Perform checksum validation
+    bool valid = wg_header_checksum(hdr) == hdr->checksum;
+
+    printk(KERN_INFO "Header Checksum Validation - Expected: 0x%x, Actual: 0x%x, Valid: %d\n",
+           ntohs(hdr->checksum), ntohs(wg_header_checksum(hdr)), valid);
+    printk(KERN_INFO "Header Hexdump: %*ph\n", (int)sizeof(*hdr), hdr);
+
+    printk(KERN_INFO "Exiting function wg_check_potential_header_validity\n");
+    return valid;
+}
+
+int wg_tcp_build_fake_headers(struct sk_buff *skb, struct wg_peer *peer)
+{
+	struct ethhdr *ethh;
+	struct iphdr *iph;
+	struct udphdr *udph;
+	int payload_len;
+	int ret;
+
+	// Diagnostic: Print SKB state on entry
+	printk(KERN_INFO "Entering wg_tcp_build_fake_headers. SKB state on entry: "
+	       "skb=%p, len=%d, head=%p, data=%p, tail=%p, end=%p, headroom=%d, tailroom=%d\n",
+	       skb, skb->len, skb->head, skb->data, skb->tail, skb->end, skb_headroom(skb), skb_tailroom(skb));
+
+	log_wireguard_endpoint(&peer->endpoint);
+
+	// Initialize address pointers
+	struct sockaddr_in *source = NULL;
+	struct sockaddr_in *dest = NULL;
+#if IS_ENABLED(CONFIG_IPV6)
+	struct sockaddr_in6 *source6 = NULL;
+	struct sockaddr_in6 *dest6 = NULL;
+#endif
+
+	// Determine source and destination addresses based on socket association
+	if (peer->peer_socket == peer->inbound_socket) {
+		if (peer->inbound_source.ss_family == AF_INET) {
+			source = (struct sockaddr_in *)&peer->inbound_dest;
+			dest = (struct sockaddr_in *)&peer->inbound_source;
+#if IS_ENABLED(CONFIG_IPV6)
+		} else if (peer->inbound_source.ss_family == AF_INET6) {
+			source6 = (struct sockaddr_in6 *)&peer->inbound_dest;
+			dest6 = (struct sockaddr_in6 *)&peer->inbound_source;
+#endif
+		}
+	} else {
+		if (peer->outbound_source.ss_family == AF_INET) {
+			source = (struct sockaddr_in *)&peer->outbound_dest;
+			dest = (struct sockaddr_in *)&peer->outbound_source;
+#if IS_ENABLED(CONFIG_IPV6)
+		} else if (peer->outbound_source.ss_family == AF_INET6) {
+			source6 = (struct sockaddr_in6 *)&peer->outbound_dest;
+			dest6 = (struct sockaddr_in6 *)&peer->outbound_source;
+#endif
+		}
+	}
+
+	// Ensure the SKB is linearized
+	if (skb_linearize(skb) != 0) {
+		printk(KERN_ERR "wg_tcp_build_fake_headers: Failed to linearize SKB.\n");
+		return -ENOMEM;
+	}
+
+	// Diagnostic: Print SKB state after linearization
+	printk(KERN_INFO "After skb_linearize: skb=%p, len=%d, head=%p, data=%p, tail=%p, end=%p, skb->len=%d, headroom=%d, tailroom=%d\n",
+	       skb, skb->len, skb->head, skb->data, skb->tail, skb->end, skb->len, skb_headroom(skb), skb_tailroom(skb));
+
+	// Calculate the payload length: initial length of skb before any header is added
+	payload_len = skb->len;
+
+	// Push and reset for UDP header
+	skb_push(skb, sizeof(struct udphdr));
+	skb_reset_transport_header(skb);
+
+	// Diagnostic: Print UDP header location
+	printk(KERN_INFO "UDP header location: %p, length: %zu\n",
+	       skb_transport_header(skb), sizeof(struct udphdr));
+
+	// Push and reset for IP header
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		skb_push(skb, sizeof(struct iphdr));
+		skb_reset_network_header(skb);
+		printk(KERN_INFO "IPv4 header location: %p, length: %zu\n",
+		       skb_network_header(skb), sizeof(struct iphdr));
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		skb_push(skb, sizeof(struct ipv6hdr));
+		skb_reset_network_header(skb);
+		printk(KERN_INFO "IPv6 header location: %p, length: %zu\n",
+		       skb_network_header(skb), sizeof(struct ipv6hdr));
+#endif
+	} else {
+		printk(KERN_ERR "wg_tcp_build_fake_headers: Unsupported address family.\n");
+		return -EAFNOSUPPORT;
+	}
+
+	// Push and reset for Ethernet header
+	skb_push(skb, sizeof(struct ethhdr));
+	skb_reset_mac_header(skb);
+
+	// Diagnostic: Print Ethernet header location
+	printk(KERN_INFO "Ethernet header location: %p, length: %zu\n",
+	       skb_mac_header(skb), sizeof(struct ethhdr));
+
+	// Diagnostic: Print SKB state after header manipulation
+	printk(KERN_INFO "After header manipulation: skb=%p, len=%d, head=%p, data=%p, tail=%p, end=%p, skb->len=%d, headroom=%d, tailroom=%d\n",
+	       skb, skb->len, skb->head, skb->data, skb->tail, skb->end, skb->len, skb_headroom(skb), skb_tailroom(skb));
+
+	// Set Ethernet header (ethh) fields
+	ethh = eth_hdr(skb);
+	ethh->h_proto = htons(peer->endpoint.addr.sa_family == AF_INET ? ETH_P_IP : ETH_P_IPV6);
+
+	// Set UDP header fields
+	udph = udp_hdr(skb);
+	if (source) { // IPv4 case
+		udph->source = source->sin_port;
+		udph->dest = dest->sin_port;
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (source6) { // IPv6 case
+		udph->source = source6->sin6_port;
+		udph->dest = dest6->sin6_port;
+#endif
+	}
+	udph->len = htons(sizeof(struct udphdr) + payload_len);
+	udph->check = 0; // Checksum will be calculated later
+
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		// Fill in the IPv4 header
+		iph = ip_hdr(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tos = 0;
+		iph->tot_len = htons(sizeof(struct iphdr) + sizeof(struct udphdr) + payload_len);
+		iph->id = 0;
+		iph->frag_off = 0;
+		iph->ttl = 64;
+		iph->protocol = IPPROTO_UDP;
+		iph->check = 0;
+		iph->saddr = source->sin_addr.s_addr;
+		iph->daddr = dest->sin_addr.s_addr;
+
+		// Calculate IP checksum
+		iph->check = ip_fast_csum((u8 *)iph, iph->ihl);
+
+		// Calculate UDP checksum for IPv4
+		__wsum csum = csum_partial(udph, ntohs(udph->len), 0);
+		udph->check = htons(csum_tcpudp_magic(iph->saddr, iph->daddr, udph->len, IPPROTO_UDP, csum));
+		if (udph->check == 0)
+			udph->check = CSUM_MANGLED_0;
+
+		skb->protocol = htons(ETH_P_IP);
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		struct ipv6hdr *ip6h = ipv6_hdr(skb);
+
+		// Fill in the IPv6 header
+		ip6h->version = 6;
+		ip6h->priority = 0;
+		memset(ip6h->flow_lbl, 0, sizeof(ip6h->flow_lbl));
+		ip6h->payload_len = htons(sizeof(struct udphdr) + payload_len);
+		ip6h->nexthdr = IPPROTO_UDP;
+		ip6h->hop_limit = 64;
+		ip6h->saddr = source6->sin6_addr;
+		ip6h->daddr = dest6->sin6_addr;
+
+		// Calculate UDP checksum for IPv6
+		__wsum csum = csum_partial(udph, ntohs(udph->len), 0);
+		csum = csum_partial(&ip6h->saddr, sizeof(struct in6_addr), csum);
+		csum = csum_partial(&ip6h->daddr, sizeof(struct in6_addr), csum);
+		csum = csum_add(csum, htons(ntohs(udph->len)));
+		csum = csum_add(csum, htons(IPPROTO_UDP));
+
+		udph->check = csum_fold(csum);
+		if (udph->check == 0)
+			udph->check = CSUM_MANGLED_0;
+
+		skb->protocol = htons(ETH_P_IPV6);
+#endif
+	} else {
+		printk(KERN_ERR "wg_tcp_build_fake_headers: Unsupported address family.\n");
+		return -EAFNOSUPPORT;
+	}
+
+	// Pull to reset skb->data pointer back to original payload start
+	skb_pull(skb, sizeof(struct ethhdr));
+	skb_pull(skb, (peer->endpoint.addr.sa_family == AF_INET) ? sizeof(struct iphdr) : sizeof(struct ipv6hdr));
+	skb_pull(skb, sizeof(struct udphdr));
+
+	// Diagnostic: Print SKB state after header manipulation
+	printk(KERN_INFO "After header pull on exit: skb=%p, len=%d, head=%p, data=%p, tail=%p, end=%p, headroom=%d, tailroom=%d\n",
+	       skb, skb->len, skb->head, skb->data, skb->tail, skb->end, skb_headroom(skb), skb_tailroom(skb));
+
+	return 0;
+}
+
+
+
+void wg_tcp_read_worker(struct work_struct *work)
+{
+	
+	printk(KERN_INFO "Entering function wg_tcp_read_worker\n");
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_read_work);
+	struct sock *sk = peer->peer_socket->sk;    
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec vec;
+	ssize_t read_bytes;
+	unsigned maxpacket;
+	struct sk_buff *new_skb = NULL;
+
+
+	if (!peer || IS_ERR(peer))
+		goto out;
+	if (!peer->peer_socket)
+		goto out;
+	print_peer_socket_info(peer);
+// XXX not sure needed	lock_sock(sk); // Lock the socket for reading
+	maxpacket = 8192;
+	while (true) {
+			printk(KERN_INFO "wg_peer diagnostic: partial_skb=%p, expected_len=%zu, received_len=%zu\n",
+       				peer->partial_skb, peer->expected_len, peer->received_len);
+			if (!peer->partial_skb) {
+				printk(KERN_INFO "wg_tcp_read_worker: Allocating new skb.\n");
+				// Allocate buffer for the maximum packet size initially, including space for ethernet, IP and UDP headers
+				new_skb = alloc_skb(maxpacket*3 + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32, GFP_ATOMIC);
+				if (!new_skb) {
+					pr_err("WireGuard: Failed to allocate skb\n");
+					break;
+				}
+				// Reserve space for headers and align the data correctly
+				skb_reserve(new_skb, NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr)  + ETH_HLEN + 32);
+
+				peer->expected_len = 0;
+				peer->partial_skb = new_skb;
+			} 
+			// Make sure we hav enough room for at least an encapsulation header
+			if (skb_tailroom(peer->partial_skb) < WG_TCP_ENCAP_HDR_LEN) {
+				printk(KERN_INFO "wg_tcp_read_worker: Reallocating skb to fit the encapsulation header.\n");
+				// Check if the current skb has enough room; if not, reallocate a new skb with sufficient space;
+				new_skb = skb_copy_expand(peer->partial_skb, skb_headroom(peer->partial_skb), WG_MAX_PACKET_SIZE + NET_IP_ALIGN + 
+				       					sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN, GFP_ATOMIC);
+				if (!new_skb) {
+					pr_err("WireGuard: Failed to reallocate skb\n");
+					wg_peer_discard_partial_read(peer);
+					break;
+				}
+				// Free the old skb and replace it with the new one
+				kfree_skb(peer->partial_skb);
+				peer->partial_skb = new_skb;
+			} 
+			// Read as much data as fits into the skb buffer
+			vec.iov_base = peer->partial_skb->data;
+			vec.iov_len = skb_tailroom(peer->partial_skb);
+			if (vec.iov_len > 0) {
+				read_bytes = kernel_recvmsg(peer->peer_socket, &msg, &vec, 1, vec.iov_len, msg.msg_flags);
+				printk(KERN_INFO "wg_tcp_read_worker: kernel_recvmsg read %ld bytes: %*ph\n", read_bytes, (int)read_bytes, vec.iov_base);
+				if (read_bytes <= 0) {
+					if (read_bytes == -EAGAIN) {
+						printk(KERN_INFO "wg_tcp_read_worker: No more data available.\n");
+						break; // No more data available, exit the loop
+					} else {
+						pr_err("WireGuard: Error receiving data from socket\n");
+						wg_peer_discard_partial_read(peer);
+		   				break;
+					}
+				}
+				skb_put(peer->partial_skb, read_bytes);
+				peer->received_len += read_bytes;
+			} 
+			// check header
+			if (peer->received_len >= WG_TCP_ENCAP_HDR_LEN) {
+	            		// Complete header received, validate and prepare for packet data
+				printk("wg_tcp_read_worker: We have a header, let's check it.\n");
+				struct wg_tcp_encap_header *hdr = (struct wg_tcp_encap_header *)peer->partial_skb->data;
+				// Check header validity
+				// Use wg_validate_header_checksum as the criteria for checking header validity
+				if (!wg_check_potential_header_validity((struct wg_tcp_encap_header *)hdr, peer->received_len)) {
+					pr_err("WireGuard: Invalid packet header detected, attempting to resynchronize\n");
+					if (!wg_sync_header(peer)) {
+						pr_err("WireGuard: Failed to find valid header in bulk read data\n");
+						wg_peer_discard_partial_read(peer);
+						break;	
+					}
+				}
+				peer->expected_len = ntohl(hdr->length);
+			} else {
+				// not enough data
+				break;
+			}
+			printk(KERN_INFO "wg_tcp_read_worker: We have a header, let's process the packet body.\n");
+			// If received_len is greater than expected_len (which includes WG_TCP_ENCAP_HDR_LEN),
+			// it implies there's more data potentially for another packet or part of the current
+			//packet beyond what was expected.
+			if (peer->received_len < peer->expected_len) {
+				printk(KERN_INFO "wg_tcp_read_worker: We need more data for a full packet expected len=%d received_len=%d\n", (int)peer->expected_len, (int)peer->received_len);
+				if ((skb_tailroom(peer->partial_skb) < peer->expected_len) &&
+						  (peer->received_len < peer->expected_len)) {
+					printk(KERN_INFO "wg_tcp_read_worker: Expanding buffer to fit whole packet.\n");
+					// check if need a bigger buffer
+					struct sk_buff *resized_skb = skb_copy_expand(peer->partial_skb, 0,
+									peer->expected_len - skb_tailroom(peer->partial_skb),
+									GFP_ATOMIC);
+					if (!resized_skb) {
+						pr_err("WireGuard: Failed to resize skb\n");
+						wg_peer_discard_partial_read(peer);
+						break;
+					}
+					if (peer->partial_skb)
+						kfree_skb(peer->partial_skb);
+					peer->partial_skb = resized_skb;
+				}
+	            
+			}
+			printk(KERN_INFO "Expected Length: %d Received Length: %d\n", peer->expected_len, peer->received_len);
+   			// Check if we've received the complete packet now
+			if (peer->received_len >= peer->expected_len && peer->received_len > WG_TCP_ENCAP_HDR_LEN) {
+				printk(KERN_INFO "wg_tcp_read_worker: We have a complete packet.\n");
+
+				// Remove the encapsulation header from the skb
+				skb_pull(peer->partial_skb, WG_TCP_ENCAP_HDR_LEN);
+				peer->received_len -= WG_TCP_ENCAP_HDR_LEN;
+				peer->expected_len -= WG_TCP_ENCAP_HDR_LEN;
+				printk(KERN_INFO "Packet: %*ph\n", peer->received_len, peer->partial_skb->data);
+				printk(KERN_INFO "skb->len=%d received_len=%d expected_len=%d\n", peer->partial_skb->len, peer->expected_len, peer->received_len);
+				// Check if the skb has a valid length
+				if (unlikely(peer->partial_skb->len <= 0)) {
+					pr_warn("wg_receive: Dropped packet with invalid length %d\n", peer->partial_skb->len);
+					wg_peer_discard_partial_read(peer);  // Reset for the next packet
+					break;
+				}
+				// Calculate leftover data length
+				size_t leftover_len = peer->received_len - peer->expected_len;
+				struct sk_buff *leftover_skb = NULL;
+				if (leftover_len > 0) {
+					// Trim the partial_skb to exclude the leftover data
+					skb_trim(peer->partial_skb, peer->expected_len);
+					leftover_skb = alloc_skb(maxpacket*3 + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32, GFP_ATOMIC);
+					if (!new_skb) {
+						pr_err("WireGuard: Failed to allocate skb\n");
+						break;
+					}
+					// Reserve space for headers and align the data correctly
+					skb_reserve(new_skb, NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr)  + ETH_HLEN + 32);
+
+
+					// Diagnostic: Check skb pointers and lengths after skb_reserve
+					printk(KERN_INFO "Diagnostic after skb_reserve:\n");
+					printk(KERN_INFO "skb=%p, len=%d, headroom=%d, tailroom=%d, head=%p, data=%p, tail=%p, end=%p\n",
+       							new_skb, new_skb->len, skb_headroom(new_skb), skb_tailroom(new_skb), new_skb->head,
+       							new_skb->data, new_skb->tail, new_skb->end);
+					// Error checking: Verify the buffer pointers are set correctly
+#ifdef BROKENCHECK
+					if (unlikely(new_skb->data != new_skb->head + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32)) {
+					pr_err("Error: skb_reserve did not set the data pointer correctly. Expected offset: %lu, Actual offset: %lu\n",
+           								(unsigned long)(new_skb->head + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32),
+           								(unsigned long)new_skb->data);
+    						kfree_skb(new_skb);
+						new_skb = NULL;
+    						wg_peer_discard_partial_read(peer);
+    						return;
+					}
+#endif // BROKENCHECK
+					if (peer->expected_len + leftover_len <= peer->partial_skb->len) {
+						if (skb_copy_bits(peer->partial_skb, peer->expected_len, leftover_skb->data, leftover_len) < 0) {
+							pr_err("wg_tcp_read_worker: Failed to copy leftover data.\n");
+								kfree_skb(leftover_skb);
+								leftover_skb = NULL;
+								wg_peer_discard_partial_read(peer);
+								break;
+						}
+					} else {
+   						pr_err("wg_tcp_read_worker: Invalid leftover length, expected_len=%d, received_len=%d, skb_len=%d\n",
+           					(int)peer->expected_len, (int)peer->received_len, (int)peer->partial_skb->len);
+    						wg_peer_discard_partial_read(peer);
+    						break;
+					}
+	
+					printk(KERN_INFO "wg_tcp_read_worker: Leftover data at end of packet, leftover_len=%d\n", (int)leftover_len);
+					// Use skb_copy_bits to copy data from the end of partial_skb to the new leftover_skb
+					if (skb_copy_bits(peer->partial_skb, peer->expected_len, leftover_skb->data, leftover_len) < 0) {
+						pr_err("wg_tcp_read_worker: Failed to copy leftover data.\n");
+						kfree_skb(leftover_skb);
+						leftover_skb = NULL;
+						wg_peer_discard_partial_read(peer);
+						break;
+					}
+
+					skb_set_tail_pointer(leftover_skb, leftover_len);
+	   				printk(KERN_INFO "wg_tcp_read_worker: leftover_skb after copy, leftover_skb=%p, len=%d, headroom=%d, data=%p, tail=%p, end=%p\n",
+						leftover_skb, leftover_skb->len, skb_headroom(leftover_skb), leftover_skb->data, leftover_skb->tail, leftover_skb->end);
+				}
+				skb_set_tail_pointer(peer->partial_skb, peer->expected_len); // should be redundant
+				// Build the UDP and IP headers
+				if (wg_tcp_build_fake_headers(peer->partial_skb, peer)) {
+					pr_err("WireGuard: Failed to build UDP/IP headers\n");
+					wg_peer_discard_partial_read(peer);
+					break;
+				}
+				// Process the complete packet
+				printk(KERN_INFO "wg_tcp_read_worker: partial_skb after trim, partial_skb=%p, len=%d, head=%p, data=%p, tail=%p, end=%p\n",
+								peer->partial_skb, peer->partial_skb->len, peer->partial_skb->head,
+								peer->partial_skb->data, peer->partial_skb->tail, peer->partial_skb->end);
+
+				wg_receive(sk, peer->partial_skb); // wg_receive consumes the skb
+
+				peer->partial_skb = NULL;  // wg_receive ate the data skb
+				if (leftover_len > 0) {
+					// Store the leftover skb (if any) in peer->partial_skb
+					peer->partial_skb = leftover_skb;
+					peer->received_len = leftover_len;
+
+				} else
+					peer->received_len = 0;
+				peer->expected_len = 0; // Reset for the next packet
+			}
+	}
+// XXX not sure needed	release_sock(sk); // Unlock the socket
+	
+out:
+	// Reset the flag to indicate the worker has finished processing
+	spin_lock_bh(&peer->tcp_read_lock);
+	peer->tcp_read_worker_scheduled = false;
+	spin_unlock_bh(&peer->tcp_read_lock);
+	printk(KERN_INFO "Exiting function wg_tcp_read_worker\n");
+}
+
+void wg_tcp_data_ready(struct sock *sk)
+{
+	printk(KERN_INFO "Entering function wg_tcp_data_ready\n");
+	
+	// Ensure the socket is valid
+	if (!sk || IS_ERR(sk)) {
+		printk(KERN_ERR "wg_tcp_data_ready: Invalid socket\n");
+		goto out;
+	}
+
+	// Retrieve the socket user data
+	struct wg_socket_data *socket_data = sk->sk_user_data;
+
+	// Check if socket_data is valid
+	if (!socket_data || IS_ERR(socket_data)) {
+		printk(KERN_ERR "wg_tcp_data_ready: Invalid or NULL socket_data\n");
+		goto out;
+	}
+
+	// Retrieve the peer from the socket_data
+	struct wg_peer *peer = socket_data->peer;
+
+	// Check if peer is valid
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_ERR "wg_tcp_data_Ready: Invalid or NULL peer\n");
+		goto out;
+	}
+
+	
+	spin_lock_bh(&peer->tcp_read_lock);
+
+	// Check if the worker is already scheduled
+	if (!peer->tcp_read_worker_scheduled) {
+        	peer->tcp_read_worker_scheduled = true;
+		queue_work(peer->tcp_read_wq, &peer->tcp_read_work);
+	}
+
+	spin_unlock_bh(&peer->tcp_read_lock);
+
+out:	
+    	// Call the original data_ready callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_data_ready) {
+        		peer->original_inbound_data_ready(sk);
+   		}
+	} else {
+		if (peer->original_outbound_data_ready) {
+        		peer->original_outbound_data_ready(sk);
+   		}
+	}
+	printk(KERN_INFO "Exiting function wg_tcp_data_ready\n");
+}
+
+void wg_tcp_write_space(struct sock *sk)
+{
+	printk(KERN_INFO "Entering function wg_tcp_write_space\n");
+	struct wg_peer *peer;
+	struct wg_socket_data *socket_data;
+	if (!sk)
+		goto out;
+	socket_data = sk->sk_user_data;
+	if (!socket_data || IS_ERR(socket_data))
+		goto out;
+	peer = socket_data->peer;
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_INFO "wg_tcp_write_space peer is NULL\n");
+		goto out;
+	}
+	if (!peer->tcp_write_wq) {
+		printk(KERN_INFO "wg_tcp_write_space peer->tcp_write_wq is NULL\n");
+		goto out;
+	}
+	
+	spin_lock_bh(&peer->tcp_write_lock);
+
+	// Check if the worker is already scheduled
+	if (!peer->tcp_write_worker_scheduled) {
+		printk(KERN_INFO "wg_tcp_write_space setting peer->tcp_write_worker_scheduled = true\n");
+        	peer->tcp_write_worker_scheduled = true;
+		printk(KERN_INFO "wg_tcp_write_space calling queue_work()\n");
+		queue_work(peer->tcp_write_wq, &peer->tcp_write_work);
+	}
+
+	spin_unlock_bh(&peer->tcp_write_lock);
+out:
+    	// Call the original write_space callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_write_space) {
+        		peer->original_inbound_write_space(sk);
+   		}
+	} else {
+		if (peer->original_outbound_write_space) {
+        		peer->original_outbound_write_space(sk);
+   		}
+	}
+	printk(KERN_INFO "Exiting function wg_tcp_write_space\n");
+}
+
+void wg_setup_tcp_socket_callbacks(struct wg_peer *peer, bool inbound)
+{
+	printk(KERN_INFO "Entering function wg_setup_tcp_socket_callbacks\n");
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_INFO "Exiting function wg_setup_tcp_socket_callbacks, no peer.\n");
+		return;
+	}
+	struct socket *target_socket = inbound ? peer->inbound_socket : peer->outbound_socket;
+
+	if (!target_socket || (inbound ? peer->tcp_inbound_callbacks_set : peer->tcp_outbound_callbacks_set)) {
+		printk(KERN_INFO "Exiting function wg_setup_tcp_socket_callbacks, nothing to do.\n");
+		return;
+	}
+
+	struct sock *sk = target_socket->sk;
+	struct wg_socket_data *socket_data;
+
+	if (inbound)
+		peer->tcp_inbound_callbacks_set = true;
+	else
+		peer->tcp_outbound_callbacks_set = true;
+
+	// Acquire lock to safely modify socket callbacks
+	write_lock_bh(&sk->sk_callback_lock);
+
+	// Check if sk_user_data is already allocated
+	socket_data = sk->sk_user_data;
+	if (socket_data) {
+		// If already allocated, update the peer
+		printk(KERN_INFO "wg_setup_tcp_socket_callbacks: sk_user_data already exists, updating peer.\n");
+		socket_data->device = peer->device;
+		socket_data->peer = peer;
+	} else {
+		// Allocate memory for wg_socket_data
+		socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+		if (!socket_data) {
+			printk(KERN_ERR "Failed to allocate memory for wg_socket_data\n");
+			write_unlock_bh(&sk->sk_callback_lock);
+			return;
+		}
+
+		// Initialize wg_socket_data with device and peer
+		socket_data->device = peer->device;
+		socket_data->peer = peer;
+		socket_data->inbound = inbound;
+
+		// Set sk_user_data to the newly allocated socket_data
+		sk->sk_user_data = socket_data;
+	}
+
+	// Save the original callbacks based on the direction (inbound or outbound)
+	if (inbound) {
+		peer->original_inbound_state_change = sk->sk_state_change;
+		peer->original_inbound_write_space = sk->sk_write_space;
+		peer->original_inbound_data_ready = sk->sk_data_ready;
+	} else {
+		peer->original_outbound_state_change = sk->sk_state_change;
+		peer->original_outbound_write_space = sk->sk_write_space;
+		peer->original_outbound_data_ready = sk->sk_data_ready;
+	}
+
+	// Assign new callbacks and pass `peer` as user data for callback functions
+	sk->sk_state_change = wg_tcp_state_change;
+	sk->sk_write_space = wg_tcp_write_space;
+	sk->sk_data_ready = wg_tcp_data_ready;
+
+	write_unlock_bh(&sk->sk_callback_lock);
+	printk(KERN_INFO "Exiting function wg_setup_tcp_socket_callbacks\n");
+}
+
+void wg_reset_tcp_socket_callbacks(struct wg_peer *peer, bool inbound)
+{
+	printk(KERN_INFO "Entering function wg_reset_tcp_socket_callbacks\n");
+	struct sock *sk;
+	struct socket *target_socket = inbound ? peer->inbound_socket : peer->outbound_socket;
+
+	if (!peer || IS_ERR(peer)) {
+		printk(KERN_INFO "Exiting function wg_reset_tcp_socket_callbacks, no peer.\n");
+		return;
+	}
+	if (!target_socket || (inbound ? !peer->tcp_inbound_callbacks_set : !peer->tcp_outbound_callbacks_set)) {
+		printk(KERN_INFO "Exiting function wg_reset_tcp_socket_callbacks, nothing to do.\n");
+		return;
+	}
+
+	if (inbound)
+		peer->tcp_inbound_callbacks_set = false;
+	else
+		peer->tcp_outbound_callbacks_set = false;
+
+	sk = target_socket->sk;
+
+	// Lock the socket to safely update callback pointers
+	write_lock_bh(&sk->sk_callback_lock);
+
+	// Check if we previously saved original callbacks and restore them
+	if (inbound) {
+		if (peer->original_inbound_state_change) {
+			sk->sk_state_change = peer->original_inbound_state_change;
+			peer->original_inbound_state_change = NULL;
+		}
+		if (peer->original_inbound_write_space) {
+			sk->sk_write_space = peer->original_inbound_write_space;
+			peer->original_inbound_write_space = NULL;
+		}
+		if (peer->original_inbound_data_ready) {
+			sk->sk_data_ready = peer->original_inbound_data_ready;
+			peer->original_inbound_data_ready = NULL;
+		}
+	} else {
+		if (peer->original_outbound_state_change) {
+			sk->sk_state_change = peer->original_outbound_state_change;
+			peer->original_outbound_state_change = NULL;
+		}
+		if (peer->original_outbound_write_space) {
+			sk->sk_write_space = peer->original_outbound_write_space;
+			peer->original_outbound_write_space = NULL;
+		}
+		if (peer->original_outbound_data_ready) {
+			sk->sk_data_ready = peer->original_outbound_data_ready;
+			peer->original_outbound_data_ready = NULL;
+		}
+	}
+
+	// Clear the user data to avoid any dangling references
+	if (sk->sk_user_data)
+		kfree(sk->sk_user_data);
+	sk->sk_user_data = NULL;
+
+	write_unlock_bh(&sk->sk_callback_lock);
+	printk(KERN_INFO "Exiting function wg_reset_tcp_socket_callbacks\n");
+}
+
+void wg_tcp_retry_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_retry_work.work);
+
+	printk(KERN_INFO "Entering function wg_tcp_retry_worker peer=%p\n", peer);
+
+	if (peer->tcp_established == false) {
+		if (peer->tcp_pending) {
+			// Check the state of the socket
+			struct sock *sk = peer->outbound_socket->sk;
+			// Connection still pending, perform cleanup
+			printk(KERN_INFO "TCP connection still pending, releasing socket\n");
+
+			// Reset connection state
+			peer->tcp_pending = false;
+			peer->tcp_established = false;
+			wg_reset_tcp_socket_callbacks(peer, false);
+			// Release the socket
+			if (peer->peer_socket == peer->outbound_socket)
+				peer->peer_socket = NULL;
+			if (peer->outbound_socket) {
+				if (peer->outbound_socket->sk->sk_user_data) {
+					kfree(peer->outbound_socket->sk->sk_user_data);
+					peer->peer_socket->sk->sk_user_data = NULL;
+				}
+				sock_release(peer->outbound_socket);
+				peer->outbound_socket = NULL;
+			}
+		}
+	}
+		
+	int ret = wg_tcp_connect(peer);
+	if (ret < 0) {
+		// Reschedule the work if the connection attempt fails
+		schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(30*HZ));
+		peer->tcp_retry_scheduled = true;
+	} else {
+		peer->tcp_retry_scheduled = false;
+	}
+
+	printk(KERN_INFO "Exiting function wg_tcp_retry_worker\n");
+}
+
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *receive_socket)
+{
+	printk(KERN_INFO "Entering function wg_add_tcp_socket_to_list\n");
+	struct wg_tcp_socket_list_entry *entry;
+	struct sockaddr_storage addr;
+
+	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry) {
+		pr_err("Failed to allocate wg_tcp_socket_list_entry\n");
+        	printk(KERN_INFO "Exiting function wg_add_tcp_socket_to_list\n");
+        	return;
+    	}
+
+    	entry->tcp_socket = receive_socket;
+    	entry->timestamp = ktime_get();
+
+    	// Initialize addr structure to zero
+    	memset(&addr, 0, sizeof(addr));
+
+    	// Get the source address from the socket
+    	if (receive_socket->ops->getname(receive_socket, (struct sockaddr *)&addr,  1) < 0) {
+        	pr_err("Failed to get peer address from socket\n");
+        	kfree(entry);
+        	printk(KERN_INFO "Exiting function wg_add_tcp_socket_to_list\n");
+        	return;
+    	}
+
+    	// Copy the obtained address to the entry's src_addr
+    	memcpy(&entry->src_addr, &addr, sizeof(addr));
+	
+	spin_lock_bh(&wg->tcp_connection_list_lock);
+    	// Add the entry to the tcp_connection_list
+    	list_add_tail_rcu(&entry->tcp_connection_ll, &wg->tcp_connection_list);
+	spin_unlock_bh(&wg->tcp_connection_list_lock);
+
+	printk(KERN_INFO "Exiting function wg_add_tcp_socket_to_list\n");
+}
+
+void wg_remove_from_tcp_connection_list(struct wg_device *wg, struct socket *pending_socket)
+{
+	printk(KERN_INFO "Entering function wg_remove_from_tcp_connection_list\n");
+	struct wg_tcp_socket_list_entry *entry;
+
+	if (!pending_socket)
+		goto out;
+
+
+	// Check if the connection list is empty
+	if (list_empty(&wg->tcp_connection_list)) {
+		printk(KERN_INFO "TCP connection list is empty, nothing to destroy\n");
+		return;
+	}
+	
+    	list_for_each_entry_rcu(entry, &wg->tcp_connection_list, tcp_connection_ll) {
+        	if (entry->tcp_socket == pending_socket) {
+			spin_lock_bh(&wg->tcp_connection_list_lock);
+            		list_del_rcu(&entry->tcp_connection_ll);
+			spin_unlock_bh(&wg->tcp_connection_list_lock);
+            		synchronize_rcu();
+            		if (entry->tcp_socket) {
+				kernel_sock_shutdown(entry->tcp_socket, SHUT_RDWR);
+				sock_release(entry->tcp_socket);
+           		}
+			// clean up old temp_peer
+			if (!IS_ERR(entry->temp_peer) && entry->temp_peer) {
+				// flush any partial data free the held buffer
+				if (entry->temp_peer->partial_skb) {
+        	        		kfree_skb(entry->temp_peer->partial_skb);
+				}
+				// Clean up packet queues
+    				skb_queue_purge(&entry->temp_peer->tcp_packet_queue);
+				
+				// Check if the TCP read work is scheduled before canceling it
+    				if (entry->temp_peer->tcp_read_worker_scheduled) {
+        				cancel_work_sync(&entry->temp_peer->tcp_read_work);
+   			     		entry->temp_peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+    				}
+
+    				// Destroy the TCP read workqueue if it exists
+    				if (entry->temp_peer->tcp_read_wq) {
+       			 		destroy_workqueue(entry->temp_peer->tcp_read_wq);
+        				entry->temp_peer->tcp_read_wq = NULL; // Avoid dangling pointers
+		    		}
+
+
+				kfree(entry->temp_peer);
+			}
+			// Free the old entry
+            		kfree(entry);
+            		break;
+        	}
+    	}
+out:
+    	printk(KERN_INFO "Exiting function wg_remove_from_tcp_connection_list\n");
+}
+
+void wg_tcp_outbound_remove_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_outbound_remove_work.work);
+
+	printk(KERN_INFO "Entering function wg_tcp_outbound_remove _worker\n");
+
+	wg_reset_tcp_socket_callbacks(peer, false);
+	wg_clean_peer_socket(peer, true, false, false); // clean and release
+	
+    	printk(KERN_INFO "Exiting function wg_tcp_outbound_remove_worker\n");
+}
+
+void wg_tcp_inbound_remove_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_inbound_remove_work.work);
+
+	printk(KERN_INFO "Entering function wg_tcp_inbound_remove _worker\n");
+
+	if (peer->temp_peer){
+		wg_remove_from_tcp_connection_list(peer->device, peer->peer_socket);
+	} else {
+		wg_reset_tcp_socket_callbacks(peer, true);
+		wg_clean_peer_socket(peer, true, false, true); // clean and release
+	}
+    	printk(KERN_INFO "Exiting function wg_inbound_remove_worker\n");
+}
+
+void wg_destruct_tcp_connection_list(struct wg_device *wg)
+{
+	printk(KERN_INFO "Entering function wg_destruct_tcp_connection_list\n");
+	struct wg_tcp_socket_list_entry *entry, *tmp;
+
+	// Iterate over the entire list and free each entry
+	list_for_each_entry_safe(entry, tmp, &wg->tcp_connection_list, tcp_connection_ll) {
+		spin_lock_bh(&wg->tcp_connection_list_lock);
+		list_del(&entry->tcp_connection_ll); // Removes the entry from the list
+        	spin_unlock_bh(&wg->tcp_connection_list_lock);
+		// Release the socket
+		if (entry->tcp_socket) {
+			kernel_sock_shutdown(entry->tcp_socket, SHUT_RDWR);
+			sock_release(entry->tcp_socket); // Release the socket
+		}
+		// Check if the TCP read work is scheduled before canceling it
+    		if (entry->temp_peer->tcp_read_worker_scheduled) {
+        		cancel_work_sync(&entry->temp_peer->tcp_read_work);
+        		entry->temp_peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+    		}
+
+    		// Destroy the TCP read workqueue if it exists
+    		if (entry->temp_peer->tcp_read_wq) {
+        		destroy_workqueue(entry->temp_peer->tcp_read_wq);
+        		entry->temp_peer->tcp_read_wq = NULL; // Avoid dangling pointers
+    		}
+
+		// clean up old temp_peer
+		if (!IS_ERR(entry->temp_peer) && entry->temp_peer) 
+			kfree(entry->temp_peer);
+		kfree(entry); // Free the memory allocated for the list entry
+	}
+
+	printk(KERN_INFO "Exiting function wg_destruct_tcp_connection_list\n");
+}
+
+void wg_tcp_cleanup_worker(struct work_struct *work)
+{
+	printk(KERN_INFO "Entering function wg_tcp_cleanup_worker\n");
+	struct wg_device *wg = container_of(work, struct wg_device, tcp_cleanup_work.work);
+	struct wg_tcp_socket_list_entry *entry, *tmp;
+	struct wg_peer *peer = NULL;
+	
+	ktime_t now = ktime_get();
+
+	// Cleanup logic: Remove old entries from the TCP connection list
+
+	list_for_each_entry_safe(entry, tmp, &wg->tcp_connection_list, tcp_connection_ll) {
+		if (ktime_ms_delta(now, entry->timestamp) > 5000) { // Check if older than 5 seconds
+			wg_remove_from_tcp_connection_list(wg, entry->tcp_socket);
+		}
+	}
+
+#ifdef WRONG
+	// Walk through the wg_dev peer list and call wg_tcp_write_space for each socket
+	rcu_read_lock();
+	list_for_each_entry_rcu(peer, &wg->peer_list, peer_list) {
+		// Check and call wg_tcp_write_space for socket if not null
+		if (peer->peer_socket) {
+			rcu_read_unlock();
+			wg_tcp_write_space(peer->peer_socket->sk);
+			rcu_read_lock();
+		}
+	}
+	rcu_read_unlock();
+#endif
+	
+	// Reschedule the worker
+	peer->device->tcp_cleanup_scheduled = true;
+	schedule_delayed_work(&wg->tcp_cleanup_work, msecs_to_jiffies(5000));
+	printk(KERN_INFO "Exiting function wg_tcp_cleanup_worker\n");
+}
+
+struct wg_peer *wg_temp_peer_create(struct wg_device *wg)
+{
+	struct wg_peer *peer;
+	int ret = -ENOMEM;
+	
+	printk(KERN_INFO "wg_peer_create: entry with wg=%p\n", wg);
+	
+	peer = kmalloc(sizeof(struct wg_peer), GFP_KERNEL);
+	if (unlikely(!peer)) {
+		printk(KERN_INFO "wg_temp_peer_create: exit with ERR_PTR(ret)\n");
+		return ERR_PTR(ret);
+	}
+	if (unlikely(dst_cache_init(&peer->endpoint_cache, GFP_KERNEL))) {
+		goto err;
+	}
+
+	
+	struct wg_socket_data *socket_data = kmalloc(sizeof(struct wg_socket_data), GFP_KERNEL);
+	if (unlikely(!socket_data)) {
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		goto err;
+	}
+
+	// Initialize the socket_data structure
+	socket_data->device = wg;
+	socket_data->peer = peer;
+	
+	peer->device = wg;
+	peer->internal_id = (u64)NULL;
+	peer->serial_work_cpu = (int)nr_cpumask_bits;
+	wg_timers_init(peer);
+	wg_prev_queue_init(&peer->tx_queue);
+	wg_prev_queue_init(&peer->rx_queue);
+	rwlock_init(&peer->endpoint_lock);
+	kref_init(&peer->refcount);
+	skb_queue_head_init(&peer->staged_packet_queue);
+	wg_noise_reset_last_sent_handshake(&peer->last_sent_handshake);
+	set_bit(NAPI_STATE_NO_BUSY_POLL, &peer->napi.state);
+	netif_napi_add(wg->dev, &peer->napi, wg_packet_rx_poll);
+	napi_enable(&peer->napi);
+	INIT_LIST_HEAD(&peer->allowedips_list);
+
+	// initialize TCP fields
+	peer->peer_socket = NULL;  // Initialize the peer socket to NULL
+
+	// Initialize the original socket callbacks to NULL
+	peer->original_outbound_state_change = NULL;
+	peer->original_outbound_write_space = NULL;
+	peer->original_outbound_data_ready = NULL;
+	peer->original_outbound_error_report = NULL;
+	peer->original_outbound_destruct = NULL;
+
+	peer->original_inbound_state_change = NULL;
+	peer->original_inbound_write_space = NULL;
+	peer->original_inbound_data_ready = NULL;
+	peer->original_inbound_error_report = NULL;
+	peer->original_inbound_destruct = NULL;
+
+	peer->partial_skb = NULL;  // Initialize the partial skb pointer to NULL
+	peer->expected_len = 0;    // Initialize expected length to 0
+	peer->received_len = 0;    // Initialize received length to 0
+
+	// Initialize the skb queue for queuing TCP packets
+	skb_queue_head_init(&peer->tcp_packet_queue);
+
+	// Initialize the delayed work for TCP connection retry
+	INIT_DELAYED_WORK(&peer->tcp_retry_work, wg_tcp_retry_worker);
+
+	// Initialize the delayed work for TCP socket removal
+	INIT_DELAYED_WORK(&peer->tcp_inbound_remove_work, wg_tcp_inbound_remove_worker);
+	INIT_DELAYED_WORK(&peer->tcp_outbound_remove_work, wg_tcp_outbound_remove_worker);
+	
+	// Initialize TCP connection status flags
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->tcp_inbound_callbacks_set = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->clean_inbound = false;
+	peer->clean_outbound = false;
+	peer->inbound_connected = false;
+	peer->outbound_connected = false;
+	peer->tcp_retry_scheduled = false;
+	peer->tcp_inbound_remove_scheduled = false;
+	peer->tcp_outbound_remove_scheduled = false;
+
+	// Initialize the spinlock for protecting TCP-related state
+	spin_lock_init(&peer->tcp_lock);
+
+	// Initialize the skb queue for the TX send queue
+	skb_queue_head_init(&peer->send_queue);
+
+	// Initialize the spinlock for the TX send queue
+	spin_lock_init(&peer->send_queue_lock);
+
+	// Initialize the list head for pending connection list
+	INIT_LIST_HEAD(&peer->pending_connection_list);
+
+	// Initialize the work structure, associating it with the worker functions
+	INIT_WORK(&peer->tcp_read_work, wg_tcp_read_worker);
+	// Create a workqueue for processing TCP read data
+	peer->tcp_read_wq = alloc_workqueue("tcp_read_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_read_wq) {
+        	pr_err("Failed to allocate read workqueue\n");
+		goto err;
+	}
+
+	INIT_WORK(&peer->tcp_write_work, wg_tcp_write_worker);
+
+	// Note this is a temp peer
+	peer->temp_peer = true;
+
+	pr_debug("%s: Temp Peer %llu created\n", wg->dev->name, peer->internal_id);
+	printk(KERN_INFO "wg_temp_peer_create: exit with peer=%p\n", peer);
+	return peer;
+
+err:
+	kfree(peer);
+	printk(KERN_INFO "wg_temp_peer_create: exit with ERR_PTR(ret) on err\n");
+	return ERR_PTR(ret);
 }
diff --git a/wireguard-linux/drivers/net/wireguard/socket.h b/wireguard-linux/drivers/net/wireguard/socket.h
index bab5848efbcd..e3b236abaa1d 100644
--- a/wireguard-linux/drivers/net/wireguard/socket.h
+++ b/wireguard-linux/drivers/net/wireguard/socket.h
@@ -6,6 +6,7 @@
 #ifndef _WG_SOCKET_H
 #define _WG_SOCKET_H
 
+#include <linux/net.h>
 #include <linux/netdevice.h>
 #include <linux/udp.h>
 #include <linux/if_vlan.h>
@@ -29,6 +30,17 @@ void wg_socket_set_peer_endpoint(struct wg_peer *peer,
 void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer,
 					  const struct sk_buff *skb);
 void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
+void wg_destruct_tcp_connection_list(struct wg_device *wg);
+
+struct wg_tcp_encap_header {
+	__be32 length;
+	__u8 type;
+	__u8 flags;
+	__be16 checksum;
+};
+
+#define WG_TCP_ENCAP_HDR_LEN sizeof(struct wg_tcp_encap_header)
+#define WG_MAX_PACKET_SIZE 65535 + WG_TCP_ENCAP_HDR_LEN
 
 #if defined(CONFIG_DYNAMIC_DEBUG) || defined(DEBUG)
 #define net_dbg_skb_ratelimited(fmt, dev, skb, ...) do {                       \
@@ -41,4 +53,45 @@ void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
 #define net_dbg_skb_ratelimited(fmt, skb, ...)
 #endif
 
+/* Forward declarations of functions */
+int wg_socket_send_skb_to_peer(struct wg_peer *peer, struct sk_buff *skb, u8 ds);
+int wg_socket_send_buffer_to_peer(struct wg_peer *peer, void *buffer, size_t len, u8 ds);
+int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg, struct sk_buff *in_skb, void *buffer, size_t len);
+int wg_socket_endpoint_from_skb(struct endpoint *endpoint, const struct sk_buff *skb);
+void wg_socket_set_peer_endpoint(struct wg_peer *peer, const struct endpoint *endpoint);
+void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer, const struct sk_buff *skb);
+void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
+void wg_socket_reinit(struct wg_device *wg, struct sock *new4, struct sock *new6);
+void wg_tcp_state_change(struct sock *sk);
+void wg_extract_endpoint_from_sock(struct sock *sk, struct endpoint *endpoint);
+bool wg_check_potential_header_validity(struct wg_tcp_encap_header *hdr, size_t remaining_len);
+
+int wg_tcp_queuepkt(struct wg_peer *, const void *, size_t);
+void wg_tcp_write_space(struct sock *sk);
+void wg_tcp_data_ready(struct sock *sk);
+
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *sock);
+void wg_remove_from_tcp_connection_list(struct wg_device *wg, struct socket *sock);
+void wg_destruct_tcp_connection_list(struct wg_device *wg);
+
+int wg_tcp_listener_socket_init(struct wg_device *wg, u16 port);
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
+void wg_tcp_connection_retry_timer(struct timer_list *);
+int wg_tcp_connect(struct wg_peer *);
+
+int wg_tcp_listener_worker(struct wg_device *wg, struct socket *tcp_socket);
+struct socket *wg_setup_tcp_listen4(struct wg_device *wg, struct net *net, u16 port);
+struct socket *wg_setup_tcp_listen6(struct wg_device *wg, struct net *net, u16 port);
+int wg_tcp_listener4_thread(void *data);
+int wg_tcp_listener6_thread(void *data);
+
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound);
+void wg_timers_init(struct wg_peer *peer);
+void wg_tcp_write_worker(struct work_struct *work);
+void wg_tcp_read_worker(struct work_struct *work);
+void wg_tcp_cleanup_worker(struct work_struct *work);
+
+void lookup_default_interface(void);
+
 #endif /* _WG_SOCKET_H */
diff --git a/wireguard-linux/drivers/net/wireguard/timers.c b/wireguard-linux/drivers/net/wireguard/timers.c
index 968bdb4df0b3..cec2e5d407fb 100644
--- a/wireguard-linux/drivers/net/wireguard/timers.c
+++ b/wireguard-linux/drivers/net/wireguard/timers.c
@@ -207,7 +207,7 @@ void wg_timers_session_derived(struct wg_peer *peer)
 }
 
 /* Should be called before a packet with authentication, whether
- * keepalive, data, or handshakem is sent, or after one is received.
+ * keepalive, data, or handshake is sent, or after one is received.
  */
 void wg_timers_any_authenticated_packet_traversal(struct wg_peer *peer)
 {
