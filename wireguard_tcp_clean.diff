diff --git a/wireguard-linux/drivers/net/wireguard/device.c b/wireguard-linux/drivers/net/wireguard/device.c
index deb9636b0ecf..546815f42017 100644
--- a/wireguard-linux/drivers/net/wireguard/device.c
+++ b/wireguard-linux/drivers/net/wireguard/device.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -19,6 +20,8 @@
 #include <linux/if_arp.h>
 #include <linux/icmp.h>
 #include <linux/suspend.h>
+#include <linux/spinlock.h>
+#include <linux/wireguard.h>
 #include <net/dst_metadata.h>
 #include <net/gso.h>
 #include <net/icmp.h>
@@ -26,6 +29,8 @@
 #include <net/ip_tunnels.h>
 #include <net/addrconf.h>
 
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
 static LIST_HEAD(device_list);
 
 static int wg_open(struct net_device *dev)
@@ -34,7 +39,7 @@ static int wg_open(struct net_device *dev)
 	struct inet6_dev *dev_v6 = __in6_dev_get(dev);
 	struct wg_device *wg = netdev_priv(dev);
 	struct wg_peer *peer;
-	int ret;
+	int ret = 0;
 
 	if (dev_v4) {
 		/* At some point we might put this check near the ip_rt_send_
@@ -47,10 +52,16 @@ static int wg_open(struct net_device *dev)
 	if (dev_v6)
 		dev_v6->cnf.addr_gen_mode = IN6_ADDR_GEN_MODE_NONE;
 
-	mutex_lock(&wg->device_update_lock);
+	wg->listener_active = false;
+	if (wg->transport == WG_TRANSPORT_TCP) {
+		ret = wg_tcp_listener_socket_init(wg, wg->incoming_port);
+		if (ret < 0)
+			goto out;
+	}
 	ret = wg_socket_init(wg, wg->incoming_port);
 	if (ret < 0)
-		goto out;
+		 goto out;
+	mutex_lock(&wg->device_update_lock);
 	list_for_each_entry(peer, &wg->peer_list, peer_list) {
 		wg_packet_send_staged_packets(peer);
 		if (peer->persistent_keepalive_interval)
@@ -129,8 +140,22 @@ static int wg_stop(struct net_device *dev)
 	mutex_unlock(&wg->device_update_lock);
 	while ((skb = ptr_ring_consume(&wg->handshake_queue.ring)) != NULL)
 		kfree_skb(skb);
+
 	atomic_set(&wg->handshake_queue_len, 0);
+
+	// Cancel the delayed work and wait for it to finish
+	cancel_delayed_work_sync(&wg->tcp_cleanup_work);
+
+	// Clear the tcp_cleanup_scheduled flag
+	spin_lock_bh(&wg->tcp_cleanup_lock);
+	wg->tcp_cleanup_scheduled = false;
+	spin_unlock_bh(&wg->tcp_cleanup_lock);
+
+	if (wg->transport == WG_TRANSPORT_TCP)
+		wg_tcp_listener_socket_release(wg);
+	
 	wg_socket_reinit(wg, NULL, NULL);
+
 	return 0;
 }
 
@@ -265,6 +290,7 @@ static void wg_destruct(struct net_device *dev)
 	free_percpu(dev->tstats);
 	kvfree(wg->index_hashtable);
 	kvfree(wg->peer_hashtable);
+	wg_destruct_tcp_connection_list(wg);
 	mutex_unlock(&wg->device_update_lock);
 
 	pr_debug("%s: Interface destroyed\n", dev->name);
@@ -286,7 +312,7 @@ static void wg_setup(struct net_device *dev)
 	dev->header_ops = &ip_tunnel_header_ops;
 	dev->hard_header_len = 0;
 	dev->addr_len = 0;
-	dev->needed_headroom = DATA_PACKET_HEAD_ROOM;
+	dev->needed_headroom = DATA_PACKET_HEAD_ROOM + (wg->transport ? WG_TCP_ENCAP_HDR_LEN : 0);
 	dev->needed_tailroom = noise_encrypted_len(MESSAGE_PADDING_MULTIPLE);
 	dev->type = ARPHRD_NONE;
 	dev->flags = IFF_POINTOPOINT | IFF_NOARP;
@@ -296,7 +322,8 @@ static void wg_setup(struct net_device *dev)
 	dev->hw_features |= WG_NETDEV_FEATURES;
 	dev->hw_enc_features |= WG_NETDEV_FEATURES;
 	dev->mtu = ETH_DATA_LEN - overhead;
-	dev->max_mtu = round_down(INT_MAX, MESSAGE_PADDING_MULTIPLE) - overhead;
+	dev->max_mtu = round_down(INT_MAX, MESSAGE_PADDING_MULTIPLE) - overhead -
+				  (wg->transport == WG_TRANSPORT_TCP ? WG_TCP_ENCAP_HDR_LEN : 0);
 
 	SET_NETDEV_DEVTYPE(dev, &device_type);
 
@@ -322,6 +349,12 @@ static int wg_newlink(struct net *src_net, struct net_device *dev,
 	wg_cookie_checker_init(&wg->cookie_checker, wg);
 	INIT_LIST_HEAD(&wg->peer_list);
 	wg->device_update_gen = 1;
+	// Initialize the tcp_cleanup_scheduled flag and spinlock
+	wg->tcp_cleanup_scheduled = false;
+	spin_lock_init(&wg->tcp_cleanup_lock);
+
+	// Initialize the work for tcp_cleanup_worker
+	INIT_DELAYED_WORK(&wg->tcp_cleanup_work, wg_tcp_cleanup_worker);
 
 	wg->peer_hashtable = wg_pubkey_hashtable_alloc();
 	if (!wg->peer_hashtable)
@@ -375,6 +408,13 @@ static int wg_newlink(struct net *src_net, struct net_device *dev,
 
 	list_add(&wg->device_list, &device_list);
 
+	wg->incoming_port = WG_INCOMING_PORT;
+
+	INIT_LIST_HEAD(&wg->tcp_connection_list);
+	spin_lock_init(&wg->tcp_connection_list_lock);
+	wg->tcp_socket4_ready = false;
+	wg->tcp_socket6_ready = false;
+
 	/* We wait until the end to assign priv_destructor, so that
 	 * register_netdevice doesn't call it for us if it fails.
 	 */
diff --git a/wireguard-linux/drivers/net/wireguard/device.h b/wireguard-linux/drivers/net/wireguard/device.h
index 43c7cebbf50b..a5e8f67a533d 100644
--- a/wireguard-linux/drivers/net/wireguard/device.h
+++ b/wireguard-linux/drivers/net/wireguard/device.h
@@ -1,5 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -14,9 +15,13 @@
 #include <linux/types.h>
 #include <linux/netdevice.h>
 #include <linux/workqueue.h>
+#include <linux/ktime.h>
 #include <linux/mutex.h>
 #include <linux/net.h>
 #include <linux/ptr_ring.h>
+#include <linux/spinlock.h>
+
+#define WG_INCOMING_PORT 51820
 
 struct wg_device;
 
@@ -37,10 +42,27 @@ struct prev_queue {
 	atomic_t count;
 };
 
+struct endpoint {
+	union {
+		struct sockaddr addr;
+		struct sockaddr_in addr4;
+		struct sockaddr_in6 addr6;
+	};
+	union {
+		struct {
+			struct in_addr src4;
+			/* Essentially the same as addr6->scope_id */
+			int src_if4;
+		};
+		struct in6_addr src6;
+	};
+};
+
 struct wg_device {
 	struct net_device *dev;
 	struct crypt_queue encrypt_queue, decrypt_queue, handshake_queue;
-	struct sock __rcu *sock4, *sock6;
+	struct sock __rcu *sock4, *sock6; // UDP listening sockets
+	struct socket __rcu *tcp_listen_socket4, *tcp_listen_socket6; // TCP listening sockets
 	struct net __rcu *creating_net;
 	struct noise_static_identity static_identity;
 	struct workqueue_struct *packet_crypt_wq,*handshake_receive_wq, *handshake_send_wq;
@@ -49,11 +71,21 @@ struct wg_device {
 	struct index_hashtable *index_hashtable;
 	struct allowedips peer_allowedips;
 	struct mutex device_update_lock, socket_update_lock;
-	struct list_head device_list, peer_list;
+	struct endpoint device_endpoint;
+	struct list_head device_list, peer_list, tcp_connection_list;
+	struct task_struct *tcp_listener4_thread, *tcp_listener6_thread;
+	struct delayed_work tcp_cleanup_work;
+	spinlock_t tcp_cleanup_lock; // Add a spinlock to protect the flag
+	bool tcp_cleanup_scheduled;
+	bool tcp_socket4_ready;
+	bool tcp_socket6_ready;
+	bool listener_active;
+	spinlock_t tcp_connection_list_lock;
 	atomic_t handshake_queue_len;
 	unsigned int num_peers, device_update_gen;
 	u32 fwmark;
 	u16 incoming_port;
+	u8 transport;
 };
 
 int wg_device_init(void);
diff --git a/wireguard-linux/drivers/net/wireguard/main.c b/wireguard-linux/drivers/net/wireguard/main.c
index ee4da9ab8013..8a4376812ad8 100644
--- a/wireguard-linux/drivers/net/wireguard/main.c
+++ b/wireguard-linux/drivers/net/wireguard/main.c
@@ -9,6 +9,7 @@
 #include "queueing.h"
 #include "ratelimiter.h"
 #include "netlink.h"
+#include "socket.h"
 
 #include <uapi/linux/wireguard.h>
 
@@ -17,6 +18,78 @@
 #include <linux/genetlink.h>
 #include <net/rtnetlink.h>
 
+#include <linux/netdevice.h>  // Required for net_device
+#include <linux/inetdevice.h> // Required for inetdev processing
+
+// Global structure to hold default network interface information
+struct default_interface_info {
+	struct net_device *dev;   	// Default network interface
+	__be32 ipv4_address;      	// IPv4 address of the default interface
+	struct in6_addr ipv6_address;	// IPv6 address of the default interface
+	bool ipv4_available;		// Flag indicating if IPv4 is available
+	bool ipv6_available;		// Flag indicating if IPv6 is available
+};
+
+struct default_interface_info default_iface_info;
+
+#include <net/route.h>			// Required for routing table access
+#include <net/ip_fib.h>			// Required for FIB (Forwarding Information Base) access
+
+void lookup_default_interface(void)
+{
+	// Use the initial network namespace
+	struct net *net = &init_net;
+	struct flowi4 fl4 = {
+		// Using 8.8.8.8 as a dummy external destination
+		.daddr = htonl(0x08080808),
+	};
+	struct rtable *rt = ip_route_output_key(net, &fl4);
+
+	if (!rt)
+		return;
+
+	// Get the main interface used for routing
+	struct net_device *main_dev = rt->dst.dev;
+
+	if (!main_dev) {
+		ip_rt_put(rt);
+		return;
+	}
+
+	default_iface_info.dev = main_dev;
+
+	// Retrieve the IPv4 address
+	struct in_device *in_dev = __in_dev_get_rtnl(main_dev);
+	struct in_ifaddr *ifa = NULL;
+
+	if (in_dev) {
+		for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+			if (ifa->ifa_scope == RT_SCOPE_UNIVERSE) {
+				default_iface_info.ipv4_address = ifa->ifa_address;
+				default_iface_info.ipv4_available = true;
+				break;
+			}
+		}
+	}
+
+	// Retrieve the IPv6 address
+	struct inet6_dev *in6_dev = __in6_dev_get(main_dev);
+	struct inet6_ifaddr *ifa6 = NULL;
+
+	if (in6_dev) {
+		list_for_each_entry(ifa6, &in6_dev->addr_list, if_list) {
+			if (ifa6->scope == RT_SCOPE_UNIVERSE) {
+				default_iface_info.ipv6_address = ifa6->addr;
+				default_iface_info.ipv6_available = true;
+				break;
+			}
+		}
+	}
+
+	// Clean up routing table reference
+	ip_rt_put(rt);
+}
+
 static int __init wg_mod_init(void)
 {
 	int ret;
@@ -47,7 +120,10 @@ static int __init wg_mod_init(void)
 
 	pr_info("WireGuard " WIREGUARD_VERSION " loaded. See www.wireguard.com for information.\n");
 	pr_info("Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.\n");
+	pr_info("TCP Transport Mode - Copyright (C) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.\n");
 
+	lookup_default_interface();
+	
 	return 0;
 
 err_netlink:
@@ -71,7 +147,7 @@ static void __exit wg_mod_exit(void)
 module_init(wg_mod_init);
 module_exit(wg_mod_exit);
 MODULE_LICENSE("GPL v2");
-MODULE_DESCRIPTION("WireGuard secure network tunnel");
+MODULE_DESCRIPTION("WireGuard secure network tunnel - with UDP/TCP");
 MODULE_AUTHOR("Jason A. Donenfeld <Jason@zx2c4.com>");
 MODULE_VERSION(WIREGUARD_VERSION);
 MODULE_ALIAS_RTNL_LINK(KBUILD_MODNAME);
diff --git a/wireguard-linux/drivers/net/wireguard/netlink.c b/wireguard-linux/drivers/net/wireguard/netlink.c
index e220d761b1f2..264aa3ca52b7 100644
--- a/wireguard-linux/drivers/net/wireguard/netlink.c
+++ b/wireguard-linux/drivers/net/wireguard/netlink.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -13,10 +14,15 @@
 #include <uapi/linux/wireguard.h>
 
 #include <linux/if.h>
+#include <linux/version.h>
 #include <net/genetlink.h>
+#include <net/netlink.h>
 #include <net/sock.h>
+#include <crypto/algapi.h>
 #include <crypto/utils.h>
 
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
 static struct genl_family genl_family;
 
 static const struct nla_policy device_policy[WGDEVICE_A_MAX + 1] = {
@@ -27,7 +33,8 @@ static const struct nla_policy device_policy[WGDEVICE_A_MAX + 1] = {
 	[WGDEVICE_A_FLAGS]		= { .type = NLA_U32 },
 	[WGDEVICE_A_LISTEN_PORT]	= { .type = NLA_U16 },
 	[WGDEVICE_A_FWMARK]		= { .type = NLA_U32 },
-	[WGDEVICE_A_PEERS]		= { .type = NLA_NESTED }
+	[WGDEVICE_A_PEERS]		= { .type = NLA_NESTED },
+	[WGDEVICE_A_TRANSPORT]		= { .type = NLA_U8 }
 };
 
 static const struct nla_policy peer_policy[WGPEER_A_MAX + 1] = {
@@ -200,7 +207,11 @@ static int wg_get_device_start(struct netlink_callback *cb)
 {
 	struct wg_device *wg;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6,6,0)
 	wg = lookup_interface(genl_info_dump(cb)->attrs, cb->skb);
+#else
+	wg = lookup_interface(genl_dumpit_info(cb)->attrs, cb->skb);
+#endif
 	if (IS_ERR(wg))
 		return PTR_ERR(wg);
 	DUMP_CTX(cb)->wg = wg;
@@ -233,7 +244,8 @@ static int wg_get_device_dump(struct sk_buff *skb, struct netlink_callback *cb)
 				wg->incoming_port) ||
 		    nla_put_u32(skb, WGDEVICE_A_FWMARK, wg->fwmark) ||
 		    nla_put_u32(skb, WGDEVICE_A_IFINDEX, wg->dev->ifindex) ||
-		    nla_put_string(skb, WGDEVICE_A_IFNAME, wg->dev->name))
+		    nla_put_string(skb, WGDEVICE_A_IFNAME, wg->dev->name) ||
+		    nla_put_u8(skb, WGDEVICE_A_TRANSPORT, wg->transport))
 			goto out;
 
 		down_read(&wg->static_identity.lock);
@@ -314,7 +326,8 @@ static int wg_get_device_done(struct netlink_callback *cb)
 static int set_port(struct wg_device *wg, u16 port)
 {
 	struct wg_peer *peer;
-
+	int rett, retu;
+	
 	if (wg->incoming_port == port)
 		return 0;
 	list_for_each_entry(peer, &wg->peer_list, peer_list)
@@ -323,7 +336,19 @@ static int set_port(struct wg_device *wg, u16 port)
 		wg->incoming_port = port;
 		return 0;
 	}
-	return wg_socket_init(wg, port);
+
+	if (wg->transport == WG_TRANSPORT_TCP) {
+		/* Release the existing listening sockets and stop the associated threads */
+		wg_tcp_listener_socket_release(wg);
+		/* Reestablish the listening socket and associated threads */
+		rett = wg_tcp_listener_socket_init(wg, port);
+	} 
+        retu = wg_socket_init(wg, port);
+	/* XXX - This disregards the return value of wg_tcp_listener_socket_init */
+        /* Check if either rett or retu is negative and return an error code if so */
+//        if (rett < 0 || retu < 0)
+//                return -1;
+        return retu;	
 }
 
 static int set_allowedip(struct wg_peer *peer, struct nlattr **attrs)
@@ -578,8 +603,13 @@ static int wg_set_device(struct sk_buff *skb, struct genl_info *info)
 		}
 		up_write(&wg->static_identity.lock);
 	}
-skip_set_private_key:
 
+	if (info->attrs[WGDEVICE_A_TRANSPORT]) {
+		u8 transport = nla_get_u8(info->attrs[WGDEVICE_A_TRANSPORT]);
+		wg->transport = transport;
+	}
+
+skip_set_private_key:
 	if (info->attrs[WGDEVICE_A_PEERS]) {
 		struct nlattr *attr, *peer[WGPEER_A_MAX + 1];
 		int rem;
diff --git a/wireguard-linux/drivers/net/wireguard/peer.c b/wireguard-linux/drivers/net/wireguard/peer.c
index 1cb502a932e0..a203efa3de18 100644
--- a/wireguard-linux/drivers/net/wireguard/peer.c
+++ b/wireguard-linux/drivers/net/wireguard/peer.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -9,15 +10,19 @@
 #include "timers.h"
 #include "peerlookup.h"
 #include "noise.h"
+#include "socket.h"
 
 #include <linux/kref.h>
 #include <linux/lockdep.h>
 #include <linux/rcupdate.h>
 #include <linux/list.h>
+#include <linux/wireguard.h>
 
 static struct kmem_cache *peer_cache;
 static atomic64_t peer_counter = ATOMIC64_INIT(0);
 
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound);
+
 struct wg_peer *wg_peer_create(struct wg_device *wg,
 			       const u8 public_key[NOISE_PUBLIC_KEY_LEN],
 			       const u8 preshared_key[NOISE_SYMMETRIC_KEY_LEN])
@@ -59,7 +64,91 @@ struct wg_peer *wg_peer_create(struct wg_device *wg,
 	list_add_tail(&peer->peer_list, &wg->peer_list);
 	INIT_LIST_HEAD(&peer->allowedips_list);
 	wg_pubkey_hashtable_add(wg->peer_hashtable, peer);
+
+	// TCP field initialization
+	peer->peer_socket = NULL;  // Initialize the peer socket to NULL
+
+	// Initialize the original socket callbacks to NULL
+	peer->original_outbound_state_change = NULL;
+	peer->original_outbound_write_space = NULL;
+	peer->original_outbound_data_ready = NULL;
+	peer->original_outbound_error_report = NULL;
+	peer->original_outbound_destruct = NULL;
+
+	peer->original_inbound_state_change = NULL;
+	peer->original_inbound_write_space = NULL;
+	peer->original_inbound_data_ready = NULL;
+	peer->original_inbound_error_report = NULL;
+	peer->original_inbound_destruct = NULL;
+
+	peer->partial_skb = NULL;  // Initialize the partial skb pointer to NULL
+	peer->expected_len = 0;    // Initialize expected length to 0
+	peer->received_len = 0;    // Initialize received length to 0
+
+	// Initialize the TCP retry scheduled flag to false
+	peer->tcp_retry_scheduled = false;
+
+	// Initialize the delayed work for TCP connection retry
+	INIT_DELAYED_WORK(&peer->tcp_retry_work, wg_tcp_retry_worker);
+
+	// Initialize the delayed work for TCP socket removal
+	INIT_DELAYED_WORK(&peer->tcp_inbound_remove_work, wg_tcp_inbound_remove_worker);
+	INIT_DELAYED_WORK(&peer->tcp_outbound_remove_work, wg_tcp_outbound_remove_worker);
+	
+	// Initialize TCP connection status flags
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->tcp_inbound_callbacks_set = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->tcp_outbound_remove_scheduled = false;
+	peer->tcp_inbound_remove_scheduled = false;
+	peer->peer_endpoint_set = false;
+
+	// Initialize the spinlock for protecting TCP-related state
+	spin_lock_init(&peer->tcp_lock);
+
+	// Initialize the skb queue for the TX send queue
+	skb_queue_head_init(&peer->send_queue);
+
+	// Initialize the spinlock for the TX send queue
+	spin_lock_init(&peer->send_queue_lock);
+
+	// Initialize the list head for pending connection list
+	INIT_LIST_HEAD(&peer->pending_connection_list);
+
+	// Initialize the work structure, associating it with the worker functions
+	INIT_WORK(&peer->tcp_read_work, wg_tcp_read_worker);
+	// Create a workqueue for processing TCP read data
+	peer->tcp_read_wq = alloc_workqueue("tcp_read_wq",
+					    WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_read_wq) {
+        	pr_err("Failed to allocate read workqueue\n");
+		goto err;
+	}
+
+	INIT_WORK(&peer->tcp_write_work, wg_tcp_write_worker);
+	// Create a workqueue for processing TCP write data
+	peer->tcp_write_wq = alloc_workqueue("tcp_write_wq",
+					     WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_write_wq) {
+		pr_err("Failed to allocate write workqueue\n");
+		goto err;
+	}
+
+	// Initialize the lock for the tcp_transfer queue
+	spin_lock_init(&peer->tcp_transfer_lock);  // Initialize the lock for the tcp_transfer queue
+	peer->tcp_transfer_wq = alloc_workqueue("wg_tcp_transfer_wq",
+						 WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_transfer_wq)
+		goto err;
+	
+	// Indicate this is a real peer not a temp peer
+	peer->temp_peer = false;
+	peer->peer_endpoint = peer->endpoint;
+	
 	++wg->num_peers;
+	if (wg->transport == WG_TRANSPORT_TCP)
+		wg_tcp_connect(peer);
 	pr_debug("%s: Peer %llu created\n", wg->dev->name, peer->internal_id);
 	return peer;
 
@@ -79,15 +168,56 @@ struct wg_peer *wg_peer_get_maybe_zero(struct wg_peer *peer)
 
 static void peer_make_dead(struct wg_peer *peer)
 {
+	if(!peer || IS_ERR(peer))
+		return;
+
 	/* Remove from configuration-time lookup structures. */
 	list_del_init(&peer->peer_list);
 	wg_allowedips_remove_by_peer(&peer->device->peer_allowedips, peer,
 				     &peer->device->device_update_lock);
 	wg_pubkey_hashtable_remove(peer->device->peer_hashtable, peer);
-
+	
 	/* Mark as dead, so that we don't allow jumping contexts after. */
 	WRITE_ONCE(peer->is_dead, true);
 
+	// Check if the TCP read work is scheduled before canceling it
+	if (peer->tcp_read_worker_scheduled) {
+        	cancel_work_sync(&peer->tcp_read_work);
+        	peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+	}
+
+	// Destroy the TCP read workqueue if it exists
+	if (peer->tcp_read_wq) {
+		destroy_workqueue(peer->tcp_read_wq);
+		peer->tcp_read_wq = NULL; // Avoid dangling pointers
+	}
+
+	// Check if the TCP write work is scheduled before canceling it
+	if (peer->tcp_write_worker_scheduled) {
+		cancel_work_sync(&peer->tcp_write_work);
+		peer->tcp_write_worker_scheduled = false;  // Reset the flag after canceling
+    	}
+
+	// Destroy the TCP write workqueue if it exists
+	if (peer->tcp_write_wq) {
+		destroy_workqueue(peer->tcp_write_wq);
+		peer->tcp_write_wq = NULL; // Avoid dangling pointers
+	}
+
+
+	// Destroy the TCP transfer workqueue if it exists
+    	if (peer->tcp_transfer_wq) {
+        	destroy_workqueue(peer->tcp_transfer_wq);
+		peer->tcp_transfer_wq = NULL; // Avoid dangling pointers
+    	}
+
+	// clean up any partial TCP data if it exists
+	if (peer->partial_skb) {
+		kfree_skb(peer->partial_skb);
+	    	peer->partial_skb = NULL;
+	}	
+
+	
 	/* The caller must now synchronize_net() for this to take effect. */
 }
 
@@ -158,7 +288,8 @@ void wg_peer_remove(struct wg_peer *peer)
 	if (unlikely(!peer))
 		return;
 	lockdep_assert_held(&peer->device->device_update_lock);
-
+	wg_clean_peer_socket(peer, true, true, false); // clean up tcp socket stuff
+	wg_clean_peer_socket(peer, true, true, true);  // both inbound and outbound
 	peer_make_dead(peer);
 	synchronize_net();
 	peer_remove_after_dead(peer);
diff --git a/wireguard-linux/drivers/net/wireguard/peer.h b/wireguard-linux/drivers/net/wireguard/peer.h
index 76e4d3128ad4..ab475ace3883 100644
--- a/wireguard-linux/drivers/net/wireguard/peer.h
+++ b/wireguard-linux/drivers/net/wireguard/peer.h
@@ -1,5 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -18,20 +19,11 @@
 
 struct wg_device;
 
-struct endpoint {
-	union {
-		struct sockaddr addr;
-		struct sockaddr_in addr4;
-		struct sockaddr_in6 addr6;
-	};
-	union {
-		struct {
-			struct in_addr src4;
-			/* Essentially the same as addr6->scope_id */
-			int src_if4;
-		};
-		struct in6_addr src6;
-	};
+
+struct wg_tcp_transfer_work {
+    struct work_struct work;
+    struct sk_buff *skb;
+    struct wg_peer *peer;
 };
 
 struct wg_peer {
@@ -41,7 +33,7 @@ struct wg_peer {
 	int serial_work_cpu;
 	bool is_dead;
 	struct noise_keypairs keypairs;
-	struct endpoint endpoint;
+	struct endpoint endpoint, tcp_reply_endpoint, peer_endpoint;
 	struct dst_cache endpoint_cache;
 	rwlock_t endpoint_lock;
 	struct noise_handshake handshake;
@@ -64,6 +56,69 @@ struct wg_peer {
 	struct list_head allowedips_list;
 	struct napi_struct napi;
 	u64 internal_id;
+
+        // TCP-related members
+	bool peer_endpoint_set;
+	struct socket *peer_socket, *inbound_socket, *outbound_socket;
+	void (*original_outbound_state_change)(struct sock *sk);
+	void (*original_outbound_write_space)(struct sock *sk);
+	void (*original_outbound_data_ready)(struct sock *sk);
+	void (*original_outbound_error_report)(struct sock *sk);
+	void (*original_outbound_destruct)(struct sock *sk);
+	void (*original_inbound_state_change)(struct sock *sk);
+	void (*original_inbound_write_space)(struct sock *sk);
+	void (*original_inbound_data_ready)(struct sock *sk);
+	void (*original_inbound_error_report)(struct sock *sk);
+	void (*original_inbound_destruct)(struct sock *sk);
+	bool tcp_outbound_callbacks_set;			// Flag to track if the inbound socket callbacks have been set
+	bool tcp_inbound_callbacks_set;				// Flag to track if the inbound socket callbacks have been set
+	ktime_t outbound_timestamp, inbound_timestamp;	// timestamps for connections
+	struct sockaddr_storage	inbound_source, outbound_source, inbound_dest, outbound_dest;
+
+	struct sk_buff *partial_skb;
+	size_t expected_len;
+	size_t received_len;
+
+	struct delayed_work tcp_retry_work;	// Work for retrying TCP connection
+	bool tcp_retry_scheduled;		// Flag to track connect retry scheduling
+
+	struct delayed_work tcp_outbound_remove_work;	// Work for removing outbound peer TCP connection
+	bool tcp_outbound_remove_scheduled;		// Flag to track outbound peer removal scheduling
+	struct delayed_work tcp_inbound_remove_work;	// Work for removing inbound peer TCP connection
+	bool tcp_inbound_remove_scheduled;		// Flag to track inbound peer removal scheduling
+
+	struct delayed_work tcp_cleanup_work;	// Work for removing TCP connections in pending list
+	bool tcp_cleanup_scheduled;		// Flag to track removal scheduling
+
+	bool tcp_established;			// Flag to track TCP connection status
+	bool tcp_pending;			// Flag to track outbount pending TCP connection status
+	bool inbound_connected;			// peer connected to us
+	bool outbound_connected;		// we connected to them
+	bool clean_outbound;			// release outbound at next cleanup
+	bool clean_inbound;			// release inbound at next cleanup
+	bool temp_peer;				// is this a temporary peer
+
+
+	struct sk_buff_head send_queue;		// TX queue
+        spinlock_t send_queue_lock;		// TX lock
+
+	struct list_head pending_connection_list;	//peers pending connection handshake
+	spinlock_t tcp_lock;			// Protects TCP-related state
+
+	struct work_struct tcp_read_work;	// Work struct for scheduling the worker
+	struct workqueue_struct *tcp_read_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_read_lock;		// Spinlock to protect access to the socket data
+	bool tcp_read_worker_scheduled;		// Flag to indicate if the TCP read worker is scheduled
+
+	struct work_struct tcp_write_work;      // Work struct for scheduling the worker
+	struct workqueue_struct *tcp_write_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_write_lock;              // Spinlock to protect access to the socket data
+	bool tcp_write_worker_scheduled; 	// Flag to indicate if the TCP write worker is scheduled
+
+	struct work_struct tcp_transfer_work;		// Work struct for scheduling the worker
+	struct workqueue_struct *tcp_transfer_wq;	// Workqueue for handling TCP data processing
+	spinlock_t tcp_transfer_lock;			// Spinlock to protect access to the socket data
+	bool tcp_transfer_worker_scheduled;		// Flag to indicate if the TCP transfer worker is scheduled
 };
 
 struct wg_peer *wg_peer_create(struct wg_device *wg,
@@ -83,4 +138,12 @@ void wg_peer_remove_all(struct wg_device *wg);
 int wg_peer_init(void);
 void wg_peer_uninit(void);
 
+void wg_peer_tcp_connect(struct work_struct *work);
+void wg_peer_tcp_send(struct work_struct *work);
+void wg_peer_tcp_receive(struct work_struct *work);
+void wg_tcp_inbound_remove_worker(struct work_struct *work);
+void wg_tcp_outbound_remove_worker(struct work_struct *work);
+
+void wg_tcp_retry_worker(struct work_struct *work);
+
 #endif /* _WG_PEER_H */
diff --git a/wireguard-linux/drivers/net/wireguard/receive.c b/wireguard-linux/drivers/net/wireguard/receive.c
index db01ec03bda0..044b936b4d1e 100644
--- a/wireguard-linux/drivers/net/wireguard/receive.c
+++ b/wireguard-linux/drivers/net/wireguard/receive.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
@@ -13,8 +14,32 @@
 
 #include <linux/ip.h>
 #include <linux/ipv6.h>
+#include <linux/tcp.h>
 #include <linux/udp.h>
 #include <net/ip_tunnels.h>
+#include <linux/skbuff.h>
+#include <linux/net.h>
+#include <linux/in.h>
+#include <linux/ipv6.h>
+#include <linux/wireguard.h>
+#include <net/ipv6.h>
+#include <net/ip.h>
+
+bool endpoint_eq(const struct endpoint *a, const struct endpoint *b);
+
+struct wg_tcp_socket_list_entry {
+	struct socket *tcp_socket;		// Socket associated with the connection
+	struct sockaddr_storage src_addr;	// Source address for the connection
+	struct wg_peer *temp_peer;		// temporary peer for dataready
+	struct list_head tcp_connection_ll;	// List pointer for the linked list
+	ktime_t timestamp;			// Timestamp when the connection was added
+};
+
+struct wg_socket_data {
+	struct wg_device *device;
+	struct wg_peer *peer;
+	bool inbound;
+};
 
 /* Must be called with bh disabled. */
 static void update_rx_stats(struct wg_peer *peer, size_t len)
@@ -47,48 +72,173 @@ static size_t validate_header_len(struct sk_buff *skb)
 static int prepare_skb_header(struct sk_buff *skb, struct wg_device *wg)
 {
 	size_t data_offset, data_len, header_len;
-	struct udphdr *udp;
+	struct udphdr _udp, *udp;
 
+	// Check packet protocol and header validity
 	if (unlikely(!wg_check_packet_protocol(skb) ||
 		     skb_transport_header(skb) < skb->head ||
 		     (skb_transport_header(skb) + sizeof(struct udphdr)) >
 			     skb_tail_pointer(skb)))
 		return -EINVAL; /* Bogus IP header */
-	udp = udp_hdr(skb);
-	data_offset = (u8 *)udp - skb->data;
+
+	// Safely access UDP header using skb_header_pointer
+	udp = skb_header_pointer(skb, skb_transport_offset(skb),
+				 sizeof(_udp), &_udp);
+	if (!udp)
+		return -EINVAL;
+
+	// Check for valid UDP ports
+	if (unlikely(udp->source == 0 || udp->dest == 0))
+		return -EINVAL;
+
+	// Calculate data offset and validate
+	data_offset = skb_transport_offset(skb) + sizeof(struct udphdr);
+
 	if (unlikely(data_offset > U16_MAX ||
 		     data_offset + sizeof(struct udphdr) > skb->len))
-		/* Packet has offset at impossible location or isn't big enough
-		 * to have UDP fields.
-		 */
 		return -EINVAL;
+
+	// Get the UDP length field
 	data_len = ntohs(udp->len);
+
+	// Validate data length
 	if (unlikely(data_len < sizeof(struct udphdr) ||
-		     data_len > skb->len - data_offset))
-		/* UDP packet is reporting too small of a size or lying about
-		 * its size.
-		 */
+		     data_len > skb->len - skb_transport_offset(skb)))
 		return -EINVAL;
+
+	// Adjust data length to exclude UDP header
 	data_len -= sizeof(struct udphdr);
-	data_offset = (u8 *)udp + sizeof(struct udphdr) - skb->data;
-	if (unlikely(!pskb_may_pull(skb,
-				data_offset + sizeof(struct message_header)) ||
+	data_offset = skb_transport_offset(skb) + sizeof(struct udphdr);
+
+	// Check pull and trim capabilities
+	if (unlikely(!pskb_may_pull(skb,data_offset + sizeof(struct message_header)) ||
 		     pskb_trim(skb, data_len + data_offset) < 0))
 		return -EINVAL;
+
 	skb_pull(skb, data_offset);
+
+	// Validate the SKB length against calculated data length
 	if (unlikely(skb->len != data_len))
-		/* Final len does not agree with calculated len */
 		return -EINVAL;
+
+	// Validate header length
 	header_len = validate_header_len(skb);
+
 	if (unlikely(!header_len))
 		return -EINVAL;
+
 	__skb_push(skb, data_offset);
+
+	// Check pull capabilities after push
 	if (unlikely(!pskb_may_pull(skb, data_offset + header_len)))
 		return -EINVAL;
+
 	__skb_pull(skb, data_offset);
+
+out:
 	return 0;
 }
 
+// Function to extract source and destination sockaddr_storage from an skb
+int extract_sockaddr_from_skb(struct sk_buff *skb, struct sockaddr_storage *source,
+			      struct sockaddr_storage *dest)
+{
+	struct iphdr *ip_header;
+	struct ipv6hdr *ipv6_header;
+	struct tcphdr *tcp_header;
+	struct udphdr *udp_header;
+
+	if (!skb) {
+		return -1; // Invalid skb
+	}
+
+	// Handle IPv4 packets
+	if (skb->protocol == htons(ETH_P_IP)) {
+		ip_header = ip_hdr(skb);
+			if (!ip_header) {
+			return -1; // Failed to get IP header
+		}
+
+		struct sockaddr_in *src_in = (struct sockaddr_in *)source;
+		struct sockaddr_in *dest_in = (struct sockaddr_in *)dest;
+
+		memset(src_in, 0, sizeof(struct sockaddr_in));
+		memset(dest_in, 0, sizeof(struct sockaddr_in));
+
+		src_in->sin_family = AF_INET;
+		dest_in->sin_family = AF_INET;
+
+		src_in->sin_addr.s_addr = ip_header->saddr;
+		dest_in->sin_addr.s_addr = ip_header->daddr;
+
+		// Determine transport protocol
+		if (ip_header->protocol == IPPROTO_TCP) {
+			tcp_header = tcp_hdr(skb);
+			if (!tcp_header) {
+				return -1; // Failed to get TCP header
+			}
+			src_in->sin_port = tcp_header->source;
+			dest_in->sin_port = tcp_header->dest;
+		} else if (ip_header->protocol == IPPROTO_UDP) {
+			udp_header = udp_hdr(skb);
+			if (!udp_header) {
+				return -1; // Failed to get UDP header
+			}
+			src_in->sin_port = udp_header->source;
+			dest_in->sin_port = udp_header->dest;
+		} else {
+			return -1; // Unsupported protocol
+		}
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	// Handle IPv6 packets
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		ipv6_header = ipv6_hdr(skb);
+		if (!ipv6_header) {
+			return -1; // Failed to get IPv6 header
+		}
+
+		struct sockaddr_in6 *src_in6 = (struct sockaddr_in6 *)source;
+		struct sockaddr_in6 *dest_in6 = (struct sockaddr_in6 *)dest;
+
+		memset(src_in6, 0, sizeof(struct sockaddr_in6));
+		memset(dest_in6, 0, sizeof(struct sockaddr_in6));
+
+		src_in6->sin6_family = AF_INET6;
+		dest_in6->sin6_family = AF_INET6;
+
+		src_in6->sin6_addr = ipv6_header->saddr;
+		dest_in6->sin6_addr = ipv6_header->daddr;
+
+		// Determine transport protocol
+		if (ipv6_header->nexthdr == IPPROTO_TCP) {
+			tcp_header = tcp_hdr(skb);
+			if (!tcp_header) {
+				return -1; // Failed to get TCP header
+			}
+			src_in6->sin6_port = tcp_header->source;
+			dest_in6->sin6_port = tcp_header->dest;
+		} else if (ipv6_header->nexthdr == IPPROTO_UDP) {
+			udp_header = udp_hdr(skb);
+			if (!udp_header) {
+				return -1; // Failed to get UDP header
+			}
+			src_in6->sin6_port = udp_header->source;
+			dest_in6->sin6_port = udp_header->dest;
+		} else {
+			return -1; // Unsupported protocol
+		}
+	}
+#endif
+
+	else {
+		return -1; // Unsupported packet type
+	}
+
+	return 0; // Success
+}
+
 static void wg_receive_handshake_packet(struct wg_device *wg,
 					struct sk_buff *skb)
 {
@@ -101,6 +251,13 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 	bool packet_needs_cookie;
 	bool under_load;
 
+	if (wg->transport == WG_TRANSPORT_UDP) { 
+		// For TCP, skip cookie check
+		packet_needs_cookie = true;
+		goto nocookie;
+	}
+
+	// Handle handshake cookie response
 	if (SKB_TYPE_LE32(skb) == cpu_to_le32(MESSAGE_HANDSHAKE_COOKIE)) {
 		net_dbg_skb_ratelimited("%s: Receiving cookie response from %pISpfsc\n",
 					wg->dev->name, skb);
@@ -109,6 +266,7 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 		return;
 	}
 
+	// Load calculation to decide if system is under load
 	under_load = atomic_read(&wg->handshake_queue_len) >=
 			MAX_QUEUED_INCOMING_HANDSHAKES / 8;
 	if (under_load) {
@@ -118,19 +276,26 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 		if (!under_load)
 			last_under_load = 0;
 	}
+
+	// Validate packet's MAC and set packet_needs_cookie flag
 	mac_state = wg_cookie_validate_packet(&wg->cookie_checker, skb,
 					      under_load);
+
+	// Determine if a cookie is needed based on load and MAC state
 	if ((under_load && mac_state == VALID_MAC_WITH_COOKIE) ||
 	    (!under_load && mac_state == VALID_MAC_BUT_NO_COOKIE)) {
 		packet_needs_cookie = false;
 	} else if (under_load && mac_state == VALID_MAC_BUT_NO_COOKIE) {
-		packet_needs_cookie = true;
+		if (wg->transport == WG_TRANSPORT_UDP)
+			packet_needs_cookie = true;
 	} else {
 		net_dbg_skb_ratelimited("%s: Invalid MAC of handshake, dropping packet from %pISpfsc\n",
 					wg->dev->name, skb);
 		return;
 	}
 
+nocookie:
+	// Process handshake packets
 	switch (SKB_TYPE_LE32(skb)) {
 	case cpu_to_le32(MESSAGE_HANDSHAKE_INITIATION): {
 		struct message_handshake_initiation *message =
@@ -141,6 +306,8 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 							message->sender_index);
 			return;
 		}
+
+		// Handle handshake initiation
 		peer = wg_noise_handshake_consume_initiation(message, wg);
 		if (unlikely(!peer)) {
 			net_dbg_skb_ratelimited("%s: Invalid handshake initiation from %pISpfsc\n",
@@ -163,37 +330,129 @@ static void wg_receive_handshake_packet(struct wg_device *wg,
 							message->sender_index);
 			return;
 		}
+
+		// Handle handshake response
 		peer = wg_noise_handshake_consume_response(message, wg);
 		if (unlikely(!peer)) {
 			net_dbg_skb_ratelimited("%s: Invalid handshake response from %pISpfsc\n",
 						wg->dev->name, skb);
 			return;
 		}
+
+		// TCP-specific endpoint handling
+		struct endpoint ep = peer->endpoint;
+		struct wg_tcp_socket_list_entry *socket_iter;
+		bool found = false;
 		wg_socket_set_peer_endpoint_from_skb(peer, skb);
+
+		if (!endpoint_eq(&peer->endpoint, &ep) && peer->device->transport == WG_TRANSPORT_TCP) {
+			
+			if (!skb->sk) {
+				pr_err("Wireguard: no sock for skb=%p, peer=%p\n", skb, peer);
+				return;
+			}
+
+			if (skb->sk->sk_socket) {
+					
+				if ((skb->sk->sk_socket != peer->inbound_socket) &&
+					(skb->sk->sk_socket != peer->outbound_socket)) {
+
+					if (list_empty(&peer->device->tcp_connection_list)) {
+						pr_err("Wireguard: tcp_connection_list is empty\n");
+					} else {
+						found = false;
+						list_for_each_entry_rcu(socket_iter, &peer->device->tcp_connection_list, tcp_connection_ll) {
+						// Defensive checks to ensure all relevant fields are populated
+							if (!socket_iter)
+								continue;
+							if (!socket_iter->tcp_socket || !socket_iter->tcp_socket->sk)
+								continue;
+							if (!socket_iter->temp_peer || IS_ERR(socket_iter->temp_peer))
+								continue;
+
+							if (endpoint_eq(&peer->endpoint, &socket_iter->temp_peer->endpoint)) {
+								found = true;
+								break;
+							}
+						}
+
+						if (found) {
+							spin_lock_bh(&peer->device->tcp_connection_list_lock);
+							list_del_rcu(&socket_iter->tcp_connection_ll);
+							spin_unlock_bh(&peer->device->tcp_connection_list_lock);
+							synchronize_rcu();
+
+							spin_lock(&peer->tcp_lock);
+							if (socket_iter->temp_peer->tcp_inbound_callbacks_set) {
+								peer->original_inbound_state_change = socket_iter->temp_peer->original_inbound_state_change;
+								peer->original_inbound_write_space = socket_iter->temp_peer->original_inbound_write_space;
+								peer->original_inbound_data_ready = socket_iter->temp_peer->original_inbound_data_ready;
+								peer->tcp_inbound_callbacks_set = true;
+							}
+							peer->inbound_connected = true;
+							peer->inbound_timestamp = ktime_get();
+							peer->tcp_established = true;
+							peer->tcp_inbound_remove_scheduled = false;
+							peer->clean_inbound = false;
+							peer->clean_outbound = true;
+							spin_unlock(&peer->tcp_lock);	
+
+							// Clean up temporary peer structure
+							wg_clean_peer_socket(socket_iter->temp_peer, false, false, false);
+							if (!IS_ERR(socket_iter->temp_peer) && socket_iter->temp_peer)
+								kfree(socket_iter->temp_peer);
+
+							write_lock_bh(&peer->endpoint_lock);
+							peer->tcp_reply_endpoint = socket_iter->temp_peer->endpoint;
+							peer->peer_socket = skb->sk->sk_socket;
+							peer->inbound_socket = skb->sk->sk_socket;
+							extract_sockaddr_from_skb(skb, &peer->inbound_source, &peer->inbound_dest);
+							((struct wg_socket_data *)(peer->peer_socket->sk->sk_user_data))->peer = peer;
+							write_unlock_bh(&peer->endpoint_lock);
+
+							kfree(socket_iter);
+						} else
+							pr_err("Wireguard: TCP connection not found in pending list\n");
+					}	
+				}
+			}
+		}
+	}
+
+	// Update timestamps and session handling for TCP
+	if (peer->device->transport == WG_TRANSPORT_TCP && skb->sk) {
+		write_lock_bh(&peer->endpoint_lock);
+		peer->peer_socket = skb->sk->sk_socket;
+		write_unlock_bh(&peer->endpoint_lock);
+		if(skb->sk->sk_socket) {
+			if (skb->sk->sk_socket == peer->inbound_socket) {
+				peer->inbound_timestamp = ktime_get();
+			} else {
+				peer->outbound_timestamp = ktime_get();
+			}
+		}
+	}
 		net_dbg_ratelimited("%s: Receiving handshake response from peer %llu (%pISpfsc)\n",
-				    wg->dev->name, peer->internal_id,
-				    &peer->endpoint.addr);
-		if (wg_noise_handshake_begin_session(&peer->handshake,
-						     &peer->keypairs)) {
+				    wg->dev->name, peer->internal_id, &peer->endpoint.addr);
+
+		if (wg_noise_handshake_begin_session(&peer->handshake, &peer->keypairs)) {
 			wg_timers_session_derived(peer);
 			wg_timers_handshake_complete(peer);
-			/* Calling this function will either send any existing
-			 * packets in the queue and not send a keepalive, which
-			 * is the best case, Or, if there's nothing in the
-			 * queue, it will send a keepalive, in order to give
-			 * immediate confirmation of the session.
-			 */
 			wg_packet_send_keepalive(peer);
 		}
 		break;
-	}
+
+	default:
+		break;
 	}
 
+	// Final check to ensure peer is valid
 	if (unlikely(!peer)) {
-		WARN(1, "Somehow a wrong type of packet wound up in the handshake queue!\n");
+		WARN(1, "Unexpected state: No valid peer found after handshake processing\n");
 		return;
 	}
 
+	// Update statistics and state
 	local_bh_disable();
 	update_rx_stats(peer, skb->len);
 	local_bh_enable();
@@ -340,6 +599,9 @@ static void wg_packet_consume_data_done(struct wg_peer *peer,
 	unsigned int len, len_before_trim;
 	struct wg_peer *routed_peer;
 
+	if (unlikely(!endpoint))
+    		return;
+	
 	wg_socket_set_peer_endpoint(peer, endpoint);
 
 	if (unlikely(wg_noise_received_with_keypair(&peer->keypairs,
@@ -398,6 +660,11 @@ static void wg_packet_consume_data_done(struct wg_peer *peer,
 	if (unlikely(len > skb->len))
 		goto dishonest_packet_size;
 	len_before_trim = skb->len;
+
+
+	if (unlikely(len == 0 || len_before_trim == 0))
+    		return;
+	
 	if (unlikely(pskb_trim(skb, len)))
 		goto packet_processed;
 
diff --git a/wireguard-linux/drivers/net/wireguard/socket.c b/wireguard-linux/drivers/net/wireguard/socket.c
index 0414d7a6ce74..e9743cf9cf36 100644
--- a/wireguard-linux/drivers/net/wireguard/socket.c
+++ b/wireguard-linux/drivers/net/wireguard/socket.c
@@ -1,25 +1,103 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved.
  */
 
+
 #include "device.h"
 #include "peer.h"
 #include "socket.h"
 #include "queueing.h"
 #include "messages.h"
 
+#include <asm/byteorder.h> // For ntohl
 #include <linux/ctype.h>
-#include <linux/net.h>
 #include <linux/if_vlan.h>
 #include <linux/if_ether.h>
 #include <linux/inetdevice.h>
+#include <linux/wireguard.h>
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/net.h>
+#include <linux/tcp.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include <linux/socket.h>
+#include <linux/in6.h>
+#include <net/checksum.h>
 #include <net/udp_tunnel.h>
 #include <net/ipv6.h>
+#include <net/sock.h>
+#include <net/udp.h>
+#include <net/inet_sock.h>
+#include <net/inet_common.h>
+
+
+
+struct wg_tcp_socket_list_entry {
+    struct socket *tcp_socket;        // Socket associated with the connection
+    struct sockaddr_storage src_addr; // Source address for the connection
+    struct wg_peer *temp_peer;	      // temporary peer for dataready
+    struct list_head tcp_connection_ll;  // List pointer for the linked list
+    ktime_t timestamp;                // Timestamp when the connection was added
+};
+
+struct wg_socket_data {
+	struct wg_device *device;
+	struct wg_peer *peer;
+	bool inbound;
+};
+
+// Global structure to hold default network interface information
+struct default_interface_info {
+    struct net_device *dev;   // Default network interface
+    __be32 ipv4_address;      // IPv4 address of the default interface
+    struct in6_addr ipv6_address; // IPv6 address of the default interface
+    bool ipv4_available;      // Flag indicating if IPv4 is available
+    bool ipv6_available;      // Flag indicating if IPv6 is available
+};
+
+extern struct default_interface_info default_iface_info;
+
+void wg_setup_tcp_socket_callbacks(struct wg_peer *peer, bool inbound);
+void wg_reset_tcp_socket_callbacks(struct wg_peer *peer, bool inbound);
+void wg_get_endpoint_from_socket(struct socket *epsocket, struct endpoint *ep);
+
+
+// Function to create and return an endpoint from source and destination sockaddr_in
+struct endpoint create_endpoint(const struct sockaddr_in *source, const struct sockaddr_in *destination) {
+    struct endpoint ep;
+
+    // Initialize the endpoint to zero
+    memset(&ep, 0, sizeof(struct endpoint));
+
+    // Set the address family to AF_INET
+    ep.addr.sa_family = AF_INET;
+
+    // Copy the destination address to the endpoint's addr4 field
+    ep.addr4.sin_family = AF_INET;
+    ep.addr4.sin_port = destination->sin_port;       // Destination port (network byte order)
+    ep.addr4.sin_addr = destination->sin_addr;       // Destination IP address
+
+    // Copy the source address to the endpoint's source fields
+    ep.src4 = source->sin_addr;                      // Source IP address
+    // ep.src_if4 can be set here if you have an interface index
+
+    return ep; // Return the populated endpoint structure
+}
+
 
 static int send4(struct wg_device *wg, struct sk_buff *skb,
 		 struct endpoint *endpoint, u8 ds, struct dst_cache *cache)
 {
+
+
 	struct flowi4 fl = {
 		.saddr = endpoint->src4.s_addr,
 		.daddr = endpoint->addr4.sin_addr.s_addr,
@@ -98,6 +176,8 @@ static int send6(struct wg_device *wg, struct sk_buff *skb,
 		 struct endpoint *endpoint, u8 ds, struct dst_cache *cache)
 {
 #if IS_ENABLED(CONFIG_IPV6)
+
+
 	struct flowi6 fl = {
 		.saddr = endpoint->src6,
 		.daddr = endpoint->addr6.sin6_addr,
@@ -165,39 +245,136 @@ static int send6(struct wg_device *wg, struct sk_buff *skb,
 #endif
 }
 
+void wg_tcp_transfer_worker(struct work_struct *work);
+
+void wg_tcp_transfer_worker(struct work_struct *work)
+{
+
+	// Validate the work structure
+	if (!work) {
+		return;
+	}
+
+	// Get the wg_tcp_transfer_work container
+	struct wg_tcp_transfer_work *work_item = container_of(work, struct wg_tcp_transfer_work, work);
+
+	// Validate the work_item structure
+	if (!work_item) {
+		return;
+	}
+
+	// Extract peer and skb from the work item
+	struct wg_peer *peer = work_item->peer;
+	struct sk_buff *skb = work_item->skb;
+
+	// Validate the peer structure
+	if (!peer || IS_ERR(peer)) {
+		goto out;
+	}
+	// Validate the sk_buff structure before processing
+	if (skb) {
+		// Process the packet by passing it to wg_tcp_queuepkt
+		wg_tcp_queuepkt(peer, skb->data, skb->len);
+	} 
+out:
+	// Free the skb if it was allocated
+	if (skb) {
+		kfree_skb(skb);
+	}
+
+	// Free the work item structure
+	if (work_item) {
+		kfree(work_item);
+	}
+
+	// Validate and update the peer's tcp_transfer_worker_scheduled flag
+	if (peer && !IS_ERR(peer)) {
+		spin_lock_bh(&peer->tcp_transfer_lock);
+		peer->tcp_transfer_worker_scheduled = false;
+		spin_unlock_bh(&peer->tcp_transfer_lock);
+	}
+}
+
+
 int wg_socket_send_skb_to_peer(struct wg_peer *peer, struct sk_buff *skb, u8 ds)
 {
 	size_t skb_len = skb->len;
 	int ret = -EAFNOSUPPORT;
 
-	read_lock_bh(&peer->endpoint_lock);
-	if (peer->endpoint.addr.sa_family == AF_INET)
-		ret = send4(peer->device, skb, &peer->endpoint, ds,
-			    &peer->endpoint_cache);
-	else if (peer->endpoint.addr.sa_family == AF_INET6)
-		ret = send6(peer->device, skb, &peer->endpoint, ds,
-			    &peer->endpoint_cache);
-	else
-		dev_kfree_skb(skb);
-	if (likely(!ret))
-		peer->tx_bytes += skb_len;
-	read_unlock_bh(&peer->endpoint_lock);
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		ret = -EINVAL;
+		goto out;
+	}
+	if (unlikely(!skb)){
+		ret = -ENOMEM;
+		goto out;
+	}
+		
+        if (peer->device->transport == WG_TRANSPORT_TCP) {
+		struct wg_tcp_transfer_work *work_item;
+
+		/* Allocate the work item */
+		work_item = kmalloc(sizeof(*work_item), GFP_ATOMIC);
+		if (unlikely(!work_item)) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		/* Initialize the work item */
+		INIT_WORK(&work_item->work, wg_tcp_transfer_worker);
+		work_item->skb = skb;
+		work_item->peer = peer;
 
+		// Check if the worker is already scheduled
+		if (!peer->tcp_transfer_worker_scheduled) {
+			peer->tcp_transfer_worker_scheduled = true;
+			queue_work(peer->tcp_transfer_wq, &work_item->work);
+		}
+		ret = 0;
+	} else {
+		read_lock_bh(&peer->endpoint_lock);	
+		if (peer->endpoint.addr.sa_family == AF_INET)
+			ret = send4(peer->device, skb, &peer->endpoint, ds,
+		    		&peer->endpoint_cache);
+		else if (peer->endpoint.addr.sa_family == AF_INET6)
+			ret = send6(peer->device, skb, &peer->endpoint, ds,
+			    	&peer->endpoint_cache);
+		else {
+			read_unlock_bh(&peer->endpoint_lock);
+			dev_kfree_skb(skb);
+			return -EAGAIN;
+		}
+		read_unlock_bh(&peer->endpoint_lock);
+	}
+	peer->tx_bytes += skb_len;
+out:
 	return ret;
+
 }
 
 int wg_socket_send_buffer_to_peer(struct wg_peer *peer, void *buffer,
 				  size_t len, u8 ds)
 {
+	int ret;
+	
 	struct sk_buff *skb = alloc_skb(len + SKB_HEADER_LEN, GFP_ATOMIC);
-
-	if (unlikely(!skb))
-		return -ENOMEM;
-
+	
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))) {
+		ret = -EINVAL;
+		goto out;
+	}
+	if (unlikely(!skb)){
+		ret = -ENOMEM;
+		goto out;
+	}
+	
 	skb_reserve(skb, SKB_HEADER_LEN);
 	skb_set_inner_network_header(skb, 0);
 	skb_put_data(skb, buffer, len);
-	return wg_socket_send_skb_to_peer(peer, skb, ds);
+	ret = wg_socket_send_skb_to_peer(peer, skb, ds);
+
+out:
+	return ret;
 }
 
 int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg,
@@ -232,8 +409,8 @@ int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg,
 	return ret;
 }
 
-int wg_socket_endpoint_from_skb(struct endpoint *endpoint,
-				const struct sk_buff *skb)
+
+int wg_socket_endpoint_from_skb(struct endpoint *endpoint, const struct sk_buff *skb)
 {
 	memset(endpoint, 0, sizeof(*endpoint));
 	if (skb->protocol == htons(ETH_P_IP)) {
@@ -246,16 +423,16 @@ int wg_socket_endpoint_from_skb(struct endpoint *endpoint,
 		endpoint->addr6.sin6_family = AF_INET6;
 		endpoint->addr6.sin6_port = udp_hdr(skb)->source;
 		endpoint->addr6.sin6_addr = ipv6_hdr(skb)->saddr;
-		endpoint->addr6.sin6_scope_id = ipv6_iface_scope_id(
-			&ipv6_hdr(skb)->saddr, skb->skb_iif);
+		endpoint->addr6.sin6_scope_id = ipv6_iface_scope_id(&ipv6_hdr(skb)->saddr, skb->skb_iif);
 		endpoint->src6 = ipv6_hdr(skb)->daddr;
 	} else {
 		return -EINVAL;
 	}
+
 	return 0;
 }
 
-static bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
+bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
 {
 	return (a->addr.sa_family == AF_INET && b->addr.sa_family == AF_INET &&
 		a->addr4.sin_port == b->addr4.sin_port &&
@@ -270,30 +447,59 @@ static bool endpoint_eq(const struct endpoint *a, const struct endpoint *b)
 	       unlikely(!a->addr.sa_family && !b->addr.sa_family);
 }
 
-void wg_socket_set_peer_endpoint(struct wg_peer *peer,
-				 const struct endpoint *endpoint)
+static void wg_release_peer_tcp_connection(struct wg_peer *peer);
+
+void wg_socket_set_peer_endpoint(struct wg_peer *peer, const struct endpoint *endpoint)
 {
+	char addr_str[INET6_ADDRSTRLEN];
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		return;
+	}
+
 	/* First we check unlocked, in order to optimize, since it's pretty rare
 	 * that an endpoint will change. If we happen to be mid-write, and two
 	 * CPUs wind up writing the same thing or something slightly different,
 	 * it doesn't really matter much either.
 	 */
-	if (endpoint_eq(endpoint, &peer->endpoint))
+	if (endpoint_eq(endpoint, &peer->endpoint)) {
 		return;
-	write_lock_bh(&peer->endpoint_lock);
+	}
+	
 	if (endpoint->addr.sa_family == AF_INET) {
+		snprintf(addr_str, INET_ADDRSTRLEN, "%pI4", &endpoint->addr4.sin_addr);
+		write_lock_bh(&peer->endpoint_lock);
 		peer->endpoint.addr4 = endpoint->addr4;
 		peer->endpoint.src4 = endpoint->src4;
 		peer->endpoint.src_if4 = endpoint->src_if4;
+		write_unlock_bh(&peer->endpoint_lock);  // Unlock before making connection
 	} else if (IS_ENABLED(CONFIG_IPV6) && endpoint->addr.sa_family == AF_INET6) {
+		snprintf(addr_str, INET6_ADDRSTRLEN, "%pI6", &endpoint->addr6.sin6_addr);
+		write_lock_bh(&peer->endpoint_lock);
 		peer->endpoint.addr6 = endpoint->addr6;
 		peer->endpoint.src6 = endpoint->src6;
+		write_unlock_bh(&peer->endpoint_lock);  // Unlock before making connection
+
 	} else {
-		goto out;
+		return;
 	}
 	dst_cache_reset(&peer->endpoint_cache);
-out:
-	write_unlock_bh(&peer->endpoint_lock);
+	peer->tcp_reply_endpoint = peer->endpoint;
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		// For IPv4 address
+		peer->endpoint.addr4.sin_port = htons(peer->device->incoming_port);
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		// For IPv6 address
+		peer->endpoint.addr6.sin6_port = htons(peer->device->incoming_port);
+	} else {
+		pr_err("Unsupported address family\n");
+	}
+
+	if (!peer->peer_endpoint_set) {
+		peer->peer_endpoint = peer->endpoint;
+		peer->peer_endpoint_set;
+	}
+	if (peer->device->transport == WG_TRANSPORT_TCP && !peer->tcp_established)
+		wg_tcp_connect(peer);
 }
 
 void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer,
@@ -301,8 +507,13 @@ void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer,
 {
 	struct endpoint endpoint;
 
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		return;
+	}
+	
 	if (!wg_socket_endpoint_from_skb(&endpoint, skb))
 		wg_socket_set_peer_endpoint(peer, &endpoint);
+
 }
 
 void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer)
@@ -316,12 +527,16 @@ void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer)
 static int wg_receive(struct sock *sk, struct sk_buff *skb)
 {
 	struct wg_device *wg;
-
+	struct wg_socket_data *socket_data;
+	
 	if (unlikely(!sk))
 		goto err;
-	wg = sk->sk_user_data;
-	if (unlikely(!wg))
+	socket_data = sk->sk_user_data;
+	if (unlikely(!socket_data))
 		goto err;
+	if (unlikely(!socket_data->device))
+ 		goto err;
+	wg = socket_data->device;
 	skb_mark_not_on_list(skb);
 	wg_packet_receive(wg, skb);
 	return 0;
@@ -349,9 +564,10 @@ static void set_sock_opts(struct socket *sock)
 int wg_socket_init(struct wg_device *wg, u16 port)
 {
 	struct net *net;
+	struct wg_socket_data *socket_data;
 	int ret;
 	struct udp_tunnel_sock_cfg cfg = {
-		.sk_user_data = wg,
+		.sk_user_data = NULL,  // will set later
 		.encap_type = 1,
 		.encap_rcv = wg_receive
 	};
@@ -379,7 +595,17 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 	rcu_read_unlock();
 	if (unlikely(!net))
 		return -ENONET;
+	
+	socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+	if (!socket_data) {
+		put_net(net);
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		return -ENOMEM;
+	}
 
+ 	socket_data->device = wg;
+ 	socket_data->peer = NULL; // Set this to a valid peer where appropriate
+	
 #if IS_ENABLED(CONFIG_IPV6)
 retry:
 #endif
@@ -390,6 +616,11 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 		goto out;
 	}
 	set_sock_opts(new4);
+
+	// Set the socket data in the cfg structure
+	cfg.sk_user_data = socket_data;
+
+	// Now setup the UDP tunnel socket with the updated cfg
 	setup_udp_tunnel_sock(net, new4, &cfg);
 
 #if IS_ENABLED(CONFIG_IPV6)
@@ -405,6 +636,8 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 			goto out;
 		}
 		set_sock_opts(new6);
+
+		// Setup the IPv6 UDP tunnel socket with the same socket data
 		setup_udp_tunnel_sock(net, new6, &cfg);
 	}
 #endif
@@ -416,6 +649,7 @@ int wg_socket_init(struct wg_device *wg, u16 port)
 	return ret;
 }
 
+
 void wg_socket_reinit(struct wg_device *wg, struct sock *new4,
 		      struct sock *new6)
 {
@@ -435,3 +669,2251 @@ void wg_socket_reinit(struct wg_device *wg, struct sock *new4,
 	sock_free(old4);
 	sock_free(old6);
 }
+
+static int wg_set_socket_timeouts(struct socket *sock, unsigned long snd_timeout,
+				  unsigned long rcv_timeout)
+{
+	if (!sock || !sock->sk) {
+		pr_err("Invalid socket or sock is NULL\n");
+		return -EINVAL;
+	}
+
+	struct sock *sk = sock->sk;
+
+	sk->sk_sndtimeo = snd_timeout*30;
+	sk->sk_rcvtimeo = rcv_timeout*30;
+
+	return 0;
+}
+
+static bool wg_endpoints_match(const struct endpoint *a,
+                               const struct endpoint *b)
+{
+	// Compare endpoints
+	if (a->addr.sa_family != b->addr.sa_family) {
+		return false;
+	}
+
+	if (a->addr.sa_family == AF_INET) {
+		return a->addr4.sin_port == b->addr4.sin_port &&
+		a->addr4.sin_addr.s_addr == b->addr4.sin_addr.s_addr;
+	} else if (a->addr.sa_family == AF_INET6) {
+		// For IPv6, also compare the scope ID if the address is link-local
+		bool is_link_local_a = ipv6_addr_type(&a->addr6.sin6_addr) & IPV6_ADDR_LINKLOCAL;
+		bool is_link_local_b = ipv6_addr_type(&b->addr6.sin6_addr) & IPV6_ADDR_LINKLOCAL;
+        
+		return a->addr6.sin6_port == b->addr6.sin6_port &&
+		ipv6_addr_equal(&a->addr6.sin6_addr, &b->addr6.sin6_addr) &&
+		(!is_link_local_a || !is_link_local_b || a->addr6.sin6_scope_id == b->addr6.sin6_scope_id);
+	}
+	return false;
+}
+
+void wg_free_peer_socket_data(struct wg_peer *peer);
+
+void wg_free_peer_socket_data(struct wg_peer *peer)
+{
+	if (peer && !IS_ERR(peer))
+		if (peer->peer_socket)
+			if (peer->peer_socket->sk)
+				if (peer->peer_socket->sk->sk_user_data)
+					kfree(peer->peer_socket->sk->sk_user_data);
+}
+
+
+
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound)
+{
+	if (!peer || IS_ERR(peer)) {
+		return;
+	}
+	if ((inbound && peer->peer_socket == peer->inbound_socket) ||
+	    (!inbound && peer->peer_socket == peer->outbound_socket)) {
+		// Cleanup partial skb buffer
+		if (peer->partial_skb) {
+			kfree_skb(peer->partial_skb);
+			peer->partial_skb = NULL;
+		}
+	
+		// Cancel and flush the TCP read workqueue
+		if (peer->tcp_read_worker_scheduled) {
+			cancel_work_sync(&peer->tcp_read_work);
+			peer->tcp_read_worker_scheduled = false;
+		}
+		if (peer->tcp_read_wq && destroy) {
+			destroy_workqueue(peer->tcp_read_wq);
+			peer->tcp_read_wq = NULL;
+		}
+	
+		// Cancel and flush the TCP write workqueue
+		if (peer->tcp_write_worker_scheduled) {
+			cancel_work_sync(&peer->tcp_write_work);
+			peer->tcp_write_worker_scheduled = false;
+		}
+		if (peer->tcp_write_wq && destroy) {
+			destroy_workqueue(peer->tcp_write_wq);
+			peer->tcp_write_wq = NULL;
+		}
+	
+		// Clean up packet queues
+		if (!skb_queue_empty(&peer->send_queue))
+			skb_queue_purge(&peer->send_queue);
+	
+		// Reset TCP state
+		peer->received_len = 0;
+		peer->expected_len = 0;
+		peer->tcp_established = false;
+		peer->tcp_pending = false;
+		peer->tcp_retry_scheduled = false;
+	}
+	
+	// Determine which socket and related resources to clean based on the 'inbound' flag
+	struct socket **socket_to_clean = inbound ? &peer->inbound_socket : &peer->outbound_socket;
+	bool *callbacks_set_flag = inbound ? &peer->tcp_inbound_callbacks_set : &peer->tcp_outbound_callbacks_set;
+	bool *connected_flag = inbound ? &peer->inbound_connected : &peer->outbound_connected;
+	ktime_t *timestamp = inbound ? &peer->inbound_timestamp : &peer->outbound_timestamp;
+	struct delayed_work *remove_work = inbound ? &peer->tcp_inbound_remove_work : &peer->tcp_outbound_remove_work;
+	bool *remove_scheduled_flag = inbound ? &peer->tcp_inbound_remove_scheduled : &peer->tcp_outbound_remove_scheduled;
+
+	// Cleanup socket if necessary
+	if (*socket_to_clean) {
+		if (release) {
+			// Directly free peer socket data as per wg_free_peer_socket_data logic
+			if (*socket_to_clean && (*socket_to_clean)->sk) {
+				if ((*socket_to_clean)->sk->sk_user_data) {
+					kfree((*socket_to_clean)->sk->sk_user_data);
+					(*socket_to_clean)->sk->sk_user_data = NULL;
+				}
+			}
+			kernel_sock_shutdown(*socket_to_clean, SHUT_RDWR);
+			sock_release(*socket_to_clean);
+		}
+		*socket_to_clean = NULL;
+	}
+
+	// Reset callbacks set flag
+	*callbacks_set_flag = false;
+
+	// Reset connection status and timestamp
+	*connected_flag = false;
+	*timestamp = 0;
+
+	// Cancel and clean up remove work if scheduled
+	if (*remove_scheduled_flag) {
+		*remove_scheduled_flag = false;
+		cancel_delayed_work_sync(remove_work);
+	}
+
+	
+	// Check if a retry is scheduled and clean up
+	if (peer->tcp_retry_scheduled && !inbound) {
+		peer->tcp_retry_scheduled = false;
+		cancel_delayed_work_sync(&peer->tcp_retry_work);
+	}
+}
+
+
+struct wg_peer *wg_temp_peer_create(struct wg_device *wg);
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *receive_socket);
+
+
+// Function to copy source and destination addresses from a TCP socket
+
+
+int copy_sock_addresses(struct socket *tcp_socket, struct sockaddr_storage *inbound_source, struct sockaddr_storage *inbound_dest) {
+    struct sock *sk;
+    struct inet_sock *inet;
+    int family;
+
+    // Check if the socket is valid
+    if (!tcp_socket || !tcp_socket->sk) {
+        return -1; // Invalid socket
+    }
+
+    sk = tcp_socket->sk; // Retrieve the socket's 'sock' structure
+    family = sk->sk_family;
+
+    if (family == AF_INET) {
+        struct inet_sock *inet = inet_sk(sk);
+
+        // Validate inet_sk
+        if (!inet) {
+            return -1;
+        }
+
+        // Clear the sockaddr_storage structures
+        memset(inbound_source, 0, sizeof(struct sockaddr_storage));
+        memset(inbound_dest, 0, sizeof(struct sockaddr_storage));
+
+        // Cast to sockaddr_in for IPv4
+        struct sockaddr_in *source_in = (struct sockaddr_in *)inbound_source;
+        struct sockaddr_in *dest_in = (struct sockaddr_in *)inbound_dest;
+
+        // Set the address family to AF_INET
+        source_in->sin_family = AF_INET;
+        dest_in->sin_family = AF_INET;
+
+        // Populate the source and destination information
+        source_in->sin_addr.s_addr = inet->inet_rcv_saddr; // Local IP address
+        source_in->sin_port = inet->inet_sport;            // Local port (already in network byte order)
+        dest_in->sin_addr.s_addr = inet->inet_daddr;       // Remote IP address
+        dest_in->sin_port = inet->inet_dport;              // Remote port (already in network byte order)
+
+    } 
+#if IS_ENABLED(CONFIG_IPV6)
+    else if (family == AF_INET6) {
+        struct ipv6_pinfo *np = inet6_sk(sk);
+
+        // Validate ipv6_pinfo
+        if (!np) {
+            return -1;
+        }
+
+        // Clear the sockaddr_storage structures
+        memset(inbound_source, 0, sizeof(struct sockaddr_storage));
+        memset(inbound_dest, 0, sizeof(struct sockaddr_storage));
+
+        // Cast to sockaddr_in6 for IPv6
+        struct sockaddr_in6 *source_in6 = (struct sockaddr_in6 *)inbound_source;
+        struct sockaddr_in6 *dest_in6 = (struct sockaddr_in6 *)inbound_dest;
+
+        // Set the address family to AF_INET6
+        source_in6->sin6_family = AF_INET6;
+        dest_in6->sin6_family = AF_INET6;
+
+        // Populate the source and destination information
+        source_in6->sin6_addr = sk->sk_v6_rcv_saddr;       // Local IPv6 address
+        source_in6->sin6_port = inet->inet_sport;          // Local port (already in network byte order)
+        dest_in6->sin6_addr = sk->sk_v6_daddr;             // Remote IPv6 address
+        dest_in6->sin6_port = inet->inet_dport;            // Remote port (already in network byte order)
+        source_in6->sin6_scope_id = ipv6_iface_scope_id(&sk->sk_v6_rcv_saddr, sk->sk_bound_dev_if);
+    }
+#endif
+    else {
+        return -1; // Unsupported address family
+    }
+
+    return 0; // Success
+}
+
+struct wg_peer *wg_find_peer_by_endpoints(struct wg_device *wg, const struct endpoint *endpoint)
+{
+    struct wg_peer *peer = NULL;
+    struct wg_peer *matched_peer = NULL;
+
+    if (!wg || !endpoint) {
+        return NULL;
+    }
+
+    rcu_read_lock();
+    list_for_each_entry_rcu(peer, &wg->peer_list, peer_list) {
+        // Check if the current peer's endpoint, peer_endpoint, or tcp_reply_endpoint matches the provided endpoint
+        if (endpoint_eq(&peer->endpoint, endpoint) ||
+            endpoint_eq(&peer->peer_endpoint, endpoint) ||
+            endpoint_eq(&peer->tcp_reply_endpoint, endpoint)) {
+            matched_peer = peer;
+            break;
+        }
+    }
+    rcu_read_unlock();
+
+    return matched_peer;
+}
+
+
+int wg_tcp_listener_worker(struct wg_device *wg, struct socket *tcp_socket)
+{
+	bool found = false;
+	struct socket *new_peer_connection = NULL;
+
+	if (!tcp_socket) {
+        	pr_err("tcp_socket is NULL\n");
+        	return -EINVAL;
+    	}
+	if (wg->listener_active) {
+		pr_err("Device TCP listener is already active.");
+		return -EINVAL;
+	}
+	wg->listener_active = true;
+	while (!kthread_should_stop()) {
+		int err;
+
+		err = kernel_accept(tcp_socket, &new_peer_connection, 0);
+		if (err < 0) {
+			if (err == -EAGAIN || err == -ERESTARTSYS)
+				continue;
+			pr_err("Error accepting new connection: %d\n", err);
+        		continue;
+		}
+
+		if (!new_peer_connection) {
+			pr_err("new_peer_connection is NULL after kernel_accept\n");
+			continue;
+		}
+
+	        struct wg_peer *matched_peer = NULL;
+		struct wg_peer *new_temp_peer = NULL;
+	        struct endpoint new_endpoint;
+		struct wg_tcp_socket_list_entry *socket_iter = NULL;
+		struct wg_socket_data *socket_data = NULL;  // New structure for sk_user_data
+
+		
+		new_peer_connection->ops->getname(new_peer_connection, (struct sockaddr *)&new_endpoint, 1);
+
+		if (!list_empty(&wg->peer_list)) {
+			// search device peer list to see if inbound connection is from an established peer address.
+	      	 	rcu_read_lock();
+	       	 	list_for_each_entry_rcu(matched_peer, &wg->peer_list, peer_list) {
+				if (wg_endpoints_match(&matched_peer->endpoint, &new_endpoint)) {
+					// read data if there is any available
+					found = true;
+					break;
+	        		}
+	       		 }
+			rcu_read_unlock();
+		}
+		if (found)
+			if (!skb_queue_empty(&new_peer_connection->sk->sk_receive_queue)) {
+				wg_tcp_data_ready(new_peer_connection->sk);
+			}
+
+		
+		// if no matched_peer then this is a new connection and we need to process the packet to see if this is an esiting peer roaming.    
+		if (!matched_peer) {
+			// some kind of martian, toss it.
+			// Perform a graceful shutdown and release the socket
+			kernel_sock_shutdown(new_peer_connection, SHUT_RDWR);
+			sock_release(new_peer_connection);
+	        } else {
+			// We need to search if there is already a pendign connection and remove the old pending connection if there.
+			  // Check if the peer list is empty before proceeding
+			if (!list_empty(&wg->peer_list)) {
+				rcu_read_lock();
+				// check device pending connections in tcp_connection_list
+				list_for_each_entry_rcu(socket_iter, &wg->tcp_connection_list, tcp_connection_ll) {
+					// Defensive checks to ensure all relevant fields are populated
+					// Skip to the next entry if any critical field is NULL
+					if (!socket_iter) {
+						continue;
+					}	
+					if (!socket_iter->tcp_socket) {
+						continue;
+					}
+					if (!socket_iter->tcp_socket->sk) {
+						continue;
+					}	
+
+					if (endpoint_eq(&new_endpoint, (struct endpoint *)&socket_iter->src_addr)) {
+						found = true;
+						break;
+					}
+				}
+				rcu_read_unlock();
+			}
+			if (found) {
+				// Remove the older socket from the pending TCP connection list
+				spin_lock_bh(&wg->tcp_connection_list_lock);
+				list_del_rcu(&socket_iter->tcp_connection_ll);
+				spin_unlock_bh(&wg->tcp_connection_list_lock);
+				synchronize_rcu(); // Ensure safe removal
+				// nuke old socket
+				wg_clean_peer_socket(socket_iter->temp_peer, true, true, true);
+				// clean up old temp_peer
+				if (!IS_ERR(socket_iter->temp_peer) && socket_iter->temp_peer) {
+					kfree(socket_iter->temp_peer);
+				}
+				// Free the old entry
+				kfree(socket_iter);
+			}	
+				
+			// we have a new peer end point roaming potentially, 
+			// add to pending connection list and hand packets to upper layer for verificaiton
+	
+			new_temp_peer = wg_temp_peer_create(wg);
+			if (!IS_ERR(new_temp_peer) && new_temp_peer) {
+				// Allocate memory for the new socket data structure
+				socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+				if (!socket_data) {
+					pr_err("Failed to allocate memory for socket_data\n");
+					kernel_sock_shutdown(new_peer_connection, SHUT_RDWR);
+					sock_release(new_peer_connection);
+					continue;
+				}
+
+				// Initialize the socket data with the device and the temp peer
+				socket_data->device = wg;
+				socket_data->peer = new_temp_peer;
+				socket_data->inbound = true;
+
+				// Set the socket data as sk_user_data
+				new_peer_connection->sk->sk_user_data = socket_data;
+				
+				new_temp_peer->peer_socket = new_peer_connection;
+				new_temp_peer->inbound_socket = new_peer_connection;
+				wg_get_endpoint_from_socket(new_peer_connection, &new_temp_peer->tcp_reply_endpoint);
+				new_temp_peer->endpoint = new_temp_peer->tcp_reply_endpoint;
+				
+				new_temp_peer->tcp_established = true;
+				new_temp_peer->inbound_connected = true;
+				new_temp_peer->inbound_timestamp = ktime_get();
+				new_temp_peer->clean_inbound = false;
+				new_temp_peer->tcp_inbound_callbacks_set = false;
+				copy_sock_addresses(new_peer_connection, &new_temp_peer->inbound_source, &new_temp_peer->inbound_dest);
+				new_temp_peer->peer_endpoint = new_temp_peer->endpoint;
+				new_temp_peer->peer_endpoint_set = true;
+				// Set the port to incoming port
+				if (new_temp_peer->peer_endpoint.addr.sa_family == AF_INET) {
+        			// IPv4 address
+        				new_temp_peer->peer_endpoint.addr4.sin_port = htons(new_temp_peer->device->incoming_port);
+    				} else if (new_temp_peer->peer_endpoint.addr.sa_family == AF_INET6) {
+        			// IPv6 address
+        				new_temp_peer->peer_endpoint.addr6.sin6_port = htons(new_temp_peer->device->incoming_port);
+    				} 
+				wg_add_tcp_socket_to_list(wg, new_peer_connection);
+				//  we need to set up a data reader for pending connections
+				wg_setup_tcp_socket_callbacks(new_temp_peer, true);  // ready to read data from pending connection and hand handshake to upper layers
+				// read data if there is some pending
+				if (!skb_queue_empty(&new_peer_connection->sk->sk_receive_queue)) {
+					wg_tcp_data_ready(new_peer_connection->sk);
+				}
+			}
+		}
+	}	
+	return 0;
+}
+	
+int wg_tcp_listener4_thread(void *data)
+{
+	struct wg_device *wg = data;
+	struct socket *listen_socket;
+
+	// Check if tcp_socket4_ready is set
+	if (!wg->tcp_socket4_ready) {
+		return 0;
+	}
+	listen_socket = wg->tcp_listen_socket4;
+
+	return wg_tcp_listener_worker(wg, listen_socket);
+}
+
+int wg_tcp_listener6_thread(void *data)
+{
+	struct wg_device *wg = data;
+	struct socket *listen_socket;
+
+	if (!wg->tcp_socket6_ready) {
+		return 0;
+	}
+
+	listen_socket = wg->tcp_listen_socket6;
+
+	return wg_tcp_listener_worker(wg, listen_socket);
+}
+
+void wg_tcp_listener_socket_release(struct wg_device *wg)
+{
+
+	wg->listener_active = false;
+	// Signal listener threads to stop
+	if (wg->tcp_listener4_thread) {
+        	kthread_stop(wg->tcp_listener4_thread);
+        	wg->tcp_listener4_thread = NULL;
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+    	if (wg->tcp_listener6_thread) {
+        	kthread_stop(wg->tcp_listener6_thread);
+        	wg->tcp_listener6_thread = NULL;
+    	}
+#endif
+
+	// Release IPv4 socket
+    	if (wg->tcp_listen_socket4) {
+        	sock_release(wg->tcp_listen_socket4);
+        	wg->tcp_listen_socket4 = NULL;
+        	wg->tcp_socket4_ready = false;
+    	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+    	// Release IPv6 socket
+    	if (wg->tcp_listen_socket6) {
+        	sock_release(wg->tcp_listen_socket6);
+        	wg->tcp_listen_socket6 = NULL;
+        	wg->tcp_socket6_ready = false;
+    	}
+#endif
+}
+
+struct socket *wg_setup_tcp_listen4(struct wg_device *wg, struct net *net, u16 port)
+{
+	if (!wg || !net || port == 0) {
+		return NULL;
+	}
+
+	int ret = -EINVAL; // Initialize ret with an invalid argument error
+	struct socket *listen_socket4 = NULL;
+	struct sockaddr_in addr4 = {
+		.sin_family = AF_INET,
+		.sin_port = htons(port),
+		.sin_addr = { htonl(INADDR_ANY) }
+	};
+
+	ret = sock_create_kern(net, AF_INET, SOCK_STREAM, IPPROTO_TCP, &listen_socket4);
+	if (ret < 0) {
+		pr_err("%s: Could not create IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+
+	// Set socket options to reuse port
+	sock_set_reuseport(listen_socket4->sk);
+
+	ret = kernel_bind(listen_socket4, (struct sockaddr *)&addr4, sizeof(addr4));
+	if (ret < 0) {
+		pr_err("%s: Could not bind IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+
+	ret = kernel_listen(listen_socket4, SOMAXCONN);
+	if (ret < 0) {
+		pr_err("%s: Could not listen on IPv4 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv4;
+	}
+	goto out;
+
+
+release_ipv4:
+	if (ret < 0 && listen_socket4) {
+		sock_release(listen_socket4);
+		return NULL;
+	}
+
+out:
+	put_net(net);
+	return listen_socket4;
+}
+
+struct socket *wg_setup_tcp_listen6(struct wg_device *wg, struct net *net, u16 port)
+{
+	if (!wg || !net || port == 0) {
+		return NULL;
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	int ret = -EINVAL; // Initialize ret with an invalid argument error
+	struct socket *listen_socket6 = NULL;
+	struct sockaddr_in6 addr6 = {
+		.sin6_family = AF_INET6,
+		.sin6_port = htons(port),
+		.sin6_addr = IN6ADDR_ANY_INIT,
+	};
+	ret = sock_create_kern(net, AF_INET6, SOCK_STREAM, IPPROTO_TCP, &listen_socket6);
+	if (ret < 0) {
+		pr_err("%s: Could not create IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+
+	ret = kernel_bind(listen_socket6, (struct sockaddr *)&addr6, sizeof(addr6));
+	if (ret < 0) {
+		pr_err("%s: Could not bind IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+
+	ret = kernel_listen(listen_socket6, SOMAXCONN);
+	if (ret < 0) {
+		pr_err("%s: Could not listen on IPv6 TCP socket, error: %d\n", wg->dev->name, ret);
+		goto release_ipv6;
+	}
+	goto out;
+
+release_ipv6:
+	if (ret < 0 && listen_socket6) {
+		sock_release(listen_socket6);
+		return NULL;
+	}
+
+out:
+	put_net(net);
+	return listen_socket6;
+#endif
+}
+
+int wg_tcp_listener_socket_init(struct wg_device *wg, u16 port)
+{
+	if (!wg || port == 0) {
+		return -EINVAL;
+	}
+
+	if (wg->tcp_socket4_ready || wg->tcp_socket6_ready) {
+		return 0;
+	}
+
+	if (!wg->dev) {
+		return -EINVAL;
+	}
+	
+	struct in_device *dev_v4 = __in_dev_get_rtnl(wg->dev);
+	struct inet6_dev *dev_v6 = __in6_dev_get(wg->dev);
+	struct net *net;
+	bool ipv4_configured = false, ipv6_configured = false;
+
+	rcu_read_lock();
+	net = rcu_dereference(wg->creating_net);
+	net = net ? maybe_get_net(net) : NULL;
+	rcu_read_unlock();
+
+	if (unlikely(!net)) {
+		return -ENONET;
+	}
+
+
+
+
+	// Use the default interface info to set up the IPv4 listener
+	if (default_iface_info.ipv4_available) {
+		wg->tcp_listen_socket4 = wg_setup_tcp_listen4(wg, net, port);
+		if (wg->tcp_listen_socket4) {
+			wg->tcp_socket4_ready = true;
+			ipv4_configured = true;
+
+			// Set the device endpoint using the global structure
+			wg->device_endpoint.addr4.sin_family = AF_INET;
+			wg->device_endpoint.addr4.sin_addr.s_addr = default_iface_info.ipv4_address;
+			wg->device_endpoint.addr4.sin_port = htons(port);
+			wg->device_endpoint.src4.s_addr = default_iface_info.ipv4_address;
+			wg->device_endpoint.src_if4 = default_iface_info.dev->ifindex; // Interface index
+		}
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	// Use the default interface info to set up the IPv6 listener
+	if (default_iface_info.ipv6_available) {
+		wg->tcp_listen_socket6 = wg_setup_tcp_listen6(wg, net, port);
+		if (wg->tcp_listen_socket6) {
+			wg->tcp_socket6_ready = true;
+			ipv6_configured = true;
+
+			// Set the device endpoint using the global structure
+			wg->device_endpoint.addr6.sin6_family = AF_INET6;
+			wg->device_endpoint.addr6.sin6_addr = default_iface_info.ipv6_address;
+			wg->device_endpoint.addr6.sin6_port = htons(port);
+			wg->device_endpoint.src6 = default_iface_info.ipv6_address;
+		}
+	}
+#endif
+	
+	// Start the IPv4 listener thread if IPv4 is configured
+	if (dev_v4 && ipv4_configured && !wg->tcp_listener4_thread) {
+		wg->tcp_listener4_thread = kthread_run(wg_tcp_listener4_thread, (void *)wg, "wg_listener");
+		if (IS_ERR(wg->tcp_listener4_thread)) {
+		pr_err("Failed to establish IPv4 TCP listener thread\n");
+		} 
+	}
+
+    // Start the IPv6 listener thread if IPv6 is configured
+#if IS_ENABLED(CONFIG_IPV6)
+	if (dev_v6 && ipv6_configured && !wg->tcp_listener6_thread) {
+		wg->tcp_listener6_thread = kthread_run(wg_tcp_listener6_thread, (void *)wg, "wg_listener");
+		if (IS_ERR(wg->tcp_listener6_thread)) {
+			pr_err("Failed to establish IPv6 TCP listener thread\n");
+		} 
+	}
+#endif
+
+	// Schedule TCP cleanup work if not already scheduled
+#ifdef NOTTEST
+        if (!wg->tcp_cleanup_scheduled) {
+		spin_lock_bh(&wg->tcp_cleanup_lock);
+                wg->tcp_cleanup_scheduled = true;
+	        spin_unlock_bh(&wg->tcp_cleanup_lock);
+		schedule_delayed_work(&wg->tcp_cleanup_work, msecs_to_jiffies(5000));
+	}
+#endif
+	return 0;
+}
+
+// Attempt to establish a TCP connection
+int wg_tcp_connect(struct wg_peer *peer)
+{
+    	struct wg_socket_data *socket_data;
+	struct sockaddr_storage src_addr_storage;
+	struct sockaddr *src_addr = (struct sockaddr *)&src_addr_storage; // Correctly define src_addr pointer
+
+	// Ensure wg_tcp_listener_socket_init is called
+	if (!peer->device->tcp_socket4_ready && !peer->device->tcp_socket6_ready) {
+		int ret = wg_tcp_listener_socket_init(peer->device, peer->device->incoming_port);
+        	if (ret < 0) {
+                	return ret;
+        	}
+    	}
+
+	// Check if the connection has already been established or is pending
+	if (peer->peer_socket || peer->tcp_pending || peer->inbound_connected) {
+        	return 0;
+    	}
+
+	// Check if endpoint is properly set before attempting to connect
+	if (peer->peer_endpoint.addr.sa_family != AF_INET && peer->peer_endpoint.addr.sa_family != AF_INET6) {
+       		return -EAFNOSUPPORT;
+	}
+
+	struct sockaddr_storage addr_storage;
+    	struct sockaddr *addr = (struct sockaddr *)&addr_storage;
+	unsigned long timeout = 30 * HZ; // 5 seconds in jiffies
+	int ret;
+
+	if (peer->device->transport != WG_TRANSPORT_TCP || peer->tcp_established || peer->outbound_connected) {
+		pr_err("Invalid state for TCP connection attempt.\n");
+		return -EINVAL;
+    	}
+
+	memset(&addr_storage, 0, sizeof(addr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+        	struct sockaddr_in *addr4 = (struct sockaddr_in *)&addr_storage;
+		addr4->sin_family = AF_INET;
+		addr4->sin_port = peer->peer_endpoint.addr4.sin_port; // Use correct port from endpoint
+		addr4->sin_addr.s_addr = peer->peer_endpoint.addr4.sin_addr.s_addr;
+		addr = (struct sockaddr *)addr4;
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *addr6 = (struct sockaddr_in6 *)&addr_storage;
+		addr6->sin6_family = AF_INET6;
+		addr6->sin6_port = peer->peer_endpoint.addr6.sin6_port; // Use correct port from endpoint
+		memcpy(&addr6->sin6_addr, &peer->peer_endpoint.addr6.sin6_addr, sizeof(peer->peer_endpoint.addr6.sin6_addr));
+		addr = (struct sockaddr *)addr6;
+    	}
+#endif
+	else {
+        	pr_err("Unsupported address family: %d\n", peer->endpoint.addr.sa_family);
+		return -EAFNOSUPPORT;
+    	}
+
+	// Create the socket
+	ret = sock_create_kern(&init_net, peer->peer_endpoint.addr.sa_family, SOCK_STREAM, IPPROTO_TCP, &peer->peer_socket);
+	if (ret) {
+        	pr_err("Failed to create TCP socket for address family %d: %d\n", peer->peer_endpoint.addr.sa_family, ret);
+		return ret;
+	}
+
+	
+	// ** New code to bind the socket to the default interface's IP address **
+	memset(&src_addr_storage, 0, sizeof(src_addr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+		struct sockaddr_in *src_addr4 = (struct sockaddr_in *)&src_addr_storage;
+		src_addr4->sin_family = AF_INET;
+		src_addr4->sin_port = 0; // Let the system choose the port
+		src_addr4->sin_addr.s_addr = default_iface_info.ipv4_address; // Use the default interface's IP address
+		src_addr = (struct sockaddr *)src_addr4;
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *src_addr6 = (struct sockaddr_in6 *)&src_addr_storage;
+		src_addr6->sin6_family = AF_INET6;
+		src_addr6->sin6_port = 0; // Let the system choose the port
+		src_addr6->sin6_addr = default_iface_info.ipv6_address; // Use the default interface's IPv6 address
+		src_addr = (struct sockaddr *)src_addr6;
+	}
+ #endif
+
+
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->outbound_connected = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->outbound_timestamp = ktime_set(0, 0);
+	peer->outbound_socket = peer->peer_socket;
+
+	struct inet_sock *inet = inet_sk(peer->peer_socket->sk);
+
+	// Set up outbound source and destination using sockaddr_storage
+	memset(&peer->outbound_source, 0, sizeof(struct sockaddr_storage));
+	memset(&peer->outbound_dest, 0, sizeof(struct sockaddr_storage));
+
+	if (peer->peer_endpoint.addr.sa_family == AF_INET) {
+        	struct sockaddr_in *source = (struct sockaddr_in *)&peer->outbound_source;
+		struct sockaddr_in *dest = (struct sockaddr_in *)&peer->outbound_dest;
+
+		source->sin_family = AF_INET;
+		source->sin_port = inet->inet_sport;  // Source port from socket
+		source->sin_addr.s_addr = inet->inet_saddr; // Source IP from socket
+
+		dest->sin_family = AF_INET;
+		dest->sin_port = peer->peer_endpoint.addr4.sin_port; // Destination port from endpoint
+		dest->sin_addr = peer->peer_endpoint.addr4.sin_addr; // Destination IP from endpoint
+	}
+#ifdef CONFIG_IPV6
+	else if (peer->peer_endpoint.addr.sa_family == AF_INET6) {
+		struct sockaddr_in6 *source6 = (struct sockaddr_in6 *)&peer->outbound_source;
+		struct sockaddr_in6 *dest6 = (struct sockaddr_in6 *)&peer->outbound_dest;
+
+		source6->sin6_family = AF_INET6;
+		source6->sin6_port = inet->inet_sport;  // Source port from socket
+		memcpy(&source6->sin6_addr, &inet6_sk(peer->peer_socket->sk)->saddr, sizeof(struct in6_addr)); // Source IP from socket
+
+		dest6->sin6_family = AF_INET6;
+		dest6->sin6_port = peer->peer_endpoint.addr6.sin6_port; // Destination port from endpoint
+		memcpy(&dest6->sin6_addr, &peer->endpoint.addr6.sin6_addr, sizeof(struct in6_addr)); // Destination IP from endpoint
+    	}
+#endif
+
+	socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+	if (!socket_data) {
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		sock_release(peer->peer_socket);
+		peer->peer_socket = NULL;
+		return -ENOMEM;
+    	}
+	socket_data->device = peer->device;
+	socket_data->peer = peer;
+	socket_data->inbound = false;
+	peer->peer_socket->sk->sk_user_data = socket_data;
+
+	// Set up the socket callbacks before initiating the connect
+	wg_setup_tcp_socket_callbacks(peer, false); // set outbound callbacks
+
+	// Set socket timeouts for send and receive operations
+    	ret = wg_set_socket_timeouts(peer->peer_socket, timeout, timeout);
+    	if (ret) {
+        	pr_err("Failed to set socket timeouts: %d\n", ret);
+        	sock_release(peer->peer_socket);
+        	peer->peer_socket = NULL;
+        	return ret;
+    	}
+
+	// Initiate the non-blocking connect
+	ret = kernel_connect(peer->peer_socket, addr, addr->sa_family == AF_INET ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6), O_NONBLOCK);
+	if (ret != -EINPROGRESS && ret != 0) {
+		pr_err("TCP connection attempt failed: %d\n", ret);
+		sock_release(peer->peer_socket);
+		peer->peer_socket = NULL;
+		return ret;
+    	}
+
+	pr_info("TCP connection attempt initiated\n");
+	spin_lock_bh(&peer->tcp_lock);
+	peer->tcp_pending = true;
+	spin_unlock_bh(&peer->tcp_lock);
+
+	if (!peer->tcp_retry_scheduled) {
+		peer->tcp_retry_scheduled = true;
+		schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(10000));
+	}
+
+	return 0;
+}
+
+// Function to release and clean up an old peer TCP connection - clean the active connection
+static void wg_release_peer_tcp_connection(struct wg_peer *peer)
+{
+	bool inbound = false;
+	if (unlikely(!peer) || unlikely(IS_ERR(peer))){
+		return;
+	}
+	if (!peer->peer_socket || !(peer->tcp_established || peer->tcp_pending)){
+		return;
+	}
+	if (peer->peer_socket == peer->inbound_socket)
+		inbound = true;
+	// Reset socket callbacks and release the socket
+	wg_reset_tcp_socket_callbacks(peer, inbound);
+
+	// Perform a graceful shutdown and release the socket
+	kernel_sock_shutdown(peer->peer_socket, SHUT_RDWR);
+	sock_release(peer->peer_socket);
+	
+	// Lock to safely modify the peer's TCP connection state
+	spin_lock_bh(&peer->tcp_lock);
+	peer->peer_socket = NULL;
+	if (inbound)
+		peer->inbound_socket = NULL;
+	else
+		peer->outbound_socket = NULL;
+	// Clear TCP connection flags
+	peer->tcp_pending = true;
+	peer->tcp_established = false;
+	spin_unlock_bh(&peer->tcp_lock);
+	// flush any partial data before we switch and free the held buffer
+	if (peer->partial_skb) {
+                kfree_skb(peer->partial_skb);
+		peer->partial_skb = NULL;
+	}
+		
+	// Check if a retry is scheduled and clean up
+    	if (peer->tcp_retry_scheduled) {
+        	peer->tcp_retry_scheduled = false;
+        	cancel_delayed_work_sync(&peer->tcp_retry_work);
+	}
+
+	// Clean up packet queues
+    	skb_queue_purge(&peer->send_queue);
+}
+
+
+void wg_extract_endpoint_from_sock(struct sock *sk,
+                                   struct endpoint *endpoint)
+{
+	if (!sk || !endpoint) {
+		pr_warn("Socket or endpoint is NULL.\n");
+		return;
+	}
+	memset(endpoint, 0, sizeof(*endpoint)); // Clear the endpoint structure
+
+	if (sk->sk_family == AF_INET) {
+		// IPv4
+		struct inet_sock *inet = inet_sk(sk);
+
+		endpoint->addr4.sin_family = AF_INET;
+		endpoint->addr4.sin_port = inet->inet_dport; // Destination port
+		endpoint->addr4.sin_addr.s_addr = inet->inet_daddr; // Destination IP address
+	} else if (sk->sk_family == AF_INET6) {
+#if IS_ENABLED(CONFIG_IPV6)
+		// IPv6
+		endpoint->addr6.sin6_family = AF_INET6;
+		endpoint->addr6.sin6_port = sk->sk_dport; // Destination port
+		endpoint->addr6.sin6_addr = sk->sk_v6_daddr; // Destination IP address
+
+		if (ipv6_addr_type((struct in6_addr *)&sk->sk_v6_daddr) & IPV6_ADDR_LINKLOCAL) {
+			// The destination address is link-local; use the socket's bound device for the scope ID
+			endpoint->addr6.sin6_scope_id = sk->sk_bound_dev_if;
+		} else {
+			// Not a link-local address; no scope ID required
+			endpoint->addr6.sin6_scope_id = 0;
+		}
+	} else {
+#endif
+		pr_warn("Unsupported socket family: %d.\n", sk->sk_family);
+	}
+}
+
+
+void wg_tcp_state_change(struct sock *sk)
+{
+
+	// Check if the socket is valid
+	if (!sk || IS_ERR(sk)) {
+		pr_err("wg_tcp_state_change: Invalid socket passed to the function\n");
+		goto out;
+	}
+
+	// Retrieve the socket user data
+	struct wg_socket_data *socket_data = sk->sk_user_data;
+
+	// Check if socket_data is valid
+	if (!socket_data || IS_ERR(socket_data)) {
+		pr_err("wg_tcp_state_change: Invalid or NULL socket_data for socket %p\n", sk);
+		goto out;
+	}
+
+	// Retrieve the peer from the socket_data
+	struct wg_peer *peer = socket_data->peer;
+
+	// Check if peer is valid
+	if (!peer || IS_ERR(peer)) {
+		pr_err("wg_tcp_state_change: Invalid or NULL peer in socket_data for socket %p\n", sk);
+		goto out;
+	}
+
+
+	if (sk->sk_state == TCP_ESTABLISHED) {
+		struct tcp_sock *tp = tcp_sk(sk);
+	}
+
+	// first lets figure out if this is an inbound connect
+	
+	switch (sk->sk_state) {
+    		case TCP_ESTABLISHED:
+			if (peer->temp_peer) {
+				pr_err("Wireguard: Inbound peer connection previously established.\n");
+				break;
+			}
+			if (!peer->tcp_established && !peer->outbound_connected) {
+				peer->tcp_pending = false;
+        			peer->tcp_established = true;
+				peer->outbound_connected = true;
+				peer->outbound_timestamp = ktime_get();
+				peer->tcp_outbound_remove_scheduled = false;
+				// Check if a retry is scheduled and clean up
+    				if (peer->tcp_retry_scheduled) {
+       		 			peer->tcp_retry_scheduled = false;
+        				cancel_delayed_work_sync(&peer->tcp_retry_work);
+				}
+				peer->tcp_retry_scheduled = false;  // Clear the retry flag upon successful connection
+				pr_info("TCP connection established.\n");
+				break;
+			} else
+				pr_err("Wireguard: Outbound connection previously established.\n");
+			break;
+		case TCP_CLOSE:
+		case TCP_CLOSE_WAIT:
+		case TCP_CLOSING:
+		case TCP_FIN_WAIT1:
+		case TCP_FIN_WAIT2:
+		case TCP_LAST_ACK:
+			if (peer->tcp_established || peer->tcp_pending) {
+                		// Connection failed or closed unexpectedly
+				// first we have to figure out if this is inbound or outbound connection;
+				struct endpoint *ep;
+				bool inbound = false;
+				if (sk->sk_user_data)
+					if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+						inbound = true;
+						if (peer->tcp_established && !peer->outbound_connected)
+							peer->tcp_established = false;
+						peer->inbound_timestamp = ktime_set(0, 0);
+						peer->inbound_connected = false;
+					} else {
+						if (peer->tcp_established && !peer->inbound_connected)
+							peer->tcp_established = false;
+						peer->outbound_timestamp = ktime_set(0, 0);
+						peer->outbound_connected = false;
+						peer->tcp_pending - false;
+					}
+				else
+					pr_err("Wireguard: TCP State Change, malformed socket state user data.\n");
+                		
+				// if this is a real peer and both connections down schedule a connection retry
+				if (!peer->tcp_established && !peer->tcp_retry_scheduled && !peer->temp_peer) {
+                    			peer->tcp_retry_scheduled = true;
+                    			schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(10000));
+                		}
+				
+				// Schedule TCP socket removal work if not already scheduled	
+ 				if (inbound) {
+					if (!peer->tcp_inbound_remove_scheduled) {
+						peer->tcp_inbound_remove_scheduled = true;
+						schedule_delayed_work(&peer->tcp_inbound_remove_work, 0);
+					}
+				else
+					if (!peer->tcp_outbound_remove_scheduled) {
+						peer->tcp_outbound_remove_scheduled = true;
+						schedule_delayed_work(&peer->tcp_inbound_remove_work, 0);
+					}
+				}
+				if (!peer->inbound_connected && !peer->outbound_connected)
+					peer->tcp_established = false;
+			}
+		default:
+			break;
+    	}
+out:
+	// Call the original state change callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_state_change) {
+        		peer->original_inbound_state_change(sk);
+   		}
+	} else {
+		if (peer->original_outbound_state_change) {
+        		peer->original_outbound_state_change(sk);
+   		}
+	}
+}
+
+
+void wg_get_endpoint_from_socket(struct socket *epsocket, struct endpoint *ep)
+{
+    // Validate input parameters
+    if (!epsocket || !ep) {
+        return;
+    }
+
+    // Validate the socket's `sock` structure
+    if (!epsocket->sk) {
+        return;
+    }
+
+    struct sock *sk = epsocket->sk;
+    int family = sk->sk_family;
+
+    if (family == AF_INET) {
+        struct inet_sock *inet = inet_sk(sk);
+
+        // Validate inet_sk
+        if (!inet) {
+            return;
+        }
+
+        // Ensure that the inet_daddr and inet_dport are valid before accessing
+        if (inet->inet_daddr == 0 || inet->inet_dport == 0) {
+            return;
+        }
+
+        // Populate the endpoint with IPv4 address and port
+        ep->addr4.sin_family = AF_INET;
+        ep->addr4.sin_addr.s_addr = inet->inet_daddr; // Remote IPv4 address
+        ep->addr4.sin_port = inet->inet_dport; // Remote port
+
+        // Populate src4 fields with local information
+        ep->src4.s_addr = inet->inet_saddr; // Local IPv4 address
+        ep->src_if4 = sk->sk_bound_dev_if; // Interface index
+    }
+#if IS_ENABLED(CONFIG_IPV6)
+    else if (family == AF_INET6) {
+        struct ipv6_pinfo *np = inet6_sk(sk);
+
+        // Validate ipv6_pinfo
+        if (!np) {
+            return;
+        }
+
+        // Ensure that the IPv6 address and port are valid before accessing
+        if (ipv6_addr_any(&sk->sk_v6_daddr) || inet_sk(sk)->inet_dport == 0) {
+            return;
+        }
+
+        // Populate the endpoint with IPv6 address and port
+        ep->addr6.sin6_family = AF_INET6;
+        ep->addr6.sin6_addr = sk->sk_v6_daddr; // Remote IPv6 address
+        ep->addr6.sin6_port = inet_sk(sk)->inet_dport; // Remote port
+        ep->addr6.sin6_scope_id = ipv6_iface_scope_id(&sk->sk_v6_rcv_saddr, sk->sk_bound_dev_if);
+
+        // Populate src6 fields with local information
+        ep->src6 = sk->sk_v6_rcv_saddr; // Local IPv6 address
+    }
+#endif
+    else {
+        return;
+    }
+}
+
+int wg_tcp_queuepkt(struct wg_peer *peer, const void *data,
+                           size_t len)
+{
+
+	struct endpoint current_endpoint;
+	struct wg_tcp_socket_list_entry *socket_iter;
+	bool found = false;
+	bool inbound = false;
+
+	if (!peer || IS_ERR(peer)) {
+		return -EINVAL;
+	}	
+	if (!data || len == 0) {
+		return -EINVAL;
+	}	
+
+
+    	// Find the peer matching the endpoint, peer_endpoint, or tcp_reply_endpoint
+	peer = wg_find_peer_by_endpoints(peer->device, &peer->endpoint);
+    	if (!peer || IS_ERR(peer)) {
+        	return -ENOENT;
+    	}
+	
+	struct sk_buff *skb = alloc_skb(len + SKB_HEADER_LEN, GFP_ATOMIC);
+	if (!skb) {
+		return -ENOMEM;
+	}
+
+	skb_reserve(skb, SKB_HEADER_LEN);
+	skb_put_data(skb, data, len);
+
+	if (!peer->peer_socket) {
+		// peer connenction is down reconnect
+		if (wg_tcp_connect(peer) < 0) {
+			kfree_skb(skb);
+			return -ECONNREFUSED; // Connection attempt failed
+		}
+	}	
+
+	if (!peer->tcp_established) {
+		// peer connenction is down reconnect
+		if (wg_tcp_connect(peer) < 0) {
+			kfree_skb(skb);
+			return -ECONNREFUSED; // Connection attempt failed
+		}
+	}
+	spin_lock_bh(&peer->send_queue_lock);
+	skb_queue_tail(&peer->send_queue, skb);
+	spin_unlock_bh(&peer->send_queue_lock);
+	// Trigger sending if possible
+	if (peer->peer_socket && peer->tcp_established) {
+		if (sk_stream_is_writeable(peer->peer_socket->sk)) {
+			wg_tcp_write_space(peer->peer_socket->sk);
+		} 
+	} 	
+	return 0;
+}
+
+// Simple checksum function for TCP encapsulation header
+static __be16 wg_header_checksum(const struct wg_tcp_encap_header *hdr)
+{
+    	uint16_t checksum = 0;
+    	uint32_t length = ntohl(hdr->length); // Ensure network byte order is converted to host byte order for calculation
+
+    	// Break the length into two 16-bit halves and XOR them with the flags and type
+    	checksum ^= (length >> 16) & 0xFFFF;
+    	checksum ^= length & 0xFFFF;
+    	checksum ^= (hdr->flags << 8) | hdr->type;
+
+    	// Simple rotate to mix bits a bit more
+    	checksum = (checksum << 5) | (checksum >> (16 - 5));
+
+	// XOR the checksum with a constant to prevent trivial values like all zeros or all ones passing the checksum
+	const uint16_t constant = 0xA5A5;  // constant pattern
+	checksum ^= constant;
+	
+    	return htons(checksum); // Convert back to network byte order
+}
+
+// Function to validate the header checksum
+static bool wg_validate_header_checksum(const struct wg_tcp_encap_header *hdr)
+{
+    	return wg_header_checksum(hdr) == hdr->checksum;
+}
+
+
+static int wg_tcp_send(struct socket *sock, const void *buff, size_t len,
+		       __u8 type, __u8 flags)
+{
+	struct wg_tcp_encap_header header;
+    	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL };
+    	struct kvec vec[2];
+    	int sent;
+
+    	// Logging entry into the function
+
+    	// Prepare the header
+	header.length = htonl(len + WG_TCP_ENCAP_HDR_LEN); // Include the payload length and header length
+    	header.type = type;
+    	header.flags = flags;
+    	header.checksum = wg_header_checksum(&header); // Compute checksum for the header
+
+	// Set up the vector for the header and the payload
+    	vec[0].iov_base = &header;
+	vec[0].iov_len = WG_TCP_ENCAP_HDR_LEN;
+    	vec[1].iov_base = (void *)buff; // Cast away const
+    	vec[1].iov_len = len;
+
+	// Send the message including the header and the payload
+	return kernel_sendmsg(sock, &msg, vec, 2, WG_TCP_ENCAP_HDR_LEN + len);
+}
+
+
+void wg_tcp_write_worker(struct work_struct *work)
+{
+	
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_write_work);
+	struct sock *sk = peer->peer_socket->sk;    
+    	struct sk_buff *skb;
+    	int sent;
+
+	if (!peer) {
+		goto out;
+	}
+	if (!peer->peer_socket) {
+		goto out;
+	}
+	
+    	if (!sk_stream_is_writeable(sk)) {
+        	// Socket is not ready for writing, exit and wait for sk_write_space activation
+        	goto out;
+	}
+
+	spin_lock_bh(&peer->send_queue_lock);
+    	while ((skb = skb_peek(&peer->send_queue)) != NULL && sk_stream_is_writeable(sk)) {
+		sent = wg_tcp_send(peer->peer_socket, skb->data, skb->len, 0, 0);  // no type or flags for now
+        	if (sent > 0) {
+            		if (sent < skb->len) {
+                		// Handle partial send by trimming the skb and leaving it in the queue
+                		skb_pull(skb, sent);
+			} else {
+				// Full send successful, dequeue and free the skb
+				__skb_unlink(skb, &peer->send_queue);
+				kfree_skb(skb);
+			}
+        	} else if (sent == 0) {
+            		// Socket buffer is full, stop sending and wait for sk_write_space
+            		break;
+        	} else {
+			// An error occurred, dequeue and free the skb
+			__skb_unlink(skb, &peer->send_queue);
+            		kfree_skb(skb);
+            			break;
+        	}
+    	}
+	spin_unlock_bh(&peer->send_queue_lock);
+
+out:
+	spin_lock_bh(&peer->tcp_write_lock);
+	peer->tcp_write_worker_scheduled = false;
+	spin_unlock_bh(&peer->tcp_write_lock);
+}
+
+void wg_peer_discard_partial_read(struct wg_peer *peer);
+
+void wg_peer_discard_partial_read(struct wg_peer *peer)
+{
+	if (peer->partial_skb)
+		kfree_skb(peer->partial_skb);
+	peer->partial_skb = NULL;
+	peer->expected_len = 0;
+	peer->received_len = 0;
+}
+
+bool wg_sync_header(struct wg_peer *peer);
+
+bool wg_sync_header(struct wg_peer *peer)
+{
+	struct sk_buff *read_skb = NULL;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec vec;
+	int read_bytes;
+	bool found = false;
+	// Attempt to read as much data as available from the socket
+
+	// Now attempt to find the next valid header within the data we already have
+	if (peer->partial_skb && peer->received_len > WG_TCP_ENCAP_HDR_LEN) {
+		// If there are less then 8 bytes left, give up (there's no room for a wg_tcp_encap_header)
+		for (size_t i = 0; i <= peer->received_len - WG_TCP_ENCAP_HDR_LEN; ++i) {
+			// Attempt to validate the header starting from the current byte
+			struct wg_tcp_encap_header *potential_hdr = (struct wg_tcp_encap_header *)(peer->partial_skb->data + i);
+			if (wg_validate_header_checksum(potential_hdr)) {
+				found = true;
+				// Adjust the skb to start from the found valid header
+				skb_pull(peer->partial_skb, i);
+				peer->received_len -= i; // Update received_len to remaining data length
+				peer->expected_len = ntohl(potential_hdr->length); // Set expected length from valid header
+				goto out; // Exit as we've found a starting point
+			}		
+		}
+	}
+	// not in the existing buffer, try to read more
+	read_skb = alloc_skb(WG_MAX_PACKET_SIZE + NET_IP_ALIGN, GFP_ATOMIC); // Allocate buffer for bulk read
+	if (!read_skb) {
+		pr_err("WireGuard: Failed to allocate skb for bulk data read\n");
+		goto out;
+	}
+	skb_reserve(read_skb, NET_IP_ALIGN);
+	
+	// Perform the read operation
+	vec.iov_base = skb_put(read_skb,0); // Prepare space
+	vec.iov_len = skb_tailroom(read_skb);
+	read_bytes = kernel_recvmsg(peer->peer_socket, &msg, &vec, 1, vec.iov_len,
+						MSG_DONTWAIT );
+
+	if (read_bytes <= 0) {
+		if (read_bytes == -EAGAIN) {
+			// No more data available, exit
+			kfree_skb(read_skb);
+			goto out;
+		}
+		pr_err("WireGuard: Error receiving bulk data from socket\n");
+		kfree_skb(read_skb);
+		goto out;
+	}
+	skb_trim(read_skb, read_bytes); // Trim skb to actual size of received data
+	
+	// Now attempt to find the next valid header within the newly read data
+	for (size_t i = 0; i <= read_skb->len - WG_TCP_ENCAP_HDR_LEN; ++i) {
+		// Attempt to validate the header starting from the current byte
+		struct wg_tcp_encap_header *potential_hdr = (struct wg_tcp_encap_header *)(read_skb->data + i);
+		if (wg_validate_header_checksum(potential_hdr)) {
+			found = true;
+			// Adjust the skb to start from the found valid header
+			skb_pull(read_skb, i);
+			if (peer->partial_skb)
+					kfree_skb(peer->partial_skb);  // free discarded data buffer
+			peer->partial_skb = read_skb; // Transfer ownership of the buffer to partial_skb for further processing
+			peer->received_len = read_bytes - i; // Update received_len to remaining data length
+			peer->expected_len = ntohl(potential_hdr->length); // Set expected length from valid header
+			break; // Exit the loop as we've found a starting point
+		}		
+	}
+	if (!found){
+		wg_peer_discard_partial_read(peer);
+		kfree_skb(read_skb);
+	}
+out:
+	return found;
+}
+
+// Function to check if the given data pointer has a valid WireGuard TCP encapsulation header
+bool wg_check_potential_header_validity(struct wg_tcp_encap_header *hdr, size_t remaining_len)
+{
+
+    if (remaining_len < WG_TCP_ENCAP_HDR_LEN) {
+        return false;
+    }
+
+    // Perform checksum validation
+    return wg_header_checksum(hdr) == hdr->checksum;
+}
+
+int wg_tcp_build_fake_headers(struct sk_buff *skb, struct wg_peer *peer)
+{
+	struct ethhdr *ethh;
+	struct iphdr *iph;
+	struct udphdr *udph;
+	int payload_len;
+	int ret;
+
+	// Initialize address pointers
+	struct sockaddr_in *source = NULL;
+	struct sockaddr_in *dest = NULL;
+#if IS_ENABLED(CONFIG_IPV6)
+	struct sockaddr_in6 *source6 = NULL;
+	struct sockaddr_in6 *dest6 = NULL;
+#endif
+
+	// Determine source and destination addresses based on socket association
+	if (peer->peer_socket == peer->inbound_socket) {
+		if (peer->inbound_source.ss_family == AF_INET) {
+			source = (struct sockaddr_in *)&peer->inbound_dest;
+			dest = (struct sockaddr_in *)&peer->inbound_source;
+#if IS_ENABLED(CONFIG_IPV6)
+		} else if (peer->inbound_source.ss_family == AF_INET6) {
+			source6 = (struct sockaddr_in6 *)&peer->inbound_dest;
+			dest6 = (struct sockaddr_in6 *)&peer->inbound_source;
+#endif
+		}
+	} else {
+		if (peer->outbound_source.ss_family == AF_INET) {
+			source = (struct sockaddr_in *)&peer->outbound_dest;
+			dest = (struct sockaddr_in *)&peer->outbound_source;
+#if IS_ENABLED(CONFIG_IPV6)
+		} else if (peer->outbound_source.ss_family == AF_INET6) {
+			source6 = (struct sockaddr_in6 *)&peer->outbound_dest;
+			dest6 = (struct sockaddr_in6 *)&peer->outbound_source;
+#endif
+		}
+	}
+
+	// Check for paged data in the skb before forcibly linearizing it
+	if (skb_is_nonlinear(skb)) {
+		if (skb_linearize(skb) != 0) {
+			return -ENOMEM;
+		} else {
+			// Ensure skb sanity after skb_linearize by calling skb_reset_tail_pointer
+			skb_reset_tail_pointer(skb);
+		}
+	}
+
+	// Calculate the payload length: initial length of skb before any header is added
+	payload_len = skb->len;
+
+	// Push and reset for UDP header
+	skb_push(skb, sizeof(struct udphdr));
+	skb_reset_transport_header(skb);
+
+	// Push and reset for IP header
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		skb_push(skb, sizeof(struct iphdr));
+		skb_reset_network_header(skb);
+
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		skb_push(skb, sizeof(struct ipv6hdr));
+		skb_reset_network_header(skb);
+
+#endif
+	} else {
+		return -EAFNOSUPPORT;
+	}
+
+	// Push and reset for Ethernet header
+	skb_push(skb, sizeof(struct ethhdr));
+	skb_reset_mac_header(skb);
+
+	// Set Ethernet header (ethh) fields
+	ethh = eth_hdr(skb);
+	ethh->h_proto = htons(peer->endpoint.addr.sa_family == AF_INET ? ETH_P_IP : ETH_P_IPV6);
+
+	// Set UDP header fields
+	udph = udp_hdr(skb);
+	if (source) { // IPv4 case
+		udph->source = source->sin_port;
+		udph->dest = dest->sin_port;
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (source6) { // IPv6 case
+		udph->source = source6->sin6_port;
+		udph->dest = dest6->sin6_port;
+#endif
+	}
+	udph->len = htons(sizeof(struct udphdr) + payload_len);
+	udph->check = 0; // Checksum will be calculated later
+
+	if (peer->endpoint.addr.sa_family == AF_INET) {
+		// Fill in the IPv4 header
+		iph = ip_hdr(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tos = 0;
+		iph->tot_len = htons(sizeof(struct iphdr) + sizeof(struct udphdr) + payload_len);
+		iph->id = 0;
+		iph->frag_off = 0;
+		iph->ttl = 64;
+		iph->protocol = IPPROTO_UDP;
+		iph->check = 0;
+		iph->saddr = source->sin_addr.s_addr;
+		iph->daddr = dest->sin_addr.s_addr;
+
+		// Calculate IP checksum
+		iph->check = ip_fast_csum((u8 *)iph, iph->ihl);
+
+		// Calculate UDP checksum for IPv4
+		__wsum csum = csum_partial(udph, ntohs(udph->len), 0);
+		udph->check = htons(csum_tcpudp_magic(iph->saddr, iph->daddr, udph->len, IPPROTO_UDP, csum));
+		if (udph->check == 0)
+			udph->check = CSUM_MANGLED_0;
+
+		skb->protocol = htons(ETH_P_IP);
+#if IS_ENABLED(CONFIG_IPV6)
+	} else if (peer->endpoint.addr.sa_family == AF_INET6) {
+		struct ipv6hdr *ip6h = ipv6_hdr(skb);
+
+		// Fill in the IPv6 header
+		ip6h->version = 6;
+		ip6h->priority = 0;
+		memset(ip6h->flow_lbl, 0, sizeof(ip6h->flow_lbl));
+		ip6h->payload_len = htons(sizeof(struct udphdr) + payload_len);
+		ip6h->nexthdr = IPPROTO_UDP;
+		ip6h->hop_limit = 64;
+		ip6h->saddr = source6->sin6_addr;
+		ip6h->daddr = dest6->sin6_addr;
+
+		// Calculate UDP checksum for IPv6
+		__wsum csum = csum_partial(udph, ntohs(udph->len), 0);
+		csum = csum_partial(&ip6h->saddr, sizeof(struct in6_addr), csum);
+		csum = csum_partial(&ip6h->daddr, sizeof(struct in6_addr), csum);
+		csum = csum_add(csum, htons(ntohs(udph->len)));
+		csum = csum_add(csum, htons(IPPROTO_UDP));
+
+		udph->check = csum_fold(csum);
+		if (udph->check == 0)
+			udph->check = CSUM_MANGLED_0;
+
+		skb->protocol = htons(ETH_P_IPV6);
+#endif
+	} else {
+		return -EAFNOSUPPORT;
+	}
+
+	// Pull to reset skb->data pointer back to original payload start
+	skb_pull(skb, sizeof(struct ethhdr));
+	skb_pull(skb, (peer->endpoint.addr.sa_family == AF_INET) ? sizeof(struct iphdr) : sizeof(struct ipv6hdr));
+	skb_pull(skb, sizeof(struct udphdr));
+
+	return 0;
+}
+
+
+
+void wg_tcp_read_worker(struct work_struct *work)
+{
+	
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_read_work);
+	struct sock *sk = peer->peer_socket->sk;    
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec vec;
+	ssize_t read_bytes;
+	unsigned maxpacket;
+	struct sk_buff *new_skb = NULL;
+
+
+	if (!peer || IS_ERR(peer))
+		goto out;
+	if (!peer->peer_socket)
+		goto out;
+// XXX not sure needed	lock_sock(sk); // Lock the socket for reading
+	maxpacket = 8192;
+	while (true) {
+
+			if (!peer->partial_skb) {
+				// Allocate buffer for the maximum packet size initially, including space for ethernet, IP and UDP headers
+				new_skb = alloc_skb(maxpacket*3 + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32, GFP_ATOMIC);
+				if (!new_skb) {
+					pr_err("WireGuard: Failed to allocate skb\n");
+					break;
+				}
+				// Reserve space for headers and align the data correctly
+				skb_reserve(new_skb, NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr)  + ETH_HLEN + 32);
+
+				peer->expected_len = 0;
+				peer->partial_skb = new_skb;
+			} 
+			// Make sure we hav enough room for at least an encapsulation header
+			if (skb_tailroom(peer->partial_skb) < WG_TCP_ENCAP_HDR_LEN) {
+				// Check if the current skb has enough room; if not, reallocate a new skb with sufficient space;
+				new_skb = skb_copy_expand(peer->partial_skb, skb_headroom(peer->partial_skb), WG_MAX_PACKET_SIZE + NET_IP_ALIGN + 
+				       					sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN, GFP_ATOMIC);
+				if (!new_skb) {
+					pr_err("WireGuard: Failed to reallocate skb\n");
+					wg_peer_discard_partial_read(peer);
+					break;
+				}
+				// Free the old skb and replace it with the new one
+				kfree_skb(peer->partial_skb);
+				peer->partial_skb = new_skb;
+			} 
+			// Read as much data as fits into the skb buffer
+			vec.iov_base = peer->partial_skb->data;
+			vec.iov_len = skb_tailroom(peer->partial_skb);
+			if (vec.iov_len > 0) {
+				read_bytes = kernel_recvmsg(peer->peer_socket, &msg, &vec, 1, vec.iov_len, msg.msg_flags);
+				if (read_bytes <= 0) {
+					if (read_bytes == -EAGAIN) {
+						break; // No more data available, exit the loop
+					} else {
+						pr_err("WireGuard: Error receiving data from socket\n");
+						wg_peer_discard_partial_read(peer);
+		   				break;
+					}
+				}
+				skb_put(peer->partial_skb, read_bytes);
+				peer->received_len += read_bytes;
+			} 
+			// check header
+			if (peer->received_len >= WG_TCP_ENCAP_HDR_LEN) {
+	            		// Complete header received, validate and prepare for packet data
+				struct wg_tcp_encap_header *hdr = (struct wg_tcp_encap_header *)peer->partial_skb->data;
+				// Check header validity
+				// Use wg_validate_header_checksum as the criteria for checking header validity
+				if (!wg_check_potential_header_validity((struct wg_tcp_encap_header *)hdr, peer->received_len)) {
+					if (!wg_sync_header(peer)) {
+						wg_peer_discard_partial_read(peer);
+						break;	
+					}
+				}
+				peer->expected_len = ntohl(hdr->length);
+			} else {
+				// not enough data
+				break;
+			}
+			// If received_len is greater than expected_len (which includes WG_TCP_ENCAP_HDR_LEN),
+			// it implies there's more data potentially for another packet or part of the current
+			//packet beyond what was expected.
+			if (peer->received_len < peer->expected_len) {
+				if ((skb_tailroom(peer->partial_skb) < peer->expected_len) &&
+						  (peer->received_len < peer->expected_len)) {
+					// check if need a bigger buffer
+					struct sk_buff *resized_skb = skb_copy_expand(peer->partial_skb, 0,
+									peer->expected_len - skb_tailroom(peer->partial_skb),
+									GFP_ATOMIC);
+					if (!resized_skb) {
+						pr_err("WireGuard: Failed to resize skb\n");
+						wg_peer_discard_partial_read(peer);
+						break;
+					}
+					if (peer->partial_skb)
+						kfree_skb(peer->partial_skb);
+					peer->partial_skb = resized_skb;
+				}
+	            
+			}
+   			// Check if we've received the complete packet now
+			if (peer->received_len >= peer->expected_len && peer->received_len > WG_TCP_ENCAP_HDR_LEN) {
+
+				// Remove the encapsulation header from the skb
+				skb_pull(peer->partial_skb, WG_TCP_ENCAP_HDR_LEN);
+				peer->received_len -= WG_TCP_ENCAP_HDR_LEN;
+				peer->expected_len -= WG_TCP_ENCAP_HDR_LEN;
+				// Check if the skb has a valid length
+				if (unlikely(peer->partial_skb->len <= 0)) {
+					pr_warn("wg_receive: Dropped packet with invalid length %d\n", peer->partial_skb->len);
+					wg_peer_discard_partial_read(peer);  // Reset for the next packet
+					break;
+				}
+				// Calculate leftover data length
+				size_t leftover_len = peer->received_len - peer->expected_len;
+				struct sk_buff *leftover_skb = NULL;
+				if (leftover_len > 0) {
+					// Trim the partial_skb to exclude the leftover data
+					skb_trim(peer->partial_skb, peer->expected_len);
+					leftover_skb = alloc_skb(maxpacket*3 + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32, GFP_ATOMIC);
+					if (!leftover_skb) {
+						pr_err("WireGuard: Failed to allocate skb\n");
+						break;
+					}
+					// Reserve space for headers and align the data correctly
+					skb_reserve(new_skb, NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr)  + ETH_HLEN + 32);
+
+					// Error checking: Verify the buffer pointers are set correctly
+#ifdef BROKENCHECK
+					if (unlikely(new_skb->data != new_skb->head + NET_IP_ALIGN + sizeof(struct iphdr) + sizeof(struct udphdr) + ETH_HLEN + 32)) {
+    						kfree_skb(new_skb);
+						new_skb = NULL;
+    						wg_peer_discard_partial_read(peer);
+    						return;
+					}
+#endif // BROKENCHECK
+	
+					// Use skb_copy_bits to copy data from the end of partial_skb to the new leftover_skb
+					if (skb_copy_bits(peer->partial_skb, peer->expected_len, leftover_skb->data, leftover_len) < 0) {
+						pr_err("wg_tcp_read_worker: Failed to copy leftover data.\n");
+						kfree_skb(leftover_skb);
+						leftover_skb = NULL;
+						wg_peer_discard_partial_read(peer);
+						break;
+					}
+
+					skb_set_tail_pointer(leftover_skb, leftover_len);
+				}
+				skb_set_tail_pointer(peer->partial_skb, peer->expected_len); // should be redundant
+				// Build the UDP and IP headers
+				if (wg_tcp_build_fake_headers(peer->partial_skb, peer)) {
+					pr_err("WireGuard: Failed to build UDP/IP headers\n");
+					wg_peer_discard_partial_read(peer);
+					break;
+				}
+				// Process the complete packet
+				wg_receive(sk, peer->partial_skb); // wg_receive consumes the skb
+
+				peer->partial_skb = NULL;  // wg_receive ate the data skb
+				if (leftover_len > 0) {
+					// Store the leftover skb (if any) in peer->partial_skb
+					peer->partial_skb = leftover_skb;
+					peer->received_len = leftover_len;
+
+				} else
+					peer->received_len = 0;
+				peer->expected_len = 0; // Reset for the next packet
+			}
+	}
+// XXX not sure needed	release_sock(sk); // Unlock the socket
+	
+out:
+	// Reset the flag to indicate the worker has finished processing
+	spin_lock_bh(&peer->tcp_read_lock);
+	peer->tcp_read_worker_scheduled = false;
+	spin_unlock_bh(&peer->tcp_read_lock);
+}
+
+void wg_tcp_data_ready(struct sock *sk)
+{
+	
+	// Ensure the socket is valid
+	if (!sk || IS_ERR(sk)) {
+		goto out;
+	}
+
+	// Retrieve the socket user data
+	struct wg_socket_data *socket_data = sk->sk_user_data;
+
+	// Check if socket_data is valid
+	if (!socket_data || IS_ERR(socket_data)) {
+		goto out;
+	}
+
+	// Retrieve the peer from the socket_data
+	struct wg_peer *peer = socket_data->peer;
+
+	// Check if peer is valid
+	if (!peer || IS_ERR(peer)) {
+		goto out;
+	}
+
+	spin_lock_bh(&peer->tcp_read_lock);
+
+	// Check if the worker is already scheduled
+	if (!peer->tcp_read_worker_scheduled) {
+        	peer->tcp_read_worker_scheduled = true;
+		queue_work(peer->tcp_read_wq, &peer->tcp_read_work);
+	}
+
+	spin_unlock_bh(&peer->tcp_read_lock);
+
+out:	
+    	// Call the original data_ready callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_data_ready) {
+        		peer->original_inbound_data_ready(sk);
+   		}
+	} else {
+		if (peer->original_outbound_data_ready) {
+        		peer->original_outbound_data_ready(sk);
+   		}
+	}
+}
+
+void wg_tcp_write_space(struct sock *sk)
+{
+	struct wg_peer *peer;
+	struct wg_socket_data *socket_data;
+	if (!sk)
+		goto out;
+	socket_data = sk->sk_user_data;
+	if (!socket_data || IS_ERR(socket_data))
+		goto out;
+	peer = socket_data->peer;
+	if (!peer || IS_ERR(peer)) {
+		goto out;
+	}
+	if (!peer->tcp_write_wq) {
+		goto out;
+	}
+	
+	spin_lock_bh(&peer->tcp_write_lock);
+
+	// Check if the worker is already scheduled
+	if (!peer->tcp_write_worker_scheduled) {
+        	peer->tcp_write_worker_scheduled = true;
+		queue_work(peer->tcp_write_wq, &peer->tcp_write_work);
+	}
+
+	spin_unlock_bh(&peer->tcp_write_lock);
+out:
+    	// Call the original write_space callback if it exists
+	if (((struct wg_socket_data *)sk->sk_user_data)->inbound) {
+	    	if (peer->original_inbound_write_space) {
+        		peer->original_inbound_write_space(sk);
+   		}
+	} else {
+		if (peer->original_outbound_write_space) {
+        		peer->original_outbound_write_space(sk);
+   		}
+	}
+}
+
+void wg_setup_tcp_socket_callbacks(struct wg_peer *peer, bool inbound)
+{
+	if (!peer || IS_ERR(peer)) {
+		return;
+	}
+	struct socket *target_socket = inbound ? peer->inbound_socket : peer->outbound_socket;
+
+	if (!target_socket || (inbound ? peer->tcp_inbound_callbacks_set : peer->tcp_outbound_callbacks_set)) {
+		return;
+	}
+
+	struct sock *sk = target_socket->sk;
+	struct wg_socket_data *socket_data;
+
+	if (inbound)
+		peer->tcp_inbound_callbacks_set = true;
+	else
+		peer->tcp_outbound_callbacks_set = true;
+
+	// Acquire lock to safely modify socket callbacks
+	write_lock_bh(&sk->sk_callback_lock);
+
+	// Check if sk_user_data is already allocated
+	socket_data = sk->sk_user_data;
+	if (socket_data) {
+		// If already allocated, update the peer
+		socket_data->device = peer->device;
+		socket_data->peer = peer;
+	} else {
+		// Allocate memory for wg_socket_data
+		socket_data = kzalloc(sizeof(*socket_data), GFP_KERNEL);
+		if (!socket_data) {
+			write_unlock_bh(&sk->sk_callback_lock);
+			return;
+		}
+
+		// Initialize wg_socket_data with device and peer
+		socket_data->device = peer->device;
+		socket_data->peer = peer;
+		socket_data->inbound = inbound;
+
+		// Set sk_user_data to the newly allocated socket_data
+		sk->sk_user_data = socket_data;
+	}
+
+	// Save the original callbacks based on the direction (inbound or outbound)
+	if (inbound) {
+		peer->original_inbound_state_change = sk->sk_state_change;
+		peer->original_inbound_write_space = sk->sk_write_space;
+		peer->original_inbound_data_ready = sk->sk_data_ready;
+	} else {
+		peer->original_outbound_state_change = sk->sk_state_change;
+		peer->original_outbound_write_space = sk->sk_write_space;
+		peer->original_outbound_data_ready = sk->sk_data_ready;
+	}
+
+	// Assign new callbacks and pass `peer` as user data for callback functions
+	sk->sk_state_change = wg_tcp_state_change;
+	sk->sk_write_space = wg_tcp_write_space;
+	sk->sk_data_ready = wg_tcp_data_ready;
+
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+void wg_reset_tcp_socket_callbacks(struct wg_peer *peer, bool inbound)
+{
+	struct sock *sk;
+	struct socket *target_socket = inbound ? peer->inbound_socket : peer->outbound_socket;
+
+	if (!peer || IS_ERR(peer)) {
+		return;
+	}
+	if (!target_socket || (inbound ? !peer->tcp_inbound_callbacks_set : !peer->tcp_outbound_callbacks_set)) {
+		return;
+	}
+
+	if (inbound)
+		peer->tcp_inbound_callbacks_set = false;
+	else
+		peer->tcp_outbound_callbacks_set = false;
+
+	sk = target_socket->sk;
+
+	// Lock the socket to safely update callback pointers
+	write_lock_bh(&sk->sk_callback_lock);
+
+	// Check if we previously saved original callbacks and restore them
+	if (inbound) {
+		if (peer->original_inbound_state_change) {
+			sk->sk_state_change = peer->original_inbound_state_change;
+			peer->original_inbound_state_change = NULL;
+		}
+		if (peer->original_inbound_write_space) {
+			sk->sk_write_space = peer->original_inbound_write_space;
+			peer->original_inbound_write_space = NULL;
+		}
+		if (peer->original_inbound_data_ready) {
+			sk->sk_data_ready = peer->original_inbound_data_ready;
+			peer->original_inbound_data_ready = NULL;
+		}
+	} else {
+		if (peer->original_outbound_state_change) {
+			sk->sk_state_change = peer->original_outbound_state_change;
+			peer->original_outbound_state_change = NULL;
+		}
+		if (peer->original_outbound_write_space) {
+			sk->sk_write_space = peer->original_outbound_write_space;
+			peer->original_outbound_write_space = NULL;
+		}
+		if (peer->original_outbound_data_ready) {
+			sk->sk_data_ready = peer->original_outbound_data_ready;
+			peer->original_outbound_data_ready = NULL;
+		}
+	}
+
+	// Clear the user data to avoid any dangling references
+	if (sk->sk_user_data)
+		kfree(sk->sk_user_data);
+	sk->sk_user_data = NULL;
+
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+void wg_tcp_retry_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_retry_work.work);
+
+
+	if (peer->tcp_established == false) {
+		if (peer->tcp_pending) {
+			// Check the state of the socket
+			struct sock *sk = peer->outbound_socket->sk;
+			// Connection still pending, perform cleanup
+
+			// Reset connection state
+			peer->tcp_pending = false;
+			peer->tcp_established = false;
+			wg_reset_tcp_socket_callbacks(peer, false);
+			// Release the socket
+			if (peer->peer_socket == peer->outbound_socket)
+				peer->peer_socket = NULL;
+			if (peer->outbound_socket) {
+				if (peer->outbound_socket->sk->sk_user_data) {
+					kfree(peer->outbound_socket->sk->sk_user_data);
+					peer->peer_socket->sk->sk_user_data = NULL;
+				}
+				sock_release(peer->outbound_socket);
+				peer->outbound_socket = NULL;
+			}
+		}
+	}
+		
+	int ret = wg_tcp_connect(peer);
+	if (ret < 0) {
+		// Reschedule the work if the connection attempt fails
+		schedule_delayed_work(&peer->tcp_retry_work, msecs_to_jiffies(30*HZ));
+		peer->tcp_retry_scheduled = true;
+	} else {
+		peer->tcp_retry_scheduled = false;
+	}
+
+}
+
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *receive_socket)
+{
+	struct wg_tcp_socket_list_entry *entry;
+	struct sockaddr_storage addr;
+
+	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry) {
+		pr_err("Failed to allocate wg_tcp_socket_list_entry\n");
+        	return;
+    	}
+
+    	entry->tcp_socket = receive_socket;
+    	entry->timestamp = ktime_get();
+
+    	// Initialize addr structure to zero
+    	memset(&addr, 0, sizeof(addr));
+
+    	// Get the source address from the socket
+    	if (receive_socket->ops->getname(receive_socket, (struct sockaddr *)&addr,  1) < 0) {
+        	pr_err("Failed to get peer address from socket\n");
+        	kfree(entry);
+        	return;
+    	}
+
+    	// Copy the obtained address to the entry's src_addr
+    	memcpy(&entry->src_addr, &addr, sizeof(addr));
+	
+	spin_lock_bh(&wg->tcp_connection_list_lock);
+    	// Add the entry to the tcp_connection_list
+    	list_add_tail_rcu(&entry->tcp_connection_ll, &wg->tcp_connection_list);
+	spin_unlock_bh(&wg->tcp_connection_list_lock);
+
+}
+
+void wg_remove_from_tcp_connection_list(struct wg_device *wg, struct socket *pending_socket)
+{
+	struct wg_tcp_socket_list_entry *entry;
+
+	if (!pending_socket)
+		return;
+
+
+	// Check if the connection list is empty
+	if (list_empty(&wg->tcp_connection_list)) {
+		return;
+	}
+	
+    	list_for_each_entry_rcu(entry, &wg->tcp_connection_list, tcp_connection_ll) {
+        	if (entry->tcp_socket == pending_socket) {
+			spin_lock_bh(&wg->tcp_connection_list_lock);
+            		list_del_rcu(&entry->tcp_connection_ll);
+			spin_unlock_bh(&wg->tcp_connection_list_lock);
+            		synchronize_rcu();
+            		if (entry->tcp_socket) {
+				kernel_sock_shutdown(entry->tcp_socket, SHUT_RDWR);
+				sock_release(entry->tcp_socket);
+           		}
+			// clean up old temp_peer
+			if (!IS_ERR(entry->temp_peer) && entry->temp_peer) {
+				// flush any partial data free the held buffer
+				if (entry->temp_peer->partial_skb) {
+        	        		kfree_skb(entry->temp_peer->partial_skb);
+				}
+				// Clean up packet queues
+				skb_queue_purge(&entry->temp_peer->send_queue);
+				
+				// Check if the TCP read work is scheduled before canceling it
+    				if (entry->temp_peer->tcp_read_worker_scheduled) {
+        				cancel_work_sync(&entry->temp_peer->tcp_read_work);
+   			     		entry->temp_peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+    				}
+
+    				// Destroy the TCP read workqueue if it exists
+    				if (entry->temp_peer->tcp_read_wq) {
+       			 		destroy_workqueue(entry->temp_peer->tcp_read_wq);
+        				entry->temp_peer->tcp_read_wq = NULL; // Avoid dangling pointers
+		    		}
+
+
+				kfree(entry->temp_peer);
+			}
+			// Free the old entry
+            		kfree(entry);
+            		break;
+        	}
+    	}
+}
+
+void wg_tcp_outbound_remove_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_outbound_remove_work.work);
+
+	wg_reset_tcp_socket_callbacks(peer, false);
+	wg_clean_peer_socket(peer, true, false, false); // clean and release
+}
+
+void wg_tcp_inbound_remove_worker(struct work_struct *work)
+{
+	struct wg_peer *peer = container_of(work, struct wg_peer, tcp_inbound_remove_work.work);
+
+	if (peer->temp_peer){
+		wg_remove_from_tcp_connection_list(peer->device, peer->peer_socket);
+	} else {
+		wg_reset_tcp_socket_callbacks(peer, true);
+		wg_clean_peer_socket(peer, true, false, true); // clean and release
+	}
+}
+
+void wg_destruct_tcp_connection_list(struct wg_device *wg)
+{
+	struct wg_tcp_socket_list_entry *entry, *tmp;
+
+	// Iterate over the entire list and free each entry
+	list_for_each_entry_safe(entry, tmp, &wg->tcp_connection_list, tcp_connection_ll) {
+		spin_lock_bh(&wg->tcp_connection_list_lock);
+		list_del(&entry->tcp_connection_ll); // Removes the entry from the list
+        	spin_unlock_bh(&wg->tcp_connection_list_lock);
+		// Release the socket
+		if (entry->tcp_socket) {
+			kernel_sock_shutdown(entry->tcp_socket, SHUT_RDWR);
+			sock_release(entry->tcp_socket); // Release the socket
+		}
+		// Check if the TCP read work is scheduled before canceling it
+    		if (entry->temp_peer->tcp_read_worker_scheduled) {
+        		cancel_work_sync(&entry->temp_peer->tcp_read_work);
+        		entry->temp_peer->tcp_read_worker_scheduled = false;  // Reset the flag after canceling
+    		}
+
+    		// Destroy the TCP read workqueue if it exists
+    		if (entry->temp_peer->tcp_read_wq) {
+        		destroy_workqueue(entry->temp_peer->tcp_read_wq);
+        		entry->temp_peer->tcp_read_wq = NULL; // Avoid dangling pointers
+    		}
+
+		// clean up old temp_peer
+		if (!IS_ERR(entry->temp_peer) && entry->temp_peer) 
+			kfree(entry->temp_peer);
+		kfree(entry); // Free the memory allocated for the list entry
+	}
+}
+
+void wg_tcp_cleanup_worker(struct work_struct *work)
+{
+	struct wg_device *wg = container_of(work, struct wg_device, tcp_cleanup_work.work);
+	struct wg_tcp_socket_list_entry *entry, *tmp;
+	struct wg_peer *peer = NULL;
+	
+	ktime_t now = ktime_get();
+
+	// Cleanup logic: Remove old entries from the TCP connection list
+
+	list_for_each_entry_safe(entry, tmp, &wg->tcp_connection_list, tcp_connection_ll) {
+		if (ktime_ms_delta(now, entry->timestamp) > 5000) { // Check if older than 5 seconds
+			wg_remove_from_tcp_connection_list(wg, entry->tcp_socket);
+		}
+	}
+
+#ifdef WRONG
+	// Walk through the wg_dev peer list and call wg_tcp_write_space for each socket
+	rcu_read_lock();
+	list_for_each_entry_rcu(peer, &wg->peer_list, peer_list) {
+		// Check and call wg_tcp_write_space for socket if not null
+		if (peer->peer_socket) {
+			rcu_read_unlock();
+			wg_tcp_write_space(peer->peer_socket->sk);
+			rcu_read_lock();
+		}
+	}
+	rcu_read_unlock();
+#endif
+	
+	// Reschedule the worker
+	peer->device->tcp_cleanup_scheduled = true;
+	schedule_delayed_work(&wg->tcp_cleanup_work, msecs_to_jiffies(5000));
+}
+
+struct wg_peer *wg_temp_peer_create(struct wg_device *wg)
+{
+	struct wg_peer *peer;
+	int ret = -ENOMEM;
+	
+	
+	peer = kmalloc(sizeof(struct wg_peer), GFP_KERNEL);
+	if (unlikely(!peer)) {
+		return ERR_PTR(ret);
+	}
+	if (unlikely(dst_cache_init(&peer->endpoint_cache, GFP_KERNEL))) {
+		goto err;
+	}
+
+	
+	struct wg_socket_data *socket_data = kmalloc(sizeof(struct wg_socket_data), GFP_KERNEL);
+	if (unlikely(!socket_data)) {
+		pr_err("Failed to allocate memory for wg_socket_data\n");
+		goto err;
+	}
+
+	// Initialize the socket_data structure
+	socket_data->device = wg;
+	socket_data->peer = peer;
+	
+	peer->device = wg;
+	peer->internal_id = (u64)NULL;
+	peer->serial_work_cpu = (int)nr_cpumask_bits;
+	wg_timers_init(peer);
+	wg_prev_queue_init(&peer->tx_queue);
+	wg_prev_queue_init(&peer->rx_queue);
+	rwlock_init(&peer->endpoint_lock);
+	kref_init(&peer->refcount);
+	skb_queue_head_init(&peer->staged_packet_queue);
+	wg_noise_reset_last_sent_handshake(&peer->last_sent_handshake);
+	set_bit(NAPI_STATE_NO_BUSY_POLL, &peer->napi.state);
+	netif_napi_add(wg->dev, &peer->napi, wg_packet_rx_poll);
+	napi_enable(&peer->napi);
+	INIT_LIST_HEAD(&peer->allowedips_list);
+
+	// initialize TCP fields
+	peer->peer_socket = NULL;  // Initialize the peer socket to NULL
+
+	// Initialize the original socket callbacks to NULL
+	peer->original_outbound_state_change = NULL;
+	peer->original_outbound_write_space = NULL;
+	peer->original_outbound_data_ready = NULL;
+	peer->original_outbound_error_report = NULL;
+	peer->original_outbound_destruct = NULL;
+
+	peer->original_inbound_state_change = NULL;
+	peer->original_inbound_write_space = NULL;
+	peer->original_inbound_data_ready = NULL;
+	peer->original_inbound_error_report = NULL;
+	peer->original_inbound_destruct = NULL;
+
+	peer->partial_skb = NULL;  // Initialize the partial skb pointer to NULL
+	peer->expected_len = 0;    // Initialize expected length to 0
+	peer->received_len = 0;    // Initialize received length to 0
+
+	// Initialize the delayed work for TCP connection retry
+	INIT_DELAYED_WORK(&peer->tcp_retry_work, wg_tcp_retry_worker);
+
+	// Initialize the delayed work for TCP socket removal
+	INIT_DELAYED_WORK(&peer->tcp_inbound_remove_work, wg_tcp_inbound_remove_worker);
+	INIT_DELAYED_WORK(&peer->tcp_outbound_remove_work, wg_tcp_outbound_remove_worker);
+	
+	// Initialize TCP connection status flags
+	peer->tcp_established = false;
+	peer->tcp_pending = false;
+	peer->tcp_inbound_callbacks_set = false;
+	peer->tcp_outbound_callbacks_set = false;
+	peer->clean_inbound = false;
+	peer->clean_outbound = false;
+	peer->inbound_connected = false;
+	peer->outbound_connected = false;
+	peer->tcp_retry_scheduled = false;
+	peer->tcp_inbound_remove_scheduled = false;
+	peer->tcp_outbound_remove_scheduled = false;
+
+	// Initialize the spinlock for protecting TCP-related state
+	spin_lock_init(&peer->tcp_lock);
+	spin_lock_init(&peer->tcp_transfer_lock);
+
+	// Initialize the skb queue for the TX send queue
+	skb_queue_head_init(&peer->send_queue);
+
+	// Initialize the spinlock for the TX send queue
+	spin_lock_init(&peer->send_queue_lock);
+
+	// Initialize the list head for pending connection list
+	INIT_LIST_HEAD(&peer->pending_connection_list);
+
+	// Initialize the work structure, associating it with the worker functions
+	INIT_WORK(&peer->tcp_read_work, wg_tcp_read_worker);
+	// Create a workqueue for processing TCP read data
+	peer->tcp_read_wq = alloc_workqueue("tcp_read_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (!peer->tcp_read_wq) {
+        	pr_err("Failed to allocate read workqueue\n");
+		goto err;
+	}
+
+	INIT_WORK(&peer->tcp_write_work, wg_tcp_write_worker);
+
+	// Note this is a temp peer
+	peer->temp_peer = true;
+
+	return peer;
+
+err:
+	kfree(peer);
+	return ERR_PTR(ret);
+}
diff --git a/wireguard-linux/drivers/net/wireguard/socket.h b/wireguard-linux/drivers/net/wireguard/socket.h
index bab5848efbcd..5130ed9bce76 100644
--- a/wireguard-linux/drivers/net/wireguard/socket.h
+++ b/wireguard-linux/drivers/net/wireguard/socket.h
@@ -1,11 +1,13 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
+ * TCP Support Copyright (c) 2024 Jeff Nathan and Dragos Ruiu. All Rights Reserved. 
  * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.
  */
 
 #ifndef _WG_SOCKET_H
 #define _WG_SOCKET_H
 
+#include <linux/net.h>
 #include <linux/netdevice.h>
 #include <linux/udp.h>
 #include <linux/if_vlan.h>
@@ -29,6 +31,17 @@ void wg_socket_set_peer_endpoint(struct wg_peer *peer,
 void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer,
 					  const struct sk_buff *skb);
 void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
+void wg_destruct_tcp_connection_list(struct wg_device *wg);
+
+struct wg_tcp_encap_header {
+	__be32 length;
+	__u8 type;
+	__u8 flags;
+	__be16 checksum;
+};
+
+#define WG_TCP_ENCAP_HDR_LEN sizeof(struct wg_tcp_encap_header)
+#define WG_MAX_PACKET_SIZE 65535 + WG_TCP_ENCAP_HDR_LEN
 
 #if defined(CONFIG_DYNAMIC_DEBUG) || defined(DEBUG)
 #define net_dbg_skb_ratelimited(fmt, dev, skb, ...) do {                       \
@@ -41,4 +54,45 @@ void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
 #define net_dbg_skb_ratelimited(fmt, skb, ...)
 #endif
 
+/* Forward declarations of functions */
+int wg_socket_send_skb_to_peer(struct wg_peer *peer, struct sk_buff *skb, u8 ds);
+int wg_socket_send_buffer_to_peer(struct wg_peer *peer, void *buffer, size_t len, u8 ds);
+int wg_socket_send_buffer_as_reply_to_skb(struct wg_device *wg, struct sk_buff *in_skb, void *buffer, size_t len);
+int wg_socket_endpoint_from_skb(struct endpoint *endpoint, const struct sk_buff *skb);
+void wg_socket_set_peer_endpoint(struct wg_peer *peer, const struct endpoint *endpoint);
+void wg_socket_set_peer_endpoint_from_skb(struct wg_peer *peer, const struct sk_buff *skb);
+void wg_socket_clear_peer_endpoint_src(struct wg_peer *peer);
+void wg_socket_reinit(struct wg_device *wg, struct sock *new4, struct sock *new6);
+void wg_tcp_state_change(struct sock *sk);
+void wg_extract_endpoint_from_sock(struct sock *sk, struct endpoint *endpoint);
+bool wg_check_potential_header_validity(struct wg_tcp_encap_header *hdr, size_t remaining_len);
+
+int wg_tcp_queuepkt(struct wg_peer *, const void *, size_t);
+void wg_tcp_write_space(struct sock *sk);
+void wg_tcp_data_ready(struct sock *sk);
+
+void wg_add_tcp_socket_to_list(struct wg_device *wg, struct socket *sock);
+void wg_remove_from_tcp_connection_list(struct wg_device *wg, struct socket *sock);
+void wg_destruct_tcp_connection_list(struct wg_device *wg);
+
+int wg_tcp_listener_socket_init(struct wg_device *wg, u16 port);
+void wg_tcp_listener_socket_release(struct wg_device *wg);
+
+void wg_tcp_connection_retry_timer(struct timer_list *);
+int wg_tcp_connect(struct wg_peer *);
+
+int wg_tcp_listener_worker(struct wg_device *wg, struct socket *tcp_socket);
+struct socket *wg_setup_tcp_listen4(struct wg_device *wg, struct net *net, u16 port);
+struct socket *wg_setup_tcp_listen6(struct wg_device *wg, struct net *net, u16 port);
+int wg_tcp_listener4_thread(void *data);
+int wg_tcp_listener6_thread(void *data);
+
+void wg_clean_peer_socket(struct wg_peer *peer, bool release, bool destroy, bool inbound);
+void wg_timers_init(struct wg_peer *peer);
+void wg_tcp_write_worker(struct work_struct *work);
+void wg_tcp_read_worker(struct work_struct *work);
+void wg_tcp_cleanup_worker(struct work_struct *work);
+
+void lookup_default_interface(void);
+
 #endif /* _WG_SOCKET_H */
diff --git a/wireguard-linux/include/uapi/linux/wireguard.h b/wireguard-linux/include/uapi/linux/wireguard.h
index ae88be14c947..5cde6ad9303f 100644
--- a/wireguard-linux/include/uapi/linux/wireguard.h
+++ b/wireguard-linux/include/uapi/linux/wireguard.h
@@ -136,6 +136,9 @@
 
 #define WG_KEY_LEN 32
 
+#define WG_TRANSPORT_UDP 0
+#define WG_TRANSPORT_TCP 1
+
 enum wg_cmd {
 	WG_CMD_GET_DEVICE,
 	WG_CMD_SET_DEVICE,
@@ -157,6 +160,7 @@ enum wgdevice_attribute {
 	WGDEVICE_A_LISTEN_PORT,
 	WGDEVICE_A_FWMARK,
 	WGDEVICE_A_PEERS,
+	WGDEVICE_A_TRANSPORT,
 	__WGDEVICE_A_LAST
 };
 #define WGDEVICE_A_MAX (__WGDEVICE_A_LAST - 1)
